---
layout: post
usehighlight: true
tags: [LLM, introduction, NLP]
title: Large Language Models - Aspects and Methods
---


This post is the note for reviewing the development of the Large Language Models (LLMs), for more detailed and rigorous text, please refer to the monographs.

**Warning:** This post should not serve as an introduction text to whom may not be familiar with the deep learning methodologies, since it contains intensive personal opinions, not assured to be correct.

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [Sequence-to-sequence Learning](#sequence-to-sequence-learning)
    - [Recurrent Neural Network and Long Short Term Memory Network](#recurrent-neural-network-and-long-short-term-memory-network)
    - [Encoder-Decoder Architecture](#encoder-decoder-architecture)

## Sequence-to-sequence Learning

Despite the purpose of giving an introduction of neural language processing, we will start from reviewing  the machine translation (MT) task, for the simplicity  of the ideas -- transfrom one sequence of words to another sequence of words (in some other language).  Note that in the original [paper](https://arxiv.org/pdf/1409.3215.pdf) written by Ilya et al., the sequence to sequence learning could be a quite generalizable concept, below are several examples of sequences:

<img style="display: block;" class="img-fluid" src="https://i.imgur.com/ZVfrUta.png" alt="seq2seq.">
<p class="small">"Examples of sequences" by seq2seq ICML 17' tutorial</p>

In my opinion, the motivation for developing the modern sequence to sequence learning methods is driven by the following two problems:

* The traditional ML methods rely on the consistent input dimensions. That is to say, the model only deals with the inputs that share a constant shape.

* Given the training pairs (each pair contains two sequences usually sampled from two domains), the task is essentially to match the distribution of the former to the latter. How do we model, evaluate or optimize the process?

The first problem drives the development of recurrent neural network (RNN), while the second problem drives the architecture design of the moderm seq2seq learning models. We now briefly introduce the three important models to support the further discussion, they are RNN, LSTM, and self-attention.

### Recurrent Neural Network and Long Short Term Memory Network

We would give a short introduction of the model structures and optimization techniques, the main focus of this section remains to be the discussion in the context of sequence-to-sequence learning.

In short, Recurrent Neural Network (RNN) is a kind of NN architectures in which the model computation graph is directed cyclic graph, while the Long Short Term Memory (LSTM) Network is a more complex version of the core network of RNN. Though sound intuitive, RNNs are proven to be powerful in many ways, e.g., [RNNs are Turing Complete](https://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf), [RNNs are nearly intelligence-equivalent by approaching the asymptotic limit in text compression](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf). To make the following introduction non-trivial, we consider the predicting-next-character task, where the model is provided a sequence of characters and is required to predict the next character in the end of the sequence. (Note: The discussion here are mainly derived from the Ph.D [thesis](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf) of Ilya Sutskever).

**RNN Formulation**:

There are two perspectives on RNN, view it as a single network whose computation graph is cyclic, or view it as a super deep neural network with shared weights among the blocks (unrolled view). 

<img style="display: block;" class="img-fluid" src="https://i.imgur.com/ELw9Iu9.png" alt="RNN.">
<p class="small">"An unrolled RNN" by colah's blog</p>

My apology for the inconsistency, but we are going to use a slightly different notation from the figure. Let $i_t$ be the input at t-th moment, and $o_t$ be the output at t-th moment. Aparently RNN utilizes a hidden representation of the data, denoted by $h_t$. Now we define the input-output map as:

$$
o_t = g(h^o_t) \\
h^o_t = W_{oh}h_t + b_o \\
h_t = f(h^i_t) \\
h^i_t = W_{hi}i_t + W_{hh}h_{t-1} + b_h
$$

If you are familiar with the feed forward network, these are just two dense layers connected by the corresponding activation functions. The only difference is that the input layer takes the last moment's state as part of the input, i.e., a combination of the input and the hidden state. 

Before we discuss the LSTM network, we take a glance at the ackpropagation through time algorithm (BPTT). We first define the training loss function of RNN as the cumulative loss over time:

$$
\mathcal{L}(o, y) = \sum_{t=1}^T \mathcal{l}_t(o_t, y_t)
$$

Let's calculate the partial derivative of the loss over $W_{hi}$ and $W_{hh}$, since the calculation w.r.t $W_{oh}$ is straight forward. We have the following:

$$
\frac{\partial \mathcal{L}}{\partial W_{hh}} = \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial W_{hh}} =  \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial o_t}\frac{\partial o_t}{\partial h^o_t}W_{oh}\frac{\partial h_t}{\partial h^i_t}\frac{\partial h^i_t}{\partial W_{hh}} \\
\frac{\partial \mathcal{L}}{\partial W_{hi}} = \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial W_{hi}} =  \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial o_t}\frac{\partial o_t}{\partial h^o_t}W_{oh}\frac{\partial h_t}{\partial h^i_t}\frac{\partial h^i_t}{\partial W_{hi}}
$$

Note here you cannot directly calculate that $\frac{\partial h^i_t}{\partial W_{hh}}=h_{t-1}$ since $h_{t-1} = h_{t-1}(\cdot, W_{hh})$, that's because $W_{hh}$ is shared across the whole sequence. Instead, we apply the chain rule on the partial derivative of the multiplication $W_{hh}h^{t-1}$, we have:

$$
\frac{\partial h_t}{\partial W_{hh}}=\frac{\partial h_t}{\partial h^i_t}(h_{t-1}+W_{hh}\frac{\partial h_{t-1}}{\partial W_{hh}})=\frac{\partial h_t}{\partial h^i_t}(h_{t-1}+W_{hh}(\frac{\partial h_{t-1}}{\partial h^i_{t-1}}(h_{t-2}+W_{hh}\frac{\partial h_{t-2}}{\partial W_{hh}}))))= ...
$$
Following the chain rule, we obtain the following:
$$
\frac{\partial h_t}{\partial W_{hh}} = \sum_{j=0}^{t-1} \left(\prod_{k=0}^{j}\frac{\partial h_{t-k}}{\partial h^i_{t-k}}W_{hh}\right)h_{t-j-1}
$$
The above computation intuitively not stable, since the high order matrix multiplication is introduced. The resulted training process would be in face of potential gradient explosion and gradient vanishing. A straight forward solution is to truncate the historical computation with a relatively short window, and $j$ in the formula would start from $t-w$, where $w$ is the window size. Apparently we lose the long-term learning ability immediately, and that's where the LSTM network helps. You may have noticed that one of the most apparent problems of vanila RNN is that it encodes all the historical (and any potential future) information into one matrix, which may align with the intuition of our observations on the humans, yet failed to be effectively trained with gradient descent. 
> Note: You can try to enhance RNN to mitigate these problems, sometimes it's just a problem of imagination.

LSTM network tries to solve this problem with an architecture that splits the knowledge representation into two parts: The hidden state $h$ and the memory $C$. The good thing about $C$ is $\frac{\partial C_t}{\partial C_{t-1}}=1$. How would this help? Let's look at RNN and BPTT in an abstract view. Say, we regard the recurrent network as extracting a knowledge representation $W$ from data. So what we need to do is to define an interface of the knowledge representation $h$, a data to interface transformation $i$, and a interface to data transformation $o$. So the learning process could be defined as:
$$
\min \mathbb{E}_{x,y\sim P(x,y)}[\mathcal{L}(o(x), y)]\\
o(x)=o(h(i(x),W))
$$
This looks like any ML objectives, for the data structure of sequences, we model the data as $x=[x_1, x_2, ...x_T]$, then for RNN, we could process the data segment by segment. Now $i(x)$ is just any neural layer operation $\phi_i (W_ix+b_i)$. The point is that we model $h$ as $h = [h_1, h_2, ... h_t]$, while $h_t=W_{hh}h_{t-1}+W_{ih}x_t+b_h=W[h_{t-1}, x_t, 1]$, and $o=\phi_o(h)$, then we get the formulation of RNN. The gradient calculation follows:
$$
\frac{\partial \mathcal{L}}{\partial W} = \frac{1}{T}\sum_{t=1}^T \frac{\partial \mathcal{L_t}}{\partial W} = \frac{1}{T}\sum_{t=1}^T \frac{\partial \mathcal{L_t}}{\partial o_t}\frac{\partial o_t}{\partial h_t}\sum_{k=1}^t \prod_{j=k+1}^t\left(\frac{\partial h_j}{\partial h_{j-1}}\right)\frac{\partial h_k}{\partial W}
$$
["On the difficulty of training recurrent neural networks"](https://proceedings.mlr.press/v28/pascanu13.html) pointed out that, for some constant $\gamma$:
$$
\prod_{j=k+1}^t\frac{\partial h_j}{\partial h_{j-1}} = \prod_{j=k+1}^t diag(\frac{\partial h_j}{\partial h^i_{j-1}})\frac{\partial h^i_{j-1}}{\partial h_{j-1}}\approx 
\left\{ \begin{array}{rcl}
& 0\quad \text{for }max(eigen(diag))<\frac{1}{\gamma} \\
& \infty\quad \text{for }max(eigen(diag))>\frac{1}{\gamma}
\end{array}\right.
$$

 The interesting part is that now we can see that the forward calculation already involved with the $W_{hh}^t$. Of course if we model $i(x)$ as a direct map from $x$ to $h$, the problem is solved, but imagin how big the space of $x$ is, for example $x$ represents an arbitrary sentence written in English. So for now we still model $x$ as a sequence decomposition $[x_1, x_2, ...x_T]$, and see if there are better modelling for $h$. Now let's take a look at LSTM network.

<img style="display: block;" class="img-fluid" src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="LSTM.">
<p class="small">"LSTM network" by colah's blog</p>

This needs a few explanations, so the upper horizontal line in the figure denotes the memory $C$ and the lower horizontal line denotes the hidden state $h$. The calculation follows (from the left vertical line to the right):

$$
A_t = \sigma(W_A[h_{t-1},x_t]+b_A)\\
B_t = \sigma(W_B[h_{t-1},x_t]+b_B)\\
\hat{C}_t=tanh(W_C[h_{t-1},x_t]+b_C)\\
C_t = A_t\odot C_{t-1}+B_t\odot \hat{C}_t\\
D_t = \sigma(W_D[h_{t-1},x_t]+b_D)\\
h_t = D_t\odot tanh(C_t)
$$

where $\odot$ denotes the hadamard product $A\odot B_{ij}=A_{ij}B_{ij}$. The terminology "gate" in most of the introduction of LSTM denotes the sigmoid activation, since the resulted vector would serve as a mask with values near 0 or near 1. In the origin form of LSTM network, $A_t$ is set to be $A_{i}=1, \forall i$, thus we have $\frac{\partial C_t}{\partial C_{t-1}}=I$. Here the variant uses a learnt gate (called "forget gate") to control the memory, the gate would clear some of the values in the memory. So we can initialize $b_A$ as near 1 and then the network is trained to construct forget gate. Now the architecture seems a little bit complicate, we can use the gated recurrent unit (GRU) to simplify the process (practically, GRU saves memory usage):

$$
A_t = \sigma(W_A[h_{t-1},x_t]+b_A)\\
B_t = \sigma(W_B[h_{t-1},x_t]+b_B)\\
C_t=tanh(W_C[A_t\odot h_{t-1},x_t]+b_C)\\
h_t = (\mathbf{1}-B_t)\odot h_{t-1} + B_t\odot C_t
$$

Now we've come to a critical point, before we go any further, think for a minute: how can we improve those models in every possible ways? 

### Encoder-Decoder Architecture

Recall that our goal is to model the sequence prediction problem, ["Sequence to Sequence Learning
with Neural Networks"](https://arxiv.org/pdf/1409.3215.pdf) proposes the sequence-to-sequence learning with LSTM network. 