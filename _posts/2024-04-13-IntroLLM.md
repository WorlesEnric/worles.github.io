---
layout: post
usehighlight: true
tags: [LLM, introduction, NLP]
title: Large Language Models - Aspects and Methods
---


This post is the note for reviewing the development of the Large Language Models (LLMs), for more detailed and rigorous text, please refer to the monographs.

**Warning:** This post should not serve as an introduction text to whom may not be familiar with the deep learning methodologies, since it contains intensive personal opinions, not assured to be correct.

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [Sequence-to-sequence Learning via RNN](#sequence-to-sequence-learning-via-rnn)
    - [Recurrent Neural Network and Long Short Term Memory Network](#recurrent-neural-network-and-long-short-term-memory-network)
    - [Encoder-Decoder Architecture](#encoder-decoder-architecture)
    - [Self-Attention and Transformer](#self-attention-and-transformer)

## Sequence-to-sequence Learning via RNN

Despite the purpose of giving an introduction of neural language processing, we will start from reviewing  the machine translation (MT) task, for the simplicity  of the ideas -- transfrom one sequence of words to another sequence of words (in some other language).  Note that in the original [paper](https://arxiv.org/pdf/1409.3215.pdf) written by Ilya et al., the sequence to sequence learning could be a quite generalizable concept, below are several examples of sequences:

<img style="display: block;" class="img-fluid" src="https://i.imgur.com/ZVfrUta.png" alt="seq2seq.">
<p class="small">"Examples of sequences" by seq2seq ICML 17' tutorial</p>

In my opinion, the motivation for developing the modern sequence to sequence learning methods is driven by the following two problems:

* The traditional ML methods rely on the consistent input dimensions. That is to say, the model only deals with the inputs that share a constant shape.

* Given the training pairs (each pair contains two sequences usually sampled from two domains), the task is essentially to match the distribution of the former to the latter. How do we model, evaluate or optimize the process?

The first problem drives the development of recurrent neural network (RNN), while the second problem drives the architecture design of the moderm seq2seq learning models. We now briefly introduce the three important models to support the further discussion, they are RNN, LSTM, and self-attention.

### Recurrent Neural Network and Long Short Term Memory Network

We would give a short introduction of the model structures and optimization techniques, the main focus of this section remains to be the discussion in the context of sequence-to-sequence learning.

In short, Recurrent Neural Network (RNN) is a kind of NN architectures in which the model computation graph is directed cyclic graph, while the Long Short Term Memory (LSTM) Network is a more complex version of the core network of RNN. Though sound intuitive, RNNs are proven to be powerful in many ways, e.g., [RNNs are Turing Complete](https://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf), [RNNs are nearly intelligence-equivalent by approaching the asymptotic limit in text compression](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf). To make the following introduction non-trivial, we consider the predicting-next-character task, where the model is provided a sequence of characters and is required to predict the next character in the end of the sequence. (Note: The discussion here are mainly derived from the Ph.D [thesis](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf) of Ilya Sutskever).

**RNN Formulation**:

There are two perspectives on RNN, view it as a single network whose computation graph is cyclic, or view it as a super deep neural network with shared weights among the blocks (unrolled view). 

<img style="display: block;" class="img-fluid" src="https://i.imgur.com/ELw9Iu9.png" alt="RNN.">
<p class="small">"An unrolled RNN" by colah's blog</p>

My apology for the inconsistency, but we are going to use a slightly different notation from the figure. Let $i_t$ be the input at t-th moment, and $o_t$ be the output at t-th moment. Aparently RNN utilizes a hidden representation of the data, denoted by $h_t$. Now we define the input-output map as:

$$
o_t = g(h^o_t) \\
h^o_t = W_{oh}h_t + b_o \\
h_t = f(h^i_t) \\
h^i_t = W_{hi}i_t + W_{hh}h_{t-1} + b_h
$$

If you are familiar with the feed forward network, these are just two dense layers connected by the corresponding activation functions. The only difference is that the input layer takes the last moment's state as part of the input, i.e., a combination of the input and the hidden state. 

Before we discuss the LSTM network, we take a glance at the ackpropagation through time algorithm (BPTT). We first define the training loss function of RNN as the cumulative loss over time:

$$
\mathcal{L}(o, y) = \sum_{t=1}^T \mathcal{l}_t(o_t, y_t)
$$

Let's calculate the partial derivative of the loss over $W_{hi}$ and $W_{hh}$, since the calculation w.r.t $W_{oh}$ is straight forward. We have the following:

$$
\frac{\partial \mathcal{L}}{\partial W_{hh}} = \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial W_{hh}} =  \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial o_t}\frac{\partial o_t}{\partial h^o_t}W_{oh}\frac{\partial h_t}{\partial h^i_t}\frac{\partial h^i_t}{\partial W_{hh}} \\
\frac{\partial \mathcal{L}}{\partial W_{hi}} = \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial W_{hi}} =  \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial o_t}\frac{\partial o_t}{\partial h^o_t}W_{oh}\frac{\partial h_t}{\partial h^i_t}\frac{\partial h^i_t}{\partial W_{hi}}
$$

Note here you cannot directly calculate that $\frac{\partial h^i_t}{\partial W_{hh}}=h_{t-1}$ since $h_{t-1} = h_{t-1}(\cdot, W_{hh})$, that's because $W_{hh}$ is shared across the whole sequence. Instead, we apply the chain rule on the partial derivative of the multiplication $W_{hh}h^{t-1}$, we have:

$$
\frac{\partial h_t}{\partial W_{hh}}=\frac{\partial h_t}{\partial h^i_t}(h_{t-1}+W_{hh}\frac{\partial h_{t-1}}{\partial W_{hh}})=\frac{\partial h_t}{\partial h^i_t}(h_{t-1}+W_{hh}(\frac{\partial h_{t-1}}{\partial h^i_{t-1}}(h_{t-2}+W_{hh}\frac{\partial h_{t-2}}{\partial W_{hh}}))))= ...
$$
Following the chain rule, we obtain the following:
$$
\frac{\partial h_t}{\partial W_{hh}} = \sum_{j=0}^{t-1} \left(\prod_{k=0}^{j}\frac{\partial h_{t-k}}{\partial h^i_{t-k}}W_{hh}\right)h_{t-j-1}
$$
The above computation intuitively not stable, since the high order matrix multiplication is introduced. The resulted training process would be in face of potential gradient explosion and gradient vanishing. A straight forward solution is to truncate the historical computation with a relatively short window, and $j$ in the formula would start from $t-w$, where $w$ is the window size. Apparently we lose the long-term learning ability immediately, and that's where the LSTM network helps. You may have noticed that one of the most apparent problems of vanila RNN is that it encodes all the historical (and any potential future) information into one matrix, which may align with the intuition of our observations on the humans, yet failed to be effectively trained with gradient descent. 
> Note: You can try to enhance RNN to mitigate these problems, sometimes it's just a problem of imagination.

LSTM network tries to solve this problem with an architecture that splits the knowledge representation into two parts: The hidden state $h$ and the memory $C$. The good thing about $C$ is $\frac{\partial C_t}{\partial C_{t-1}}=1$. How would this help? Let's look at RNN and BPTT in an abstract view. Say, we regard the recurrent network as extracting a knowledge representation $W$ from data. So what we need to do is to define an interface of the knowledge representation $h$, a data to interface transformation $i$, and a interface to data transformation $o$. So the learning process could be defined as:
$$
\min \mathbb{E}_{x,y\sim P(x,y)}[\mathcal{L}(o(x), y)]\\
o(x)=o(h(i(x),W))
$$
This looks like any ML objectives, for the data structure of sequences, we model the data as $x=[x_1, x_2, ...x_T]$, then for RNN, we could process the data segment by segment. Now $i(x)$ is just any neural layer operation $\phi_i (W_ix+b_i)$. The point is that we model $h$ as $h = [h_1, h_2, ... h_t]$, while $h_t=W_{hh}h_{t-1}+W_{ih}x_t+b_h=W[h_{t-1}, x_t, 1]$, and $o=\phi_o(h)$, then we get the formulation of RNN. The gradient calculation follows:
$$
\frac{\partial \mathcal{L}}{\partial W} = \frac{1}{T}\sum_{t=1}^T \frac{\partial \mathcal{L_t}}{\partial W} = \frac{1}{T}\sum_{t=1}^T \frac{\partial \mathcal{L_t}}{\partial o_t}\frac{\partial o_t}{\partial h_t}\sum_{k=1}^t \prod_{j=k+1}^t\left(\frac{\partial h_j}{\partial h_{j-1}}\right)\frac{\partial h_k}{\partial W}
$$
["On the difficulty of training recurrent neural networks"](https://proceedings.mlr.press/v28/pascanu13.html) pointed out that, for some constant $\gamma$:
$$
\prod_{j=k+1}^t\frac{\partial h_j}{\partial h_{j-1}} = \prod_{j=k+1}^t diag(\frac{\partial h_j}{\partial h^i_{j-1}})\frac{\partial h^i_{j-1}}{\partial h_{j-1}}\approx 
\left\{ \begin{array}{rcl}
& 0\quad \text{for }max(eigen(diag))<\frac{1}{\gamma} \\
& \infty\quad \text{for }max(eigen(diag))>\frac{1}{\gamma}
\end{array}\right.
$$

 The interesting part is that now we can see that the forward calculation already involved with the $W_{hh}^t$. Of course if we model $i(x)$ as a direct map from $x$ to $h$, the problem is solved, but imagin how big the space of $x$ is, for example $x$ represents an arbitrary sentence written in English. So for now we still model $x$ as a sequence decomposition $[x_1, x_2, ...x_T]$, and see if there are better modelling for $h$. Now let's take a look at LSTM network.

<img style="display: block;" class="img-fluid" src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="LSTM.">
<p class="small">"LSTM network" by colah's blog</p>

This needs a few explanations, so the upper horizontal line in the figure denotes the memory $C$ and the lower horizontal line denotes the hidden state $h$. The calculation follows (from the left vertical line to the right):

$$
A_t = \sigma(W_A[h_{t-1},x_t]+b_A)\\
B_t = \sigma(W_B[h_{t-1},x_t]+b_B)\\
\hat{C}_t=tanh(W_C[h_{t-1},x_t]+b_C)\\
C_t = A_t\odot C_{t-1}+B_t\odot \hat{C}_t\\
D_t = \sigma(W_D[h_{t-1},x_t]+b_D)\\
h_t = D_t\odot tanh(C_t)
$$

where $\odot$ denotes the hadamard product $A\odot B_{ij}=A_{ij}B_{ij}$. The terminology "gate" in most of the introduction of LSTM denotes the sigmoid activation, since the resulted vector would serve as a mask with values near 0 or near 1. In the origin form of LSTM network, $A_t$ is set to be $A_{i}=1, \forall i$, thus we have $\frac{\partial C_t}{\partial C_{t-1}}=I$. Here the variant uses a learnt gate (called "forget gate") to control the memory, the gate would clear some of the values in the memory. So we can initialize $b_A$ as near 1 and then the network is trained to construct forget gate. Now the architecture seems a little bit complicate, we can use the gated recurrent unit (GRU) to simplify the process (practically, GRU saves memory usage):

$$
A_t = \sigma(W_A[h_{t-1},x_t]+b_A)\\
B_t = \sigma(W_B[h_{t-1},x_t]+b_B)\\
C_t=tanh(W_C[A_t\odot h_{t-1},x_t]+b_C)\\
h_t = (\mathbf{1}-B_t)\odot h_{t-1} + B_t\odot C_t
$$

Now we've come to a critical point, before we go any further, think for a minute: how can we improve those models in every possible ways? 

The following are two examples in [pytorch turorial: Classifying names via RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) and [pytorch tutorial: Generating names via RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html). 

In the first tutorial, refering to the origin post:

*"We will be building and training a basic character-level Recurrent Neural Network (RNN) to classify words. ... A character-level RNN reads words as a series of characters - outputting a prediction and “hidden state” at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to. Specifically, we’ll train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling."*

Now if we take a look at the goal:
```shell
$ python predict.py Hinton
(-0.47) Scottish
(-1.52) English
(-3.57) Irish

$ python predict.py Schmidhuber
(-0.19) German
(-2.48) Czech
(-2.68) Dutch
```

We will jump the data preprocessing part in this post, and assume the input data are organized as `{language: [name1, name2, ...]}`, indicating the languages and the corresponding names.

Now we will walk through the (mostly standard) procedures:

* The input letter is converted into tensors using one-hot encoding: `a/b/c...=[0,...,0,1,0,...,0]`. The word is represented by a tensor stacking the letters: `word.size=torch.Size([line_length, 1, n_letters_alphabet])`.

  ```python
  import torch

  # Find letter index from all_letters, e.g. "a" = 0
  def letterToIndex(letter):
      return all_letters.find(letter)
  
  # Just for demonstration, turn a letter into a <1 x n_letters> Tensor
  def letterToTensor(letter):
      tensor = torch.zeros(1, n_letters)
      tensor[0][letterToIndex(letter)] = 1
      return tensor
  
  # Turn a line into a <line_length x 1 x n_letters>, or an array of one-hot letter vectors
  def lineToTensor(line):
      tensor = torch.zeros(len(line), 1, n_letters)
      for li, letter in enumerate(line):
          tensor[li][0][letterToIndex(letter)] = 1
      return tensor
  ```

* Create an RNN with only forward operations, the pytorch computation graph would handle the recurrent information transmission:

  ```python
  import torch.nn as nn
  import torch.nn.functional as F

  class RNN(nn.Module):
      def __init__(self, input_size, hidden_size, output_size):
          super(RNN, self).__init__()

          self.hidden_size = hidden_size

          self.i2h = nn.Linear(input_size, hidden_size)
          self.h2h = nn.Linear(hidden_size, hidden_size)
          self.h2o = nn.Linear(hidden_size, output_size)
          self.softmax = nn.LogSoftmax(dim=1)

      def forward(self, input, hidden):
          hidden = F.tanh(self.i2h(input) + self.h2h(hidden))
          output = self.h2o(hidden)
          output = self.softmax(output)
          return output, hidden

      def initHidden(self):
          return torch.zeros(1, self.hidden_size)

  n_hidden = 128
  rnn = RNN(n_letters, n_hidden, n_categories)
  ```

* We can train the RNN by a training function like:

  ```python
  learning_rate = 0.005

  def train(category_tensor, line_tensor):
      hidden = rnn.initHidden()

      rnn.zero_grad()

      for i in range(line_tensor.size()[0]):
          output, hidden = rnn(line_tensor[i], hidden)

      loss = criterion(output, category_tensor)
      loss.backward()

      # Add parameters' gradients to their values, multiplied by learning rate
      for p in rnn.parameters():
          p.data.add_(p.grad.data, alpha=-learning_rate)

      return output, loss.item()
  ```

* We can run the training process for the whole corpus:

  ```python
  import time
  import math

  n_iters, print_every, plot_every = 100000, 5000, 1000

  # Keep track of losses for plotting
  current_loss = 0
  all_losses = []

  def randomChoice(l):
      return l[random.randint(0, len(l) - 1)]

  def randomTrainingExample():
      category = randomChoice(all_categories)
      line = randomChoice(category_lines[category])
      category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)
      line_tensor = lineToTensor(line)
      return category, line, category_tensor, line_tensor

  def categoryFromOutput(output):
      top_n, top_i = output.topk(1)
      category_i = top_i[0].item()
      return all_categories[category_i], category_i

  def timeSince(since):
      now = time.time()
      s = now - since
      m = math.floor(s / 60)
      s -= m * 60
      return '%dm %ds' % (m, s)

  start = time.time()

  for iter in range(1, n_iters + 1):
      category, line, category_tensor, line_tensor = randomTrainingExample()
      output, loss = train(category_tensor, line_tensor)
      current_loss += loss

      # Print ``iter`` number, loss, name and guess
      if iter % print_every == 0:
          guess, guess_i = categoryFromOutput(output)
          correct = '✓' if guess == category else '✗ (%s)' % category
          print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))

      # Add current loss avg to list of losses
      if iter % plot_every == 0:
          all_losses.append(current_loss / plot_every)
          current_loss = 0
  ```

* Finally, we can evaluate our model via:

  ```python
  # Keep track of correct guesses in a confusion matrix
  confusion = torch.zeros(n_categories, n_categories)
  n_confusion = 10000

  # Just return an output given a line
  def evaluate(line_tensor):
      hidden = rnn.initHidden()

      for i in range(line_tensor.size()[0]):
          output, hidden = rnn(line_tensor[i], hidden)

      return output

  # Go through a bunch of examples and record which are correctly guessed
  for i in range(n_confusion):
      category, line, category_tensor, line_tensor = randomTrainingExample()
      output = evaluate(line_tensor)
      guess, guess_i = categoryFromOutput(output)
      category_i = all_categories.index(category)
      confusion[category_i][guess_i] += 1

  # Normalize by dividing every row by its sum
  for i in range(n_categories):
      confusion[i] = confusion[i] / confusion[i].sum()

  # Set up plot
  fig = plt.figure()
  ax = fig.add_subplot(111)
  cax = ax.matshow(confusion.numpy())
  fig.colorbar(cax)

  # Set up axes
  ax.set_xticklabels([''] + all_categories, rotation=90)
  ax.set_yticklabels([''] + all_categories)

  # Force label at every tick
  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

  # sphinx_gallery_thumbnail_number = 2
  plt.show()
  ```

* For deploying the model to handle the user inputs, we do the following:
  
  ```python
  def predict(input_line, n_predictions=3):
      print('\n> %s' % input_line)
      with torch.no_grad():
          output = evaluate(lineToTensor(input_line))

          # Get top N categories
          topv, topi = output.topk(n_predictions, 1, True)
          predictions = []

          for i in range(n_predictions):
              value = topv[0][i].item()
              category_index = topi[0][i].item()
              print('(%.2f) %s' % (value, all_categories[category_index]))
              predictions.append([value, all_categories[category_index]])

  predict('Dovesky')
  ```

Now we proceed the second task: generating names using RNN. The results should look like:

```shell
$ python sample.py Russian RUS
Rovakov
Uantov
Shavakov
```

The data preprocessing is pretty much alike to the previous task, now we take a look at the model architecture of this task.

<img style="display: block;" class="img-fluid" src="https://i.imgur.com/jzVrf7f.png" alt="RNN generator.">
<p class="small">"Network Architecture" by Sean Robertson</p>

* Construct the model as following. The input of the model is a triplet `[category, input_at_t, hidden_state]`, the output would then serve as the input for the next time step `output_at_t=input_at_t+1`.

  ```python
  import torch
  import torch.nn as nn

  class RNN(nn.Module):
      def __init__(self, input_size, hidden_size, output_size):
          super(RNN, self).__init__()
          self.hidden_size = hidden_size

          self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)
          self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)
          self.o2o = nn.Linear(hidden_size + output_size, output_size)
          self.dropout = nn.Dropout(0.1)
          self.softmax = nn.LogSoftmax(dim=1)

      def forward(self, category, input, hidden):
          input_combined = torch.cat((category, input, hidden), 1)
          hidden = self.i2h(input_combined)
          output = self.i2o(input_combined)
          output_combined = torch.cat((hidden, output), 1)
          output = self.o2o(output_combined)
          output = self.dropout(output)
          output = self.softmax(output)
          return output, hidden

      def initHidden(self):
          return torch.zeros(1, self.hidden_size)
  ```

* We need to first get the random pairs of `(category, name)`. Then construct the input item as `(category, cur_letter, hidden)`, the network predict the next letter at every time step. In practice, we create the pairs `('a', 'b')`, `('b', 'c')`, `('c', '<EOS>')` for the word `'abc'`.

  ```python
  import random

  # Random item from a list
  def randomChoice(l):
      return l[random.randint(0, len(l) - 1)]

  # Get a random category and random line from that category
  def randomTrainingPair():
      category = randomChoice(all_categories)
      line = randomChoice(category_lines[category])
      return category, line

  # One-hot vector for category
  def categoryTensor(category):
      li = all_categories.index(category)
      tensor = torch.zeros(1, n_categories)
      tensor[0][li] = 1
      return tensor

  # One-hot matrix of first to last letters (not including EOS) for input
  def inputTensor(line):
      tensor = torch.zeros(len(line), 1, n_letters)
      for li in range(len(line)):
          letter = line[li]
          tensor[li][0][all_letters.find(letter)] = 1
      return tensor

  # ``LongTensor`` of second letter to end (EOS) for target
  def targetTensor(line):
      letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]
      letter_indexes.append(n_letters - 1) # EOS
      return torch.LongTensor(letter_indexes)

  # Make category, input, and target tensors from a random category, line pair
  def randomTrainingExample():
      category, line = randomTrainingPair()
      category_tensor = categoryTensor(category)
      input_line_tensor = inputTensor(line)
      target_line_tensor = targetTensor(line)
      return category_tensor, input_line_tensor, target_line_tensor
  ```

* Now we train the network via:

  ```python
  criterion = nn.NLLLoss()

  learning_rate = 0.0005

  def train(category_tensor, input_line_tensor, target_line_tensor):
      target_line_tensor.unsqueeze_(-1)
      hidden = rnn.initHidden()

      rnn.zero_grad()

      loss = torch.Tensor([0]) # you can also just simply use ``loss = 0``

      for i in range(input_line_tensor.size(0)):
          output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)
          l = criterion(output, target_line_tensor[i])
          loss += l

      loss.backward()

      for p in rnn.parameters():
          p.data.add_(p.grad.data, alpha=-learning_rate)

      return output, loss.item() / input_line_tensor.size(0)
  ```
* Now to generate a name given a category, we sample the network via:

  ```python
  max_length = 20

  # Sample from a category and starting letter
  def sample(category, start_letter='A'):
      with torch.no_grad():  # no need to track history in sampling
          category_tensor = categoryTensor(category)
          input = inputTensor(start_letter)
          hidden = rnn.initHidden()

          output_name = start_letter

          for i in range(max_length):
              output, hidden = rnn(category_tensor, input[0], hidden)
              topv, topi = output.topk(1)
              topi = topi[0][0]
              if topi == n_letters - 1:
                  break
              else:
                  letter = all_letters[topi]
                  output_name += letter
              input = inputTensor(letter)

          return output_name

  # Get multiple samples from one category and multiple starting letters
  def samples(category, start_letters='ABC'):
      for start_letter in start_letters:
          print(sample(category, start_letter))

  samples('Russian', 'RUS')
  ```

### Encoder-Decoder Architecture

Recall that our goal is to model the sequence prediction problem, ["Sequence to Sequence Learning
with Neural Networks"](https://arxiv.org/pdf/1409.3215.pdf) proposes the sequence-to-sequence learning with LSTM network. The very fundamental idea of solving this problem is the encoder-decoder architecture. Recall that the RNN would produce the hidden state at every time step, when a sentence is feed into RNN, every word in the sentence represents the input at every time step. We recognize the final hidden state as the context vector, being an abstract representation of the input sentence. Now you can imagine how another RNN take the context vector as part of the input to decode the information from this representation. The diagram illustration is:


<img style="display: block;" class="img-fluid" src="https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/b3cd54c72cd6e4e63f672d334c795b4fe744ef92//assets/seq2seq1.png" alt="Seq-to-seq architecture">
<p class="small">"sequence to sequence RNNs" by pytorch-seq2seq tutorial</p>

The above example shows the process of machine traslation. In the decoder, we reuse the decoded output at $t-1$ as the input at $t$. This is called autoregressive model, where the overall goal is to predict the next element in the sequence with all the previous predictions. Formally, the autoregressive model is a probabilistic model $p(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(t)})$ such that:

$$
p\left(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(t)}\right)=\prod_{t=1}^T p\left(\mathbf{x}^{(t)} \mid \mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(t-1)}\right)
$$

The network usually uses the softmax activation for predicting the distribution (assuming the discrete distribution). In MT task, the decoder would output the translated sequence one word by one word, the previous predicted words would be feed into the decoder as part of the input.

We now walk through the steps in typical sequence to sequence modeling process. The following contents are mainly from this [tutorial](https://github.com/bentrevett/pytorch-seq2seq), and it's written much better than my post, please refer the origin post if anything is unclear.

* Datasets are organized to be pairs of English version and German version of one sentence.
  
  ```python
  train_data, valid_data, test_data = (
      dataset["train"],
      dataset["validation"],
      dataset["test"],
  )
  train_data[0]
  > {'en': 'Two young, White males are outside near many bushes.',
  'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}
  ```

* A tokenizer is used to turn a string into a list of tokens that make up the string. A token is a generalized word in that case, the tokens may be words, punctuation, numbers and any special symbols.
  
  ```python
  > python -m spacy download en_core_web_sm
  > python -m spacy download de_core_news_sm
  en_nlp = spacy.load("en_core_web_sm")
  de_nlp = spacy.load("de_core_news_sm")

  string = "What a lovely day it is today!"
  [token.text for token in en_nlp.tokenizer(string)]
  > ['What', 'a', 'lovely', 'day', 'it', 'is', 'today', '!']
  
  # apply tokenizer to all the datasets
  def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):
      en_tokens = [token.text for token in en_nlp.tokenizer(example["en"])][:max_length]
      de_tokens = [token.text for token in de_nlp.tokenizer(example["de"])][:max_length]
      if lower:
          en_tokens = [token.lower() for token in en_tokens]
          de_tokens = [token.lower() for token in de_tokens]
      en_tokens = [sos_token] + en_tokens + [eos_token]
      de_tokens = [sos_token] + de_tokens + [eos_token]
      return {"en_tokens": en_tokens, "de_tokens": de_tokens}
  
  max_length = 1_000
  lower = True
  sos_token = "<sos>"
  eos_token = "<eos>"

  fn_kwargs = {
      "en_nlp": en_nlp,
      "de_nlp": de_nlp,
      "max_length": max_length,
      "lower": lower,
      "sos_token": sos_token,
      "eos_token": eos_token,
  }

  train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)
  valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)
  test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)
  ```

* Next we need to build a vocabulary for the source and target languages. The vocabulary transform every token to a index. For the tokens appear in the validation and test sets but not in the training set, we use an "unknown token" (`<UNK>`) to denote. To make the model aware of `<UNK>` during training, we replace the tokens which appear less than `min_freq` with `<UNK>`. Another special token is `<PAD>`, in cases that we wish to pass a batch of sentences with different length to the model, we use `<PAD>` to pad the shorter sentences.

  ```python
  min_freq = 2
  unk_token = "<unk>"
  pad_token = "<pad>"

  special_tokens = [
      unk_token,
      pad_token,
      sos_token,
      eos_token,
  ]

  en_vocab = torchtext.vocab.build_vocab_from_iterator(
      train_data["en_tokens"],
      min_freq=min_freq,
      specials=special_tokens,
  )

  de_vocab = torchtext.vocab.build_vocab_from_iterator(
      train_data["de_tokens"],
      min_freq=min_freq,
      specials=special_tokens,
  )

  unk_index = en_vocab[unk_token]
  pad_index = en_vocab[pad_token]

  en_vocab.set_default_index(unk_index)
  de_vocab.set_default_index(unk_index)

  def numericalize_example(example, en_vocab, de_vocab):
      en_ids = en_vocab.lookup_indices(example["en_tokens"])
      de_ids = de_vocab.lookup_indices(example["de_tokens"])
      return {"en_ids": en_ids, "de_ids": de_ids}


  fn_kwargs = {"en_vocab": en_vocab, "de_vocab": de_vocab}

  train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)
  valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)
  test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)

  data_type = "torch"
  format_columns = ["en_ids", "de_ids"]

  train_data = train_data.with_format(
      type=data_type, columns=format_columns, output_all_columns=True
  )

  valid_data = valid_data.with_format(
      type=data_type,
      columns=format_columns,
      output_all_columns=True,
  )

  test_data = test_data.with_format(
      type=data_type,
      columns=format_columns,
      output_all_columns=True,
  )
  ```

* Now we create the dataloader for feeding the data into model.

  ```python
  # padding the batch with pad_index (the index of <PAD>)
  def get_collate_fn(pad_index):
      def collate_fn(batch):
          batch_en_ids = [example["en_ids"] for example in batch]
          batch_de_ids = [example["de_ids"] for example in batch]
          batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)
          batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)
          batch = {
              "en_ids": batch_en_ids,
              "de_ids": batch_de_ids,
          }
          return batch

      return collate_fn

  def get_data_loader(dataset, batch_size, pad_index, shuffle=False):
      collate_fn = get_collate_fn(pad_index)
      data_loader = torch.utils.data.DataLoader(
          dataset=dataset,
          batch_size=batch_size,
          collate_fn=collate_fn,
          shuffle=shuffle,
      )
      return data_loader

  batch_size = 128

  train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)
  valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)
  test_data_loader = get_data_loader(test_data, batch_size, pad_index)
  ```

* Build our model in three parts: The encode, the decoder, connecting the encoder and the decoder as the seq2seq model.

  ```python
  class Encoder(nn.Module):
      def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):
          super().__init__()
          self.hidden_dim = hidden_dim
          self.n_layers = n_layers
          self.embedding = nn.Embedding(input_dim, embedding_dim)
          self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)
          self.dropout = nn.Dropout(dropout)

      def forward(self, src):
          embedded = self.dropout(self.embedding(src))
          outputs, (hidden, cell) = self.rnn(embedded)
          return hidden, cell


  class Decoder(nn.Module):
      def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):
          super().__init__()
          self.output_dim = output_dim
          self.hidden_dim = hidden_dim
          self.n_layers = n_layers
          self.embedding = nn.Embedding(output_dim, embedding_dim)
          self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)
          self.fc_out = nn.Linear(hidden_dim, output_dim)
          self.dropout = nn.Dropout(dropout)

      def forward(self, input, hidden, cell):
          input = input.unsqueeze(0)
          embedded = self.dropout(self.embedding(input))
          output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
          prediction = self.fc_out(output.squeeze(0))
          return prediction, hidden, cell


  class Seq2Seq(nn.Module):
      def __init__(self, encoder, decoder, device):
          super().__init__()
          self.encoder = encoder
          self.decoder = decoder
          self.device = device
          assert (
              encoder.hidden_dim == decoder.hidden_dim
          ),
          assert (
              encoder.n_layers == decoder.n_layers
          ),

      def forward(self, src, trg, teacher_forcing_ratio):
          batch_size = trg.shape[1]
          trg_length = trg.shape[0]
          trg_vocab_size = self.decoder.output_dim
          outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)
          hidden, cell = self.encoder(src)
          input = trg[0, :]
          for t in range(1, trg_length):
              output, hidden, cell = self.decoder(input, hidden, cell)
              outputs[t] = output
              teacher_force = random.random() < teacher_forcing_ratio
              top1 = output.argmax(1)
              input = trg[t] if teacher_force else top1
          return outputs
  ```

* Train, evaluate, and serve the model via:

  ```python
  input_dim = len(de_vocab)
  output_dim = len(en_vocab)
  encoder_embedding_dim = 256
  decoder_embedding_dim = 256
  hidden_dim = 512
  n_layers = 2
  encoder_dropout = 0.5
  decoder_dropout = 0.5
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  encoder = Encoder(
      input_dim,
      encoder_embedding_dim,
      hidden_dim,
      n_layers,
      encoder_dropout,
  )

  decoder = Decoder(
      output_dim,
      decoder_embedding_dim,
      hidden_dim,
      n_layers,
      decoder_dropout,
  )

  model = Seq2Seq(encoder, decoder, device).to(device)

  def init_weights(m):
      for name, param in m.named_parameters():
          nn.init.uniform_(param.data, -0.08, 0.08)
  model.apply(init_weights)

  def count_parameters(model):
      return sum(p.numel() for p in model.parameters() if p.requires_grad)
  print(f"The model has {count_parameters(model):,} trainable parameters")
  # > The model has 13,898,501 trainable parameters

  optimizer = optim.Adam(model.parameters())
  criterion = nn.CrossEntropyLoss(ignore_index=pad_index)

  def train_fn(
      model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device
  ):
      model.train()
      epoch_loss = 0
      for i, batch in enumerate(data_loader):
          src = batch["de_ids"].to(device)
          trg = batch["en_ids"].to(device)
          optimizer.zero_grad()
          output = model(src, trg, teacher_forcing_ratio)
          output_dim = output.shape[-1]
          output = output[1:].view(-1, output_dim)
          trg = trg[1:].view(-1)
          # trg = [(trg length - 1) * batch size]
          loss = criterion(output, trg)
          loss.backward()
          torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
          optimizer.step()
          epoch_loss += loss.item()
      return epoch_loss / len(data_loader)

  def evaluate_fn(model, data_loader, criterion, device):
      model.eval()
      epoch_loss = 0
      with torch.no_grad():
          for i, batch in enumerate(data_loader):
              src = batch["de_ids"].to(device)
              trg = batch["en_ids"].to(device)
              output = model(src, trg, 0)
              output_dim = output.shape[-1]
              output = output[1:].view(-1, output_dim)
              trg = trg[1:].view(-1)
              loss = criterion(output, trg)
              epoch_loss += loss.item()
      return epoch_loss / len(data_loader)

  # Training
  n_epochs = 10
  clip = 1.0
  teacher_forcing_ratio = 0.5

  best_valid_loss = float("inf")

  for epoch in tqdm.tqdm(range(n_epochs)):
      train_loss = train_fn(
          model,
          train_data_loader,
          optimizer,
          criterion,
          clip,
          teacher_forcing_ratio,
          device,
      )
      valid_loss = evaluate_fn(
          model,
          valid_data_loader,
          criterion,
          device,
      )
      if valid_loss < best_valid_loss:
          best_valid_loss = valid_loss
          torch.save(model.state_dict(), "tut1-model.pt")
      print(f"\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}")
      print(f"\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}")

  # Evaluating
  model.load_state_dict(torch.load("tut1-model.pt"))

  test_loss = evaluate_fn(model, test_data_loader, criterion, device)

  print(f"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |")

  # Deploying
  def translate_sentence(
      sentence,
      model,
      en_nlp,
      de_nlp,
      en_vocab,
      de_vocab,
      lower,
      sos_token,
      eos_token,
      device,
      max_output_length=25,
  ):
      model.eval()
      with torch.no_grad():
          if isinstance(sentence, str):
              tokens = [token.text for token in de_nlp.tokenizer(sentence)]
          else:
              tokens = [token for token in sentence]
          if lower:
              tokens = [token.lower() for token in tokens]
          tokens = [sos_token] + tokens + [eos_token]
          ids = de_vocab.lookup_indices(tokens)
          tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)
          hidden, cell = model.encoder(tensor)
          inputs = en_vocab.lookup_indices([sos_token])
          for _ in range(max_output_length):
              inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)
              output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)
              predicted_token = output.argmax(-1).item()
              inputs.append(predicted_token)
              if predicted_token == en_vocab[eos_token]:
                  break
          tokens = en_vocab.lookup_tokens(inputs)
      return tokens
  ```
### Self-Attention and Transformer

You may noticed that the context vector serves as the information bottleneck (IB) of the encoder-decoder architecture. The IB would compress and preserve the useful representation of the origin input, this aim is, however, not assured to be accomplished. Since the context vector is merely the final hidden state of the RNN encoder, it is natural to think that the context vector would somehow lose information from the faraway input tokens. Now if we stack all the hidden states into one big tensor, apparently there would be redundance of information. We need a selection of the hidden states to preserve the important information, with respect to what? The simple solution is to let the decoder choose. We now introduce the attention mechanism. This mechanism still starts from a simple idea: we cannot take all the hidden states as the context vector, we can sum over them to create one context vector. The summation is weighted by the decoder. So for each decoder step, we regenerate the context vector, to focus on the information that is relavant to the current prediction. Still, a simple idea is to compute a relavance score between the last hidden state of the current decoder input and each encoder hidden state output $\operatorname{score}(\mathbf{h}_{i-1}^d, \mathbf{h}_j^e)=\mathbf{h}_{i-1}^d\cdot\mathbf{h}_j^e$. This similarity is then used to compute weights:

$$
\alpha_{i j} =\operatorname{softmax}\left(\operatorname{score}\left(\mathbf{h}_{i-1}^d, \mathbf{h}_j^e\right)\right) =\frac{\exp \left(\operatorname{score}\left(\mathbf{h}_{i-1}^d, \mathbf{h}_j^e\right)\right.}{\sum_k \exp \left(\operatorname{score}\left(\mathbf{h}_{i-1}^d, \mathbf{h}_k^e\right)\right)}
$$

For the discussion of transformers in this post, we mainly focus on building the connection between this architecture and the NLP problems. Several facts need to be clarified:

* We discuss the transformers in solving the problem of causal language modeling, which means to predict the next word in the sentence in an autoregressive fashion.

* Transformers are not built with recurrent blocks, so it's different from the way we build RNN (LSTM) blocks. Instead, transformers use multiple multi-head self-attention blocks to project the origin input to a series of hidden states of the same length as the input, and finally obtain a highly abstracted representation.

* Transformer's key novelty is the self-attention mechanism, which allows the transformer to build layer-by-layer information aggregation.
  
Self-attention appears in two ways: causal (backward looking) self-attention and the bidirectional self-attention. The former assumes the causal structure in the sequence where only the historical information is accessable, while the latter could access all the information in the sequence. In this post, we mainly focus on the causal setting, since it fits in the intuition of natural language directly. As we could conclude from the origin attention mechanism, the only aim is to computing a relavance score between the current hidden state and the historical hidden states, the dot product similarity may be too naive. The self-attention introduced a role-based encoding scheme:

* When the embedding is in the role of the current focus, we refer to it as a `query`.
* When the embedding is in the role of a preceding input, we refer to it as a `key`.
* When it serves as a `key`, we also compute a `value` to serve as the embedding to be weighed. So the final aggregation is calculated based on `value` instead of the embedding `x` itself.

The self-attention is calculated as:

$$
\begin{aligned}
\mathbf{q}_i=\mathbf{x}_i \mathbf{W}^{\mathbf{Q}} ; \mathbf{k}_i & =\mathbf{x}_i \mathbf{W}^{\mathbf{k}} ; \mathbf{v}_i=\mathbf{x}_i \mathbf{W}^{\mathbf{v}} \\
\operatorname{score}\left(\mathbf{x}_i, \mathbf{x}_j\right) & =\frac{\mathbf{q}_i \cdot \mathbf{k}_j}{\sqrt{d_k}} \\
\alpha_{i j} & =\operatorname{softmax}\left(\operatorname{score}\left(\mathbf{x}_i, \mathbf{x}_j\right)\right) \forall j \leq i \\
\mathbf{a}_i & =\sum_{j \leq i} \alpha_{i j} \mathbf{v}_j
\end{aligned}
$$
  
It's clear now, we transform the origin embedding into three different vector spaces, and produce the final output to the next layer in the vector space of `value`. Here $d_k$ is the dimension of the vector space spanned by `key`s, the division by $\sqrt{d_k}$ is to maintain the numerical stability of the dot product. Another advantage is that for a sequence constitutes of multiple tokens, the self-attention could be effectively calculated by stacking the embeddings of every token:

$$
\begin{aligned}
\mathbf{Q}=\mathbf{X} \mathbf{W}^{\mathbf{Q}} ; \mathbf{K} & =\mathbf{X} \mathbf{W}^{\mathbf{K}} ; \mathbf{V}=\mathbf{X} \mathbf{W}^{\mathbf{V}} \\
\mathbf{A} & =\operatorname{softmax}(\frac{\mathbf{Q} \cdot \mathbf{K}^T}{\sqrt{d_k}})\mathbf{V}
\end{aligned}
$$

For the causal self-attention, we need to mask the upper triangular of the matrix $\mathbf{Q} \cdot \mathbf{K}^T$, corresponding to the future `key`s. This matrix also reveals the quadratic relationship between the computation scale and the input sequence length.

The above description is referred to as the single head self-attention, the building block of the multi-head self-attention, which is what transformer used in practice. The multi-head self-attention adopts a group of the single-head versions, which means that we calculate $h$ different self-attention results with $\mathbf{W}_i^{\mathbf{Q}}$, $\mathbf{W}_i^{\mathbf{k}}$, and $\mathbf{W}_i^{\mathbf{V}}$, for $i\in[h]$. We aggregate the results with a linear projection $\mathbf{W}^\mathbf{O}$ to produce the output:

$$
\begin{aligned}
\mathbf{Q}_i=\mathbf{X} \mathbf{W}_i^{\mathbf{Q}} ; \mathbf{K}_i & =\mathbf{X} \mathbf{W}_i^{\mathbf{K}} ; \mathbf{V}_i=\mathbf{X} \mathbf{W}_i^{\mathbf{V}} \\
head_i & =\operatorname{softmax}(\frac{\mathbf{Q}_i \cdot \mathbf{K}_i^T}{\sqrt{d_k}})\mathbf{V}_i \\
\mathbf{A}=\text { MultiHeadAttention }(\mathbf{X})&=\left(\text { head }_1 \oplus \text { head }_2 \ldots \oplus \text { head }_h\right) \mathbf{W}^O
\end{aligned}
$$

where the $\oplus$ denotes the concatenation. 