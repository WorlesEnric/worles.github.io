<!DOCTYPE html>
<html lang="en" name="top">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6" crossorigin="anonymous">
  
  <!-- Option 1: Bootstrap Bundle with Popper -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js" integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf" crossorigin="anonymous"></script>

  <!-- Bootstrap Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.4.1/font/bootstrap-icons.css">

  <!-- FontAwesome Icons -->
  <script src="https://kit.fontawesome.com/22ab6dd597.js" crossorigin="anonymous"></script>

  <!-- Highlight JS -->
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/atom-one-light.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
  
  
  <!-- Fork Awesom -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">

  <!-- ChartJS -->
  

  <!-- Mathjax support -->
  

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/styles.css">

  <!-- SEO -->
  <title>
    
      Revisiting Mathematics in Deep Learning (or ML)
    
  </title>
  <link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" />
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Revisiting Mathematics in Deep Learning (or ML)" />
<meta name="author" content="Qihang Wang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post is the note for reviewing the fundamental mathematics in machine learning, especially, deep learning area. We will briefly revisit the basic concepts and useful tools in the analysis of ML algorithms." />
<meta property="og:description" content="This post is the note for reviewing the fundamental mathematics in machine learning, especially, deep learning area. We will briefly revisit the basic concepts and useful tools in the analysis of ML algorithms." />
<link rel="canonical" href="http://localhost:4000/2024/04/10/fundmentalmath" />
<meta property="og:url" content="http://localhost:4000/2024/04/10/fundmentalmath" />
<meta property="og:site_name" content="Qihang’s site" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-04-10T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Revisiting Mathematics in Deep Learning (or ML)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Qihang Wang"},"dateModified":"2024-04-10T00:00:00+08:00","datePublished":"2024-04-10T00:00:00+08:00","description":"This post is the note for reviewing the fundamental mathematics in machine learning, especially, deep learning area. We will briefly revisit the basic concepts and useful tools in the analysis of ML algorithms.","headline":"Revisiting Mathematics in Deep Learning (or ML)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2024/04/10/fundmentalmath"},"url":"http://localhost:4000/2024/04/10/fundmentalmath"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
  <main>
    <header><nav>
  <ul>
    <li>
      <a class="text-muted" href="/">
        
          Qihang&nbsp;Wang's
        
      </a>
    </li>
    <br />
    
    <li>
      <a href="/overview" >
      many thanks
      </a>
    </li>
    
    <li>
      <a href="/projects" >
      projects
      </a>
    </li>
    
    <li>
      <a href="/blog" >
      blog
      </a>
    </li>
    
    <li>
      <a href="/contact" >
      contact
      </a>
    </li>
    
  </ul>
</nav></header>
    <section>
      <div>
  <h1 style="color: #cc0000;">Revisiting Mathematics in Deep Learning (or ML)</h1>
  <span>
  Tags:
    
      <a href="/tag/math">math</a>, 
    
      <a href="/tag/deep learning">deep learning</a>, 
    
      <a href="/tag/algebra">algebra</a>
    
  </span>
  <br />
  <br />
  <p>This post is the note for reviewing the fundamental mathematics in machine learning, especially, deep learning area. We will briefly revisit the basic concepts and useful tools in the analysis of ML algorithms.</p>

<p>Here are some excellent materials you may find helpful:</p>

<ol>
  <li><a href="https://www.cis.upenn.edu/~jean/math-deep.pdf">Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning</a></li>
  <li><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The matrix cookbook</a></li>
</ol>

<p>In fact, most of the contents in this post derive from [1].</p>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
  <li><a href="#table-of-contents">Table of Contents</a>
    <ul>
      <li><a href="#preliminaries">Preliminaries</a></li>
      <li><a href="#calculus">Calculus</a>
        <ul>
          <li><a href="#curves-scalar-fields-and-gradients">Curves, Scalar Fields, and Gradients</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<p><strong>Note</strong>: This post assumes you have the preliminary knowledge of simple calculus in univariate case and linear algebra. Here provides some concepts which help us see the things in an abstract way (meaning that we then know where can or cannot we generalize our analysis to).</p>

<p>(Derivative of the univariate scalar-valued function) A function is called <em>differentiable</em> at a point $a$ of its domain, if its domain contains an open interval containing $a$, and the limit:
\(\lim_{h\rightarrow 0}\frac{f(a+h)-f(a)}{h}\)
exists. The $\epsilon-\delta$ definition of the derivative of $f(x)$ w.r.t its input $x$ states, $\forall \epsilon\in\mathbb{R}^+$,  $\exist\delta\in\mathbb{R}^+$, such that, $\forall |h|&lt;\delta$ and $f(a+h)$ is defined, and
\(|L-\frac{f(a+h)-f(a)}{h}|&lt;\epsilon\)</p>

<h2 id="calculus">Calculus</h2>

<p>This post assumes you have the preliminary knowledge of simple calculus in univariate case. The overall goal is to generalize the analysis on the function $f:\mathbb{R}\rightarrow\mathbb{R}$ to $f:\mathbb{R}^m\rightarrow\mathbb{R}^n$. Some useful concepts are:</p>

<h3 id="curves-scalar-fields-and-gradients">Curves, Scalar Fields, and Gradients</h3>

<p>The curves take the form $f:\mathbb{R}\rightarrow\mathbb{R}^n$. We could use $\mathbf{x}(t)$ to denote the curve with $\mathbf{x}\in \mathbf{R}^n$ and $t\in\mathbb{R}$. The curve is said to be <em>differentiable</em> if as $\Delta t\rightarrow 0$, we have $\mathbf{x}(t+\Delta t)-\mathbf{x}(t)=\mathbf{x}’(t)\Delta t+\mathcal{O}(\Delta t^2)$. This serves as the definition of the derivative $\mathbf{x}’(t)$:</p>

\[\frac{d\mathbf{x}}{dt}=\mathbf{x}'(t)=\lim_{\Delta t\rightarrow 0}\frac{\Delta\mathbf{x}}{\Delta t}\]

<p>it also follows that $\frac{d}{dt}(\mathbf{g}\mathbf{h})=\frac{d\mathbf{g}}{dt}\mathbf{h}+\frac{d\mathbf{h}}{dt}\mathbf{g}$. The derivative is also called <em>tangent vector</em> sometimes. Note that the tangent vector depends on the selection of parameter $t$, which prevents us to obtain an invariant measure of the curve. We could instead study the arc length:
\(s=\int_{t_0}^t dt'|x'(t')|\)
moreover, we have $\Delta s=|\Delta x|+\mathcal{O}(|\Delta x|^2)$ (as an aside, $|\cdot|$ here means the length or norm of a vector), we then have $\frac{ds}{dt}=sign(|x’|)$, where $sign(x)$ means that we need to set $x$ to be positive or negative according to whether the direction increase $t$ or not. With $s$ as our invariant parameterisation, we are able to define the induced tangent vector as $\frac{dx}{ds}$, easy to verify that the norm of this tangent vector is always 1. Similarly, the ‘invariance’ induced by the arc-length could be applied into the curvature of the curve (the second order derivative) as $\kappa(x)=\frac{d^2 x}{ds^2}$. Now curves map a scalar parameter to a vector value $x\in\mathbb{R}^n$, the scalar field does the contrary: $\phi:\mathbb{R}^n\rightarrow \mathbb{R}$. Generally we can define the vector field as $\phi:\mathbb{R}^n\rightarrow \mathbb{R}^n$. Consider the scalar field $\phi$, we could access the derivatives by its gradient: 
\(\nabla \phi=\frac{\partial\phi}{\partial x^i}\mathbf{e}^i\)
Basicly we just take derivative of $\phi$ w.r.t every coordinate, and constitute a new vector.</p>

  <hr>
  <a href="#top">Back to top</a>
  
</div>
    </section>
  </main>
  <footer class="stickyfooter text-center">
  <small>
  <div class="small"></div>
    &copy; Qihang Wang, 2024
    |
    


  <a href="https://twitter.com/worles_enric" target="_blank" style="color: #1da1f2">
    <span class="bi bi-twitter"></span>
  </a>


  <a href="https://github.com/worlesenric" target="_blank" style="color: #000;">
    <span class="bi bi-github"></span>
  </a>


  <a href="https://www.linkedin.com/in/unavailable" target="_blank" style="color: #0077b5;">
    <span class="bi bi-linkedin"></span>
  </a>


  <a href="mailto:" target="_blank" style="color: #666">
    <span class="bi bi-envelope-open-fill"></span>
  </a>


  <a href="https://keybase.io/keybase" target="_blank" style="color: #FFA500">
    <span class="fa fa-keybase"></span>
  </a>


    |
    Powered by <a href="https://github.com/yak-fumblepack/jekyll-academic">Jekyll Academic Theme</a>
  </small>
</footer>
</body>
</html>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>