<!DOCTYPE html>
<html lang="en" name="top">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6" crossorigin="anonymous">
  
  <!-- Option 1: Bootstrap Bundle with Popper -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js" integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf" crossorigin="anonymous"></script>

  <!-- Bootstrap Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.4.1/font/bootstrap-icons.css">

  <!-- FontAwesome Icons -->
  <script src="https://kit.fontawesome.com/22ab6dd597.js" crossorigin="anonymous"></script>

  <!-- Highlight JS -->
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/atom-one-light.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
  
  
  <!-- Fork Awesom -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">

  <!-- ChartJS -->
  

  <!-- Mathjax support -->
  

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/styles.css">

  <!-- SEO -->
  <title>
    
      Large Language Models - Aspects and Methods
    
  </title>
  <link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" />
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Large Language Models - Aspects and Methods" />
<meta name="author" content="Qihang Wang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post is the note for reviewing the development of the Large Language Models (LLMs), for more detailed and rigorous text, please refer to the monographs." />
<meta property="og:description" content="This post is the note for reviewing the development of the Large Language Models (LLMs), for more detailed and rigorous text, please refer to the monographs." />
<link rel="canonical" href="http://localhost:4000/2024/04/13/introllm" />
<meta property="og:url" content="http://localhost:4000/2024/04/13/introllm" />
<meta property="og:site_name" content="Qihang’s site" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-04-13T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Large Language Models - Aspects and Methods" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Qihang Wang"},"dateModified":"2024-04-13T00:00:00+08:00","datePublished":"2024-04-13T00:00:00+08:00","description":"This post is the note for reviewing the development of the Large Language Models (LLMs), for more detailed and rigorous text, please refer to the monographs.","headline":"Large Language Models - Aspects and Methods","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2024/04/13/introllm"},"url":"http://localhost:4000/2024/04/13/introllm"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
  <main>
    <header><nav>
  <ul>
    <li>
      <a class="text-muted" href="/">
        
          Qihang&nbsp;Wang's
        
      </a>
    </li>
    <br />
    
    <li>
      <a href="/overview" >
      many thanks
      </a>
    </li>
    
    <li>
      <a href="/projects" >
      projects
      </a>
    </li>
    
    <li>
      <a href="/blog" >
      blog
      </a>
    </li>
    
    <li>
      <a href="/contact" >
      contact
      </a>
    </li>
    
  </ul>
</nav></header>
    <section>
      <div>
  <h1 style="color: #cc0000;">Large Language Models - Aspects and Methods</h1>
  <span>
  Tags:
    
      <a href="/tag/LLM">LLM</a>, 
    
      <a href="/tag/introduction">introduction</a>, 
    
      <a href="/tag/NLP">NLP</a>
    
  </span>
  <br />
  <br />
  <p>This post is the note for reviewing the development of the Large Language Models (LLMs), for more detailed and rigorous text, please refer to the monographs.</p>

<p><strong>Warning:</strong> This post should not serve as an introduction text to whom may not be familiar with the deep learning methodologies, since it contains intensive personal opinions, not assured to be correct.</p>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
  <li><a href="#table-of-contents">Table of Contents</a>
    <ul>
      <li><a href="#sequence-to-sequence-learning">Sequence-to-sequence Learning</a>
        <ul>
          <li><a href="#recurrent-neural-network-and-long-short-term-memory-network">Recurrent Neural Network and Long Short Term Memory Network</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="sequence-to-sequence-learning">Sequence-to-sequence Learning</h2>

<p>Despite the purpose of giving an introduction of neural language processing, we will start from reviewing  the machine translation (MT) task, for the simplicity  of the ideas – transfrom one sequence of words to another sequence of words (in some other language).  Note that in the original <a href="https://arxiv.org/pdf/1409.3215.pdf">paper</a> written by Ilya et al., the sequence to sequence learning could be a quite generalizable concept, below are several examples of sequences:</p>

<p><img style="display: block;" class="img-fluid" src="https://i.imgur.com/ZVfrUta.png" alt="seq2seq." /></p>
<p class="small">"Examples of sequences" by seq2seq ICML 17' tutorial</p>

<p>In my opinion, the motivation for developing the modern sequence to sequence learning methods is driven by the following two problems:</p>

<ul>
  <li>
    <p>The traditional ML methods rely on the consistent input dimensions. That is to say, the model only deals with the inputs that share a constant shape.</p>
  </li>
  <li>
    <p>Given the training pairs (each pair contains two sequences usually sampled from two domains), the task is essentially to match the distribution of the former to the latter. How do we model, evaluate or optimize the process?</p>
  </li>
</ul>

<p>The first problem drives the development of recurrent neural network (RNN), while the second problem drives the architecture design of the moderm seq2seq learning models. We now briefly introduce the three important models to support the further discussion, they are RNN, LSTM, and self-attention.</p>

<h3 id="recurrent-neural-network-and-long-short-term-memory-network">Recurrent Neural Network and Long Short Term Memory Network</h3>

<p>We would give a short introduction of the model structures and optimization techniques, the main focus of this section remains to be the discussion in the context of sequence-to-sequence learning.</p>

<p>In short, Recurrent Neural Network (RNN) is a kind of NN architectures in which the model computation graph is directed cyclic graph, while the Long Short Term Memory (LSTM) Network is a more complex version of the core network of RNN. Though sound intuitive, RNNs are proven to be powerful in many ways, e.g., <a href="https://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf">RNNs are Turing Complete</a>, <a href="http://www.vetta.org/documents/Machine_Super_Intelligence.pdf">RNNs are nearly intelligence-equivalent by approaching the asymptotic limit in text compression</a>. To make the following introduction non-trivial, we consider the predicting-next-character task, where the model is provided a sequence of characters and is required to predict the next character in the end of the sequence. (Note: The discussion here are mainly derived from the Ph.D <a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf">thesis</a> of Ilya Sutskever).</p>

<p><strong>RNN Formulation</strong>:</p>

<p>There are two perspectives on RNN, view it as a single network whose computation graph is cyclic, or view it as a super deep neural network with shared weights among the blocks (unrolled view).</p>

<p><img style="display: block;" class="img-fluid" src="https://i.imgur.com/ELw9Iu9.png" alt="RNN." /></p>
<p class="small">"An unrolled RNN" by colah's blog</p>

<p>My apology for the inconsistency, but we are going to use a slightly different notation from the figure. Let $i_t$ be the input at t-th moment, and $o_t$ be the output at t-th moment. Aparently RNN utilizes a hidden representation of the data, denoted by $h_t$. Now we define the input-output map as:</p>

\[o_t = g(h^o_t) \\
h^o_t = W_{oh}h_t + b_o \\
h_t = f(h^i_t) \\
h^i_t = W_{hi}i_t + W_{hh}h_{t-1} + b_h\]

<p>If you are familiar with the feed forward network, these are just two dense layers connected by the corresponding activation functions. The only difference is that the input layer takes the last moment’s state as part of the input, i.e., a combination of the input and the hidden state.</p>

<p>Before we discuss the LSTM network, we take a glance at the ackpropagation through time algorithm (BPTT). We first define the training loss function of RNN as the cumulative loss over time:</p>

\[\mathcal{L}(o, y) = \sum_{t=1}^T \mathcal{l}_t(o_t, y_t)\]

<p>Let’s calculate the partial derivative of the loss over $W_{hi}$ and $W_{hh}$, since the calculation w.r.t $W_{oh}$ is straight forward. We have the following:</p>

\[\frac{\partial \mathcal{L}}{\partial W_{hh}} = \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial W_{hh}} =  \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial o_t}\frac{\partial o_t}{\partial h^o_t}W_{oh}\frac{\partial h_t}{\partial h^i_t}\frac{\partial h^i_t}{\partial W_{hh}} \\
\frac{\partial \mathcal{L}}{\partial W_{hi}} = \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial W_{hi}} =  \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial o_t}\frac{\partial o_t}{\partial h^o_t}W_{oh}\frac{\partial h_t}{\partial h^i_t}\frac{\partial h^i_t}{\partial W_{hi}}\]

<p>Note here you cannot directly calculate that $\frac{\partial h^i_t}{\partial W_{hh}}=h_{t-1}$ since $h_{t-1} = h_{t-1}(\cdot, W_{hh})$, that’s because $W_{hh}$ is shared across the whole sequence. Instead, we view the $h_t(W_{hh})$ as the composition of functions $h_t(h_{t-1}(W_{hh}), h_{t-2}(W_{hh}), … h_{1}(W_{hh}))$, applying the chain rule on the partial derivative, we have:</p>

\[\frac{\partial h_t}{\partial W_{hh}}=\sum_{k=1}^t \frac{\partial h_{t}}{\partial h_{k}}\frac{\partial h_k}{W_{hh}}\]

  <hr>
  <a href="#top">Back to top</a>
  
</div>
    </section>
  </main>
  <footer class="stickyfooter text-center">
  <small>
  <div class="small"></div>
    &copy; Qihang Wang, 2024
    |
    


  <a href="https://twitter.com/worles_enric" target="_blank" style="color: #1da1f2">
    <span class="bi bi-twitter"></span>
  </a>


  <a href="https://github.com/worlesenric" target="_blank" style="color: #000;">
    <span class="bi bi-github"></span>
  </a>


  <a href="https://www.linkedin.com/in/unavailable" target="_blank" style="color: #0077b5;">
    <span class="bi bi-linkedin"></span>
  </a>


  <a href="mailto:" target="_blank" style="color: #666">
    <span class="bi bi-envelope-open-fill"></span>
  </a>


  <a href="https://keybase.io/keybase" target="_blank" style="color: #FFA500">
    <span class="fa fa-keybase"></span>
  </a>


    |
    Powered by <a href="https://github.com/yak-fumblepack/jekyll-academic">Jekyll Academic Theme</a>
  </small>
</footer>
</body>
</html>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>