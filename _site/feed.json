{
    "version": "https://jsonfeed.org/version/1",
    "title": "Qihang&apos;s site",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": "Here we go",
    "icon": "http://localhost:4000/apple-touch-icon.png",
    "favicon": "http://localhost:4000/favicon.ico",
    "expired": false,
    
    "author":  {
        "name": "Qihang Wang",
        "url": null,
        "avatar": null
    },
    
"items": [
    
        {
            "id": "http://localhost:4000/2024/04/15/finetunellm",
            "title": "Finetuning Your LLM - My Workflow",
            "summary": null,
            "content_text": "",
            "content_html": "",
            "url": "http://localhost:4000/2024/04/15/finetunellm",
            
            
            
            "tags": ["LLM","workflow","llama"],
            
            "date_published": "2024-04-15T00:00:00+08:00",
            "date_modified": "2024-04-15T00:00:00+08:00",
            
                "author":  {
                "name": "Qihang Wang",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/04/14/ubuntuinstallation",
            "title": "DL Development Environment 1 > System Installation and Configuration",
            "summary": null,
            "content_text": "Table of Contents  Table of Contents          OS Installation                   Preparing Ubuntu Image                     OS Configuration                  GPU Related Configuration                     OS Installation You need:      A laptop or PC with at least 25GB of storage space.        A flash drive (12GB or above recommended).  Preparing Ubuntu Image Download the image from here. We use the Ubuntu 22.04 LTS for this guide.Staying consistent to the official tutorial, we use balenaEtcher to create the bootable USB stick. Do the following on balenaEtcher:  Select your downloaded ISO  Choose your USB flash drive  Click FlashYou may need to modify the boot option, for this, you may hold F12 while the computer starts.  Notice: If you encounter the black screen after installed the Ubuntu via try or install ubuntu. Here is the solution:      Press e to enter the edit mode when the screen shows try or install ubuntu.    Find ...quiet splash ---, delete the --- and add the nomodeset in front of quiet splash: ...nomodeset quiet splash.    Then press enter, you should enter the installation interface after several seconds of black wait.    If you still encounter the problem after installation. Please do the same when you restart the machine. (remember to boot from your ubuntu disk insdead of the flash disk)    If you successfully login, edit /etc/default/grub with sudo, find GRUB_CMDLINE_LINUX_DEFAULT=quiet splash, change it to GRUB_CMDLINE_LINUX_DEFAULT=quiet splash nomodeset.  Setting up Ubuntu installation:  Computer name: Name-PC  Name: Name  User name: name  Password:OS ConfigurationRef: Ubuntu 22.04 for DLYou may update Ubuntu before further operations:$ sudo apt update$ sudo apt full-upgrade --yes$ sudo apt autoremove --yes$ sudo apt autoclean --yes$ rebootYou can create a full-update script ~/full-update.sh to pack these operations:#!/usr/bin/env bashif [ \"$EUID\" -ne 0 ]  then echo \"Error: Please run as root.\"  exitficlearecho \"################################################################################\"echo \"Updating list of available packages...\"echo \"--------------------------------------------------------------------------------\"apt updateecho \"################################################################################\"echoecho \"################################################################################\"echo \"Upgrading the system by removing/installing/upgrading packages...\"echo \"--------------------------------------------------------------------------------\"apt full-upgrade --yesecho \"################################################################################\"echoecho \"################################################################################\"echo \"Removing automatically all unused packages...\"echo \"--------------------------------------------------------------------------------\"apt autoremove --yesecho \"################################################################################\"echoecho \"################################################################################\"echo \"Clearing out the local repository of retrieved package files...\"echo \"--------------------------------------------------------------------------------\"apt autoclean --yesecho \"################################################################################\"echoInstall the Chrome web browser by (get the deb file here):$ sudo dpkg -i google-chrome-stable_current_amd64.debInstall the developement tools by:$ sudo apt install build-essential pkg-config cmake cmake-qt-gui ninja-build valgrindInstall Python3 and venv by:$ sudo apt install python3 python3-wheel python3-pip python3-venv python3-dev python3-setuptoolsInstall Git by:$ sudo apt install git$ git config --global user.name \"Name\"$ git config --global user.email \"name@domain.com\"$ git config --global core.editor \"gedit -s\"GPU Related Configuration Now it comes to the steps for setting up NVIDIA toolkits. The process here may not align with your situation, check the NVIDIA official toturial whenever there are mistakes.Check the display hardware by:$ sudo lshw -C displayCheck CUDA and NVIDIA Driver Compatibilities here.Check TensorFlow and CUDA Compatibilities here and here.Check Torch and CUDA Campatibilities here.Here we use CUDA 11.8, install the NVIDIA Driver by:$ sudo apt install nvidia-driver-535Install the prerequisites:$ sudo apt install linux-headers-$(uname -r)Download CUDA 11.8 via:$ wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.runInstall CUDA 11.8 by (select without driver):$ sudo ./cuda_11.8.0_520.61.05_linux.run --overrideSetting up the environment variables:export PATH=$PATH:/usr/local/cuda-11.8/binexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.8/extras/CUPTI/lib64Test by:$ nvidia-smi$ NVCC -vInstall cuDNN v8.6 for CUDA 11.8. Login and download the deb file hereThen install by:$ sudo dpkg -i cudnn-local-repo-ubuntu2204-8.6.0.163_1.0-1_amd64.deb$ sudo cp /var/cudnn-local-repo-ubuntu2204-8.6.0.163/cudnn-local-FAED14DD-keyring.gpg /usr/share/keyrings/$ sudo apt update$ sudo apt install libcudnn8$ sudo apt install libcudnn8-dev$ sudo apt install libcudnn8-samplesThen reboot by sudo reboot",
            "content_html": "<h1 id=\"table-of-contents\">Table of Contents</h1><ul>  <li><a href=\"#table-of-contents\">Table of Contents</a>    <ul>      <li><a href=\"#os-installation-\">OS Installation </a>        <ul>          <li><a href=\"#preparing-ubuntu-image-\">Preparing Ubuntu Image </a></li>        </ul>      </li>      <li><a href=\"#os-configuration\">OS Configuration</a>        <ul>          <li><a href=\"#gpu-related-configuration-\">GPU Related Configuration </a></li>        </ul>      </li>    </ul>  </li></ul><h2 id=\"os-installation-\">OS Installation <a name=\"osinstallation\"></a></h2><p>You need:</p><ul>  <li>    <p>A laptop or PC with at least 25GB of storage space.</p>  </li>  <li>    <p>A flash drive (12GB or above recommended).</p>  </li></ul><h3 id=\"preparing-ubuntu-image-\">Preparing Ubuntu Image <a name=\"preparation\"></a></h3><p>Download the image from <a href=\"https://ubuntu.com/download/desktop\">here</a>. We use the Ubuntu 22.04 LTS for this guide.</p><p>Staying consistent to the official tutorial, we use <a href=\"https://etcher.balena.io/\">balenaEtcher</a> to create the bootable USB stick. Do the following on balenaEtcher:</p><ul>  <li>Select your downloaded ISO</li>  <li>Choose your USB flash drive</li>  <li>Click Flash</li></ul><p>You may need to modify the boot option, for this, you may hold <code>F12</code> while the computer starts.</p><blockquote>  <p>Notice: If you encounter the black screen after installed the Ubuntu via <code>try or install ubuntu</code>. Here is the solution:</p>  <ul>    <li>Press <code>e</code> to enter the edit mode when the screen shows <code>try or install ubuntu</code>.</li>    <li>Find <code>...quiet splash ---</code>, delete the <code>---</code> and add the <code>nomodeset</code> in front of <code>quiet splash</code>: <code>...nomodeset quiet splash</code>.</li>    <li>Then press <code>enter</code>, you should enter the installation interface after several seconds of black wait.</li>    <li>If you still encounter the problem after installation. Please do the same when you restart the machine. (remember to boot from your ubuntu disk insdead of the flash disk)</li>    <li>If you successfully login, edit <code>/etc/default/grub</code> with <code>sudo</code>, find <code>GRUB_CMDLINE_LINUX_DEFAULT=</code>quiet splash<code>, change it to </code>GRUB_CMDLINE_LINUX_DEFAULT=<code>quiet splash nomodeset</code>.</li>  </ul></blockquote><p>Setting up Ubuntu installation:</p><ul>  <li>Computer name: Name-PC</li>  <li>Name: Name</li>  <li>User name: name</li>  <li>Password:</li></ul><h2 id=\"os-configuration\">OS Configuration</h2><p>Ref: <a href=\"https://gist.github.com/amir-saniyan/b3d8e06145a8569c0d0e030af6d60bea\">Ubuntu 22.04 for DL</a></p><p>You may update Ubuntu before further operations:</p><pre><code class=\"language-shell\">$ sudo apt update$ sudo apt full-upgrade --yes$ sudo apt autoremove --yes$ sudo apt autoclean --yes$ reboot</code></pre><p>You can create a full-update script <code>~/full-update.sh</code> to pack these operations:</p><pre><code class=\"language-shell\">#!/usr/bin/env bashif [ \"$EUID\" -ne 0 ]  then echo \"Error: Please run as root.\"  exitficlearecho \"################################################################################\"echo \"Updating list of available packages...\"echo \"--------------------------------------------------------------------------------\"apt updateecho \"################################################################################\"echoecho \"################################################################################\"echo \"Upgrading the system by removing/installing/upgrading packages...\"echo \"--------------------------------------------------------------------------------\"apt full-upgrade --yesecho \"################################################################################\"echoecho \"################################################################################\"echo \"Removing automatically all unused packages...\"echo \"--------------------------------------------------------------------------------\"apt autoremove --yesecho \"################################################################################\"echoecho \"################################################################################\"echo \"Clearing out the local repository of retrieved package files...\"echo \"--------------------------------------------------------------------------------\"apt autoclean --yesecho \"################################################################################\"echo</code></pre><p>Install the Chrome web browser by (get the deb file <a href=\"https://www.google.com/chrome/\">here</a>):</p><pre><code class=\"language-shell\">$ sudo dpkg -i google-chrome-stable_current_amd64.deb</code></pre><p>Install the developement tools by:</p><pre><code class=\"language-shell\">$ sudo apt install build-essential pkg-config cmake cmake-qt-gui ninja-build valgrind</code></pre><p>Install Python3 and venv by:</p><pre><code class=\"language-shell\">$ sudo apt install python3 python3-wheel python3-pip python3-venv python3-dev python3-setuptools</code></pre><p>Install Git by:</p><pre><code class=\"language-shell\">$ sudo apt install git$ git config --global user.name \"Name\"$ git config --global user.email \"name@domain.com\"$ git config --global core.editor \"gedit -s\"</code></pre><h3 id=\"gpu-related-configuration-\">GPU Related Configuration <a name=\"GPU\"></a></h3><p>Now it comes to the steps for setting up NVIDIA toolkits. The process here may not align with your situation, check the NVIDIA official toturial whenever there are mistakes.</p><p>Check the display hardware by:</p><pre><code class=\"language-shell\">$ sudo lshw -C display</code></pre><p><strong>Check CUDA and NVIDIA Driver Compatibilities</strong> <a href=\"https://docs.nvidia.com/deeplearning/cudnn/reference/support-matrix.html\">here</a>.</p><p><strong>Check TensorFlow and CUDA Compatibilities</strong> <a href=\"https://www.tensorflow.org/install/gpu\">here</a> and <a href=\"https://www.tensorflow.org/install/source#gpu\">here</a>.</p><p><strong>Check Torch and CUDA Campatibilities</strong> <a href=\"https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix\">here</a>.</p><p>Here we use CUDA 11.8, install the NVIDIA Driver by:</p><pre><code class=\"language-shell\">$ sudo apt install nvidia-driver-535</code></pre><p>Install the prerequisites:</p><pre><code class=\"language-shell\">$ sudo apt install linux-headers-$(uname -r)</code></pre><p>Download CUDA 11.8 via:</p><pre><code class=\"language-shell\">$ wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run</code></pre><p>Install CUDA 11.8 by (select without driver):</p><pre><code class=\"language-shell\">$ sudo ./cuda_11.8.0_520.61.05_linux.run --override</code></pre><p>Setting up the environment variables:</p><pre><code class=\"language-shell\">export PATH=$PATH:/usr/local/cuda-11.8/binexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.8/extras/CUPTI/lib64</code></pre><p>Test by:</p><pre><code class=\"language-shell\">$ nvidia-smi$ NVCC -v</code></pre><p>Install cuDNN v8.6 for CUDA 11.8. Login and download the deb file <a href=\"https://developer.nvidia.com/compute/cudnn/secure/8.6.0/local_installers/11.8/cudnn-local-repo-ubuntu2204-8.6.0.163_1.0-1_amd64.deb\">here</a></p><p>Then install by:</p><pre><code class=\"language-shell\">$ sudo dpkg -i cudnn-local-repo-ubuntu2204-8.6.0.163_1.0-1_amd64.deb$ sudo cp /var/cudnn-local-repo-ubuntu2204-8.6.0.163/cudnn-local-FAED14DD-keyring.gpg /usr/share/keyrings/$ sudo apt update$ sudo apt install libcudnn8$ sudo apt install libcudnn8-dev$ sudo apt install libcudnn8-samples</code></pre><p>Then reboot by <code>sudo reboot</code></p>",
            "url": "http://localhost:4000/2024/04/14/ubuntuinstallation",
            
            
            
            "tags": ["linux","installation"],
            
            "date_published": "2024-04-14T00:00:00+08:00",
            "date_modified": "2024-04-14T00:00:00+08:00",
            
                "author":  {
                "name": "Qihang Wang",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/04/14/softwares",
            "title": "DL Development Environment 2 > Tools You May Need",
            "summary": null,
            "content_text": "Table of Contents  Table of Contents          Source Mirrors                   Ubuntu Sources (22.04 LTS jammy)          PyPI          Docker CE          Dockerhub Mirror                    Proxy       Visual Studio Code       Source Mirrors For users who experience the low connection qualities on certain software sources, you may consider using the source mirrors. Here we record the operations using Tsinghua Open Source Mirror (TUNA).Ubuntu Sources (22.04 LTS jammy)Replace the content of /etc/apt/sources.list with:deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse# deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiversedeb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse# deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiversedeb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse# deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiversedeb http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse# deb-src http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse# deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse# # deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiversePyPIFor the temporary usage, you may directly use:pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-packageSet as default, you may use:python -m pip install --upgrade pip# use the below if you cannot upgrade# python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pippip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simpleDocker CEYou can automatically install docker via:export DOWNLOAD_URL=\"http://mirrors.tuna.tsinghua.edu.cn/docker-ce\"# For curlcurl -fsSL https://get.docker.com/ | sh# for wgetwget -O- https://get.docker.com/ | shOR, you can install mannually. The following commands are for Ubuntu, you can change them according to the instructions here. Uninstall the old version (if there is):for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do apt-get remove $pkg; doneThen install the dependencies:apt-get updateapt-get install ca-certificates curl gnupgAdd the repo:install -m 0755 -d /etc/apt/keyringscurl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpgsudo chmod a+r /etc/apt/keyrings/docker.gpgecho \\  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] http://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu \\  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\  tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullInstall the docker:apt-get updateapt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-pluginDockerhub MirrorYou can use Aliyun container mirror service to accelerate the docker pull xxx from docker hub.Proxy For the similar reasons, you may want to equip your environment with a proxy. This blog shows the utilization of clash for linux for this purpose. You may find other solutions via google.Download the clash via:$ git clone https://github.com/wanhebin/clash-for-linux.gitEnter the directory cd clash-for-linux and edit CLASH_URL by vim .env.Start the service via:$ sudo bash start.sh# In a new terminal$ source /etc/profile.d/clash.sh$ proxy_onYou may check if the service is correctly started:$ netstat -tln | grep -E '9090|789.'tcp        0      0 127.0.0.1:9090          0.0.0.0:*               LISTEN     tcp6       0      0 :::7890                 :::*                    LISTEN     tcp6       0      0 :::7891                 :::*                    LISTEN     tcp6       0      0 :::7892                 :::*   $ env | grep -E 'http_proxy|https_proxy'http_proxy=http://127.0.0.1:7890https_proxy=http://127.0.0.1:7890                 If you wang to shutdown or restart (maybe for updating configurations):#shuting downsudo bash shutdown.shproxy_off#restartingsudo bash restart.shclash dashboard is by default hosted on http://192.168.0.1:9090/.Visual Studio Code Download the VSCode client here, then install via sudo apt install ./&lt;file&gt;.deb.",
            "content_html": "<h1 id=\"table-of-contents\">Table of Contents</h1><ul>  <li><a href=\"#table-of-contents\">Table of Contents</a>    <ul>      <li><a href=\"#source-mirrors-\">Source Mirrors </a>        <ul>          <li><a href=\"#ubuntu-sources-2204-lts-jammy\">Ubuntu Sources (22.04 LTS jammy)</a></li>          <li><a href=\"#pypi\">PyPI</a></li>          <li><a href=\"#docker-ce\">Docker CE</a></li>          <li><a href=\"#dockerhub-mirror\">Dockerhub Mirror</a></li>        </ul>      </li>      <li><a href=\"#proxy-\">Proxy </a></li>      <li><a href=\"#visual-studio-code-\">Visual Studio Code </a></li>    </ul>  </li></ul><h2 id=\"source-mirrors-\">Source Mirrors <a name=\"mirror\"></a></h2><p>For users who experience the low connection qualities on certain software sources, you may consider using the source mirrors. Here we record the operations using Tsinghua Open Source Mirror (TUNA).</p><h3 id=\"ubuntu-sources-2204-lts-jammy\">Ubuntu Sources (22.04 LTS jammy)</h3><p>Replace the content of <code>/etc/apt/sources.list</code> with:</p><pre><code class=\"language-shell\">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse# deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiversedeb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse# deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiversedeb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse# deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiversedeb http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse# deb-src http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse# deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse# # deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse</code></pre><h3 id=\"pypi\">PyPI</h3><p>For the temporary usage, you may directly use:<code>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package</code></p><p>Set as default, you may use:</p><pre><code class=\"language-shell\">python -m pip install --upgrade pip# use the below if you cannot upgrade# python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pippip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></pre><h3 id=\"docker-ce\">Docker CE</h3><p>You can automatically install docker via:</p><pre><code class=\"language-shell\">export DOWNLOAD_URL=\"http://mirrors.tuna.tsinghua.edu.cn/docker-ce\"# For curlcurl -fsSL https://get.docker.com/ | sh# for wgetwget -O- https://get.docker.com/ | sh</code></pre><p>OR, you can install mannually. The following commands are for Ubuntu, you can change them according to the instructions <a href=\"https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/\">here</a>. Uninstall the old version (if there is):</p><pre><code class=\"language-shell\">for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do apt-get remove $pkg; done</code></pre><p>Then install the dependencies:</p><pre><code class=\"language-shell\">apt-get updateapt-get install ca-certificates curl gnupg</code></pre><p>Add the repo:</p><pre><code class=\"language-shell\">install -m 0755 -d /etc/apt/keyringscurl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpgsudo chmod a+r /etc/apt/keyrings/docker.gpgecho \\  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] http://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu \\  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\  tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</code></pre><p>Install the docker:</p><pre><code class=\"language-shell\">apt-get updateapt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</code></pre><h3 id=\"dockerhub-mirror\">Dockerhub Mirror</h3><p>You can use Aliyun container mirror service to accelerate the <code>docker pull xxx</code> from docker hub.</p><h2 id=\"proxy-\">Proxy <a name=\"proxy\"></a></h2><p>For the similar reasons, you may want to equip your environment with a proxy. This blog shows the utilization of <a href=\"https://github.com/wnlen/clash-for-linux\">clash for linux</a> for this purpose. You may find other solutions via google.</p><p>Download the clash via:</p><pre><code class=\"language-shell\">$ git clone https://github.com/wanhebin/clash-for-linux.git</code></pre><p>Enter the directory <code>cd clash-for-linux</code> and edit <code>CLASH_URL</code> by <code>vim .env</code>.</p><p>Start the service via:</p><pre><code class=\"language-shell\">$ sudo bash start.sh# In a new terminal$ source /etc/profile.d/clash.sh$ proxy_on</code></pre><p>You may check if the service is correctly started:</p><pre><code class=\"language-shell\">$ netstat -tln | grep -E '9090|789.'tcp        0      0 127.0.0.1:9090          0.0.0.0:*               LISTEN     tcp6       0      0 :::7890                 :::*                    LISTEN     tcp6       0      0 :::7891                 :::*                    LISTEN     tcp6       0      0 :::7892                 :::*   $ env | grep -E 'http_proxy|https_proxy'http_proxy=http://127.0.0.1:7890https_proxy=http://127.0.0.1:7890                 </code></pre><p>If you wang to shutdown or restart (maybe for updating configurations):</p><pre><code class=\"language-shell\">#shuting downsudo bash shutdown.shproxy_off#restartingsudo bash restart.sh</code></pre><p>clash dashboard is by default hosted on <code>http://192.168.0.1:9090/</code>.</p><h2 id=\"visual-studio-code-\">Visual Studio Code <a name=\"vscode\"></a></h2><p>Download the VSCode client <a href=\"https://code.visualstudio.com/\">here</a>, then install via <code>sudo apt install ./&lt;file&gt;.deb</code>.</p>",
            "url": "http://localhost:4000/2024/04/14/softwares",
            
            
            
            "tags": ["linux","installation"],
            
            "date_published": "2024-04-14T00:00:00+08:00",
            "date_modified": "2024-04-14T00:00:00+08:00",
            
                "author":  {
                "name": "Qihang Wang",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/04/14/dllibs",
            "title": "DL Development Environment 3 > Libararies You May Need",
            "summary": null,
            "content_text": "Table of Contents  Table of Contents          ML libararies       Pytorch       Tensorflow        ML libararies Prepare the common ML libs (you can also access them via anaconda)$ python3 -m venv ~/venvs/ml$ source ~/venvs/ml/bin/activate(ml) $ pip install --upgrade pip setuptools wheel(ml) $ pip install --upgrade numpy scipy matplotlib ipython jupyter pandas sympy nose(ml) $ pip install --upgrade scikit-learn scikit-image(ml) $ deactivatePytorch Pytorch CPU:$ python3 -m venv ~/venvs/torchcpu$ source ~/venvs/torchcpu/bin/activate(torchcpu) $ pip install --upgrade pip setuptools wheel(torchcpu) $ pip install --upgrade opencv-python opencv-contrib-python(torchcpu) $ pip install --upgrade torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu(torchcpu) $ deactivatePytorch GPU (ALERT: Check the CUDA and cuDNN and NVIDIA drive version Compatibilities here, and use the correct downloading name to repalce the ones below):$ python3 -m venv ~/venvs/torchgpu$ source ~/venvs/torchgpu/bin/activate(torchgpu) $ pip install --upgrade pip setuptools wheel(torchgpu) $ pip install --upgrade opencv-python opencv-contrib-python(torchgpu) $ pip install --upgrade torch torchvision torchaudio(torchgpu) $ deactivateAfter installation, you may check the availability:$ source ~/venvs/torchgpu/bin/activate(torchgpu) $ python&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()&gt;&gt;&gt; exit()(torchgpu) $ deactivateTensorflow  Tensorflow CPU:$ python3 -m venv ~/venvs/tfcpu$ source ~/venvs/tfcpu/bin/activate(tfcpu) $ pip install --upgrade pip setuptools wheel(tfcpu) $ pip install --upgrade opencv-python opencv-contrib-python(tfcpu) $ pip install --upgrade tensorflow-cpu tensorboard keras(tfcpu) $ deactivateTensorflow GPU (similar to Pytorch, check the official tutorial)$ python3 -m venv ~/venvs/tfgpu$ source ~/venvs/tfgpu/bin/activate(tfgpu) $ pip install --upgrade pip setuptools wheel(tfgpu) $ pip install --upgrade opencv-python opencv-contrib-python(tfgpu) $ pip install --upgrade tensorflow tensorboard keras(tfgpu) $ deactivate",
            "content_html": "<h1 id=\"table-of-contents\">Table of Contents</h1><ul>  <li><a href=\"#table-of-contents\">Table of Contents</a>    <ul>      <li><a href=\"#ml-libararies-\">ML libararies </a></li>      <li><a href=\"#pytorch-\">Pytorch </a></li>      <li><a href=\"#tensorflow--\">Tensorflow  </a></li>    </ul>  </li></ul><h2 id=\"ml-libararies-\">ML libararies <a name=\"mllibs\"></a></h2><p>Prepare the common ML libs (you can also access them via anaconda)</p><pre><code class=\"language-shell\">$ python3 -m venv ~/venvs/ml$ source ~/venvs/ml/bin/activate(ml) $ pip install --upgrade pip setuptools wheel(ml) $ pip install --upgrade numpy scipy matplotlib ipython jupyter pandas sympy nose(ml) $ pip install --upgrade scikit-learn scikit-image(ml) $ deactivate</code></pre><h2 id=\"pytorch-\">Pytorch <a name=\"pytorch\"></a></h2><p>Pytorch CPU:</p><pre><code class=\"language-shell\">$ python3 -m venv ~/venvs/torchcpu$ source ~/venvs/torchcpu/bin/activate(torchcpu) $ pip install --upgrade pip setuptools wheel(torchcpu) $ pip install --upgrade opencv-python opencv-contrib-python(torchcpu) $ pip install --upgrade torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu(torchcpu) $ deactivate</code></pre><p>Pytorch GPU (<strong>ALERT</strong>: Check the CUDA and cuDNN and NVIDIA drive version Compatibilities <a href=\"https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix\">here</a>, and use the correct downloading name to repalce the ones below):</p><pre><code class=\"language-shell\">$ python3 -m venv ~/venvs/torchgpu$ source ~/venvs/torchgpu/bin/activate(torchgpu) $ pip install --upgrade pip setuptools wheel(torchgpu) $ pip install --upgrade opencv-python opencv-contrib-python(torchgpu) $ pip install --upgrade torch torchvision torchaudio(torchgpu) $ deactivate</code></pre><p>After installation, you may check the availability:</p><pre><code class=\"language-shell\">$ source ~/venvs/torchgpu/bin/activate(torchgpu) $ python&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()&gt;&gt;&gt; exit()(torchgpu) $ deactivate</code></pre><h2 id=\"tensorflow--\">Tensorflow  <a name=\"pytorch\"></a></h2><p>Tensorflow CPU:</p><pre><code class=\"language-shell\">$ python3 -m venv ~/venvs/tfcpu$ source ~/venvs/tfcpu/bin/activate(tfcpu) $ pip install --upgrade pip setuptools wheel(tfcpu) $ pip install --upgrade opencv-python opencv-contrib-python(tfcpu) $ pip install --upgrade tensorflow-cpu tensorboard keras(tfcpu) $ deactivate</code></pre><p>Tensorflow GPU (similar to Pytorch, check the official <a href=\"https://www.tensorflow.org/install/gpu\">tutorial</a>)</p><pre><code class=\"language-shell\">$ python3 -m venv ~/venvs/tfgpu$ source ~/venvs/tfgpu/bin/activate(tfgpu) $ pip install --upgrade pip setuptools wheel(tfgpu) $ pip install --upgrade opencv-python opencv-contrib-python(tfgpu) $ pip install --upgrade tensorflow tensorboard keras(tfgpu) $ deactivate</code></pre>",
            "url": "http://localhost:4000/2024/04/14/dllibs",
            
            
            
            "tags": ["linux","installation"],
            
            "date_published": "2024-04-14T00:00:00+08:00",
            "date_modified": "2024-04-14T00:00:00+08:00",
            
                "author":  {
                "name": "Qihang Wang",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/04/13/introllm",
            "title": "Large Language Models - Aspects and Methods",
            "summary": null,
            "content_text": "This post is the note for reviewing the development of the Large Language Models (LLMs), for more detailed and rigorous text, please refer to the monographs.Warning: This post should not serve as an introduction text to whom may not be familiar with the deep learning methodologies, since it contains intensive personal opinions, not assured to be correct.Table of Contents  Table of Contents          Sequence-to-sequence Learning via RNN                  Recurrent Neural Network and Long Short Term Memory Network          Encoder-Decoder Architecture          Self-Attention and Transformer                    Sequence-to-sequence Learning via RNNDespite the purpose of giving an introduction of neural language processing, we will start from reviewing  the machine translation (MT) task, for the simplicity  of the ideas – transfrom one sequence of words to another sequence of words (in some other language).  Note that in the original paper written by Ilya et al., the sequence to sequence learning could be a quite generalizable concept, below are several examples of sequences:\"Examples of sequences\" by seq2seq ICML 17' tutorialIn my opinion, the motivation for developing the modern sequence to sequence learning methods is driven by the following two problems:      The traditional ML methods rely on the consistent input dimensions. That is to say, the model only deals with the inputs that share a constant shape.        Given the training pairs (each pair contains two sequences usually sampled from two domains), the task is essentially to match the distribution of the former to the latter. How do we model, evaluate or optimize the process?  The first problem drives the development of recurrent neural network (RNN), while the second problem drives the architecture design of the moderm seq2seq learning models. We now briefly introduce the three important models to support the further discussion, they are RNN, LSTM, and self-attention.Recurrent Neural Network and Long Short Term Memory NetworkWe would give a short introduction of the model structures and optimization techniques, the main focus of this section remains to be the discussion in the context of sequence-to-sequence learning.In short, Recurrent Neural Network (RNN) is a kind of NN architectures in which the model computation graph is directed cyclic graph, while the Long Short Term Memory (LSTM) Network is a more complex version of the core network of RNN. Though sound intuitive, RNNs are proven to be powerful in many ways, e.g., RNNs are Turing Complete, RNNs are nearly intelligence-equivalent by approaching the asymptotic limit in text compression. To make the following introduction non-trivial, we consider the predicting-next-character task, where the model is provided a sequence of characters and is required to predict the next character in the end of the sequence. (Note: The discussion here are mainly derived from the Ph.D thesis of Ilya Sutskever).RNN Formulation:There are two perspectives on RNN, view it as a single network whose computation graph is cyclic, or view it as a super deep neural network with shared weights among the blocks (unrolled view).\"An unrolled RNN\" by colah's blogMy apology for the inconsistency, but we are going to use a slightly different notation from the figure. Let $i_t$ be the input at t-th moment, and $o_t$ be the output at t-th moment. Aparently RNN utilizes a hidden representation of the data, denoted by $h_t$. Now we define the input-output map as:\\[o_t = g(h^o_t) \\\\h^o_t = W_{oh}h_t + b_o \\\\h_t = f(h^i_t) \\\\h^i_t = W_{hi}i_t + W_{hh}h_{t-1} + b_h\\]If you are familiar with the feed forward network, these are just two dense layers connected by the corresponding activation functions. The only difference is that the input layer takes the last moment’s state as part of the input, i.e., a combination of the input and the hidden state.Before we discuss the LSTM network, we take a glance at the ackpropagation through time algorithm (BPTT). We first define the training loss function of RNN as the cumulative loss over time:\\[\\mathcal{L}(o, y) = \\sum_{t=1}^T \\mathcal{l}_t(o_t, y_t)\\]Let’s calculate the partial derivative of the loss over $W_{hi}$ and $W_{hh}$, since the calculation w.r.t $W_{oh}$ is straight forward. We have the following:\\[\\frac{\\partial \\mathcal{L}}{\\partial W_{hh}} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{l}_t}{\\partial W_{hh}} =  \\sum_{t=1}^T \\frac{\\partial \\mathcal{l}_t}{\\partial o_t}\\frac{\\partial o_t}{\\partial h^o_t}W_{oh}\\frac{\\partial h_t}{\\partial h^i_t}\\frac{\\partial h^i_t}{\\partial W_{hh}} \\\\\\frac{\\partial \\mathcal{L}}{\\partial W_{hi}} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{l}_t}{\\partial W_{hi}} =  \\sum_{t=1}^T \\frac{\\partial \\mathcal{l}_t}{\\partial o_t}\\frac{\\partial o_t}{\\partial h^o_t}W_{oh}\\frac{\\partial h_t}{\\partial h^i_t}\\frac{\\partial h^i_t}{\\partial W_{hi}}\\]Note here you cannot directly calculate that $\\frac{\\partial h^i_t}{\\partial W_{hh}}=h_{t-1}$ since $h_{t-1} = h_{t-1}(\\cdot, W_{hh})$, that’s because $W_{hh}$ is shared across the whole sequence. Instead, we apply the chain rule on the partial derivative of the multiplication $W_{hh}h^{t-1}$, we have:\\(\\frac{\\partial h_t}{\\partial W_{hh}}=\\frac{\\partial h_t}{\\partial h^i_t}(h_{t-1}+W_{hh}\\frac{\\partial h_{t-1}}{\\partial W_{hh}})=\\frac{\\partial h_t}{\\partial h^i_t}(h_{t-1}+W_{hh}(\\frac{\\partial h_{t-1}}{\\partial h^i_{t-1}}(h_{t-2}+W_{hh}\\frac{\\partial h_{t-2}}{\\partial W_{hh}}))))= ...\\)Following the chain rule, we obtain the following:\\(\\frac{\\partial h_t}{\\partial W_{hh}} = \\sum_{j=0}^{t-1} \\left(\\prod_{k=0}^{j}\\frac{\\partial h_{t-k}}{\\partial h^i_{t-k}}W_{hh}\\right)h_{t-j-1}\\)The above computation intuitively not stable, since the high order matrix multiplication is introduced. The resulted training process would be in face of potential gradient explosion and gradient vanishing. A straight forward solution is to truncate the historical computation with a relatively short window, and $j$ in the formula would start from $t-w$, where $w$ is the window size. Apparently we lose the long-term learning ability immediately, and that’s where the LSTM network helps. You may have noticed that one of the most apparent problems of vanila RNN is that it encodes all the historical (and any potential future) information into one matrix, which may align with the intuition of our observations on the humans, yet failed to be effectively trained with gradient descent.  Note: You can try to enhance RNN to mitigate these problems, sometimes it’s just a problem of imagination.LSTM network tries to solve this problem with an architecture that splits the knowledge representation into two parts: The hidden state $h$ and the memory $C$. The good thing about $C$ is $\\frac{\\partial C_t}{\\partial C_{t-1}}=1$. How would this help? Let’s look at RNN and BPTT in an abstract view. Say, we regard the recurrent network as extracting a knowledge representation $W$ from data. So what we need to do is to define an interface of the knowledge representation $h$, a data to interface transformation $i$, and a interface to data transformation $o$. So the learning process could be defined as:\\(\\min \\mathbb{E}_{x,y\\sim P(x,y)}[\\mathcal{L}(o(x), y)]\\\\o(x)=o(h(i(x),W))\\)This looks like any ML objectives, for the data structure of sequences, we model the data as $x=[x_1, x_2, …x_T]$, then for RNN, we could process the data segment by segment. Now $i(x)$ is just any neural layer operation $\\phi_i (W_ix+b_i)$. The point is that we model $h$ as $h = [h_1, h_2, … h_t]$, while $h_t=W_{hh}h_{t-1}+W_{ih}x_t+b_h=W[h_{t-1}, x_t, 1]$, and $o=\\phi_o(h)$, then we get the formulation of RNN. The gradient calculation follows:\\(\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial \\mathcal{L_t}}{\\partial W} = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial \\mathcal{L_t}}{\\partial o_t}\\frac{\\partial o_t}{\\partial h_t}\\sum_{k=1}^t \\prod_{j=k+1}^t\\left(\\frac{\\partial h_j}{\\partial h_{j-1}}\\right)\\frac{\\partial h_k}{\\partial W}\\)“On the difficulty of training recurrent neural networks” pointed out that, for some constant $\\gamma$:\\(\\prod_{j=k+1}^t\\frac{\\partial h_j}{\\partial h_{j-1}} = \\prod_{j=k+1}^t diag(\\frac{\\partial h_j}{\\partial h^i_{j-1}})\\frac{\\partial h^i_{j-1}}{\\partial h_{j-1}}\\approx \\left\\{ \\begin{array}{rcl}&amp; 0\\quad \\text{for }max(eigen(diag))&lt;\\frac{1}{\\gamma} \\\\&amp; \\infty\\quad \\text{for }max(eigen(diag))&gt;\\frac{1}{\\gamma}\\end{array}\\right.\\)The interesting part is that now we can see that the forward calculation already involved with the $W_{hh}^t$. Of course if we model $i(x)$ as a direct map from $x$ to $h$, the problem is solved, but imagin how big the space of $x$ is, for example $x$ represents an arbitrary sentence written in English. So for now we still model $x$ as a sequence decomposition $[x_1, x_2, …x_T]$, and see if there are better modelling for $h$. Now let’s take a look at LSTM network.\"LSTM network\" by colah's blogThis needs a few explanations, so the upper horizontal line in the figure denotes the memory $C$ and the lower horizontal line denotes the hidden state $h$. The calculation follows (from the left vertical line to the right):\\[A_t = \\sigma(W_A[h_{t-1},x_t]+b_A)\\\\B_t = \\sigma(W_B[h_{t-1},x_t]+b_B)\\\\\\hat{C}_t=tanh(W_C[h_{t-1},x_t]+b_C)\\\\C_t = A_t\\odot C_{t-1}+B_t\\odot \\hat{C}_t\\\\D_t = \\sigma(W_D[h_{t-1},x_t]+b_D)\\\\h_t = D_t\\odot tanh(C_t)\\]where $\\odot$ denotes the hadamard product $A\\odot B_{ij}=A_{ij}B_{ij}$. The terminology “gate” in most of the introduction of LSTM denotes the sigmoid activation, since the resulted vector would serve as a mask with values near 0 or near 1. In the origin form of LSTM network, $A_t$ is set to be $A_{i}=1, \\forall i$, thus we have $\\frac{\\partial C_t}{\\partial C_{t-1}}=I$. Here the variant uses a learnt gate (called “forget gate”) to control the memory, the gate would clear some of the values in the memory. So we can initialize $b_A$ as near 1 and then the network is trained to construct forget gate. Now the architecture seems a little bit complicate, we can use the gated recurrent unit (GRU) to simplify the process (practically, GRU saves memory usage):\\[A_t = \\sigma(W_A[h_{t-1},x_t]+b_A)\\\\B_t = \\sigma(W_B[h_{t-1},x_t]+b_B)\\\\C_t=tanh(W_C[A_t\\odot h_{t-1},x_t]+b_C)\\\\h_t = (\\mathbf{1}-B_t)\\odot h_{t-1} + B_t\\odot C_t\\]Now we’ve come to a critical point, before we go any further, think for a minute: how can we improve those models in every possible ways?The following are two examples in pytorch turorial: Classifying names via RNN and pytorch tutorial: Generating names via RNN.In the first tutorial, refering to the origin post:“We will be building and training a basic character-level Recurrent Neural Network (RNN) to classify words. … A character-level RNN reads words as a series of characters - outputting a prediction and “hidden state” at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to. Specifically, we’ll train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling.”Now if we take a look at the goal:$ python predict.py Hinton(-0.47) Scottish(-1.52) English(-3.57) Irish$ python predict.py Schmidhuber(-0.19) German(-2.48) Czech(-2.68) DutchWe will jump the data preprocessing part in this post, and assume the input data are organized as {language: [name1, name2, ...]}, indicating the languages and the corresponding names.Now we will walk through the (mostly standard) procedures:      The input letter is converted into tensors using one-hot encoding: a/b/c...=[0,...,0,1,0,...,0]. The word is represented by a tensor stacking the letters: word.size=torch.Size([line_length, 1, n_letters_alphabet]).    import torch# Find letter index from all_letters, e.g. \"a\" = 0def letterToIndex(letter):    return all_letters.find(letter)  # Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensordef letterToTensor(letter):    tensor = torch.zeros(1, n_letters)    tensor[0][letterToIndex(letter)] = 1    return tensor  # Turn a line into a &lt;line_length x 1 x n_letters&gt;, or an array of one-hot letter vectorsdef lineToTensor(line):    tensor = torch.zeros(len(line), 1, n_letters)    for li, letter in enumerate(line):        tensor[li][0][letterToIndex(letter)] = 1    return tensor        Create an RNN with only forward operations, the pytorch computation graph would handle the recurrent information transmission:    import torch.nn as nnimport torch.nn.functional as Fclass RNN(nn.Module):    def __init__(self, input_size, hidden_size, output_size):        super(RNN, self).__init__()        self.hidden_size = hidden_size        self.i2h = nn.Linear(input_size, hidden_size)        self.h2h = nn.Linear(hidden_size, hidden_size)        self.h2o = nn.Linear(hidden_size, output_size)        self.softmax = nn.LogSoftmax(dim=1)    def forward(self, input, hidden):        hidden = F.tanh(self.i2h(input) + self.h2h(hidden))        output = self.h2o(hidden)        output = self.softmax(output)        return output, hidden    def initHidden(self):        return torch.zeros(1, self.hidden_size)n_hidden = 128rnn = RNN(n_letters, n_hidden, n_categories)        We can train the RNN by a training function like:    learning_rate = 0.005def train(category_tensor, line_tensor):    hidden = rnn.initHidden()    rnn.zero_grad()    for i in range(line_tensor.size()[0]):        output, hidden = rnn(line_tensor[i], hidden)    loss = criterion(output, category_tensor)    loss.backward()    # Add parameters' gradients to their values, multiplied by learning rate    for p in rnn.parameters():        p.data.add_(p.grad.data, alpha=-learning_rate)    return output, loss.item()        We can run the training process for the whole corpus:    import timeimport mathn_iters, print_every, plot_every = 100000, 5000, 1000# Keep track of losses for plottingcurrent_loss = 0all_losses = []def randomChoice(l):    return l[random.randint(0, len(l) - 1)]def randomTrainingExample():    category = randomChoice(all_categories)    line = randomChoice(category_lines[category])    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)    line_tensor = lineToTensor(line)    return category, line, category_tensor, line_tensordef categoryFromOutput(output):    top_n, top_i = output.topk(1)    category_i = top_i[0].item()    return all_categories[category_i], category_idef timeSince(since):    now = time.time()    s = now - since    m = math.floor(s / 60)    s -= m * 60    return '%dm %ds' % (m, s)start = time.time()for iter in range(1, n_iters + 1):    category, line, category_tensor, line_tensor = randomTrainingExample()    output, loss = train(category_tensor, line_tensor)    current_loss += loss    # Print ``iter`` number, loss, name and guess    if iter % print_every == 0:        guess, guess_i = categoryFromOutput(output)        correct = '✓' if guess == category else '✗ (%s)' % category        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))    # Add current loss avg to list of losses    if iter % plot_every == 0:        all_losses.append(current_loss / plot_every)        current_loss = 0        Finally, we can evaluate our model via:    # Keep track of correct guesses in a confusion matrixconfusion = torch.zeros(n_categories, n_categories)n_confusion = 10000# Just return an output given a linedef evaluate(line_tensor):    hidden = rnn.initHidden()    for i in range(line_tensor.size()[0]):        output, hidden = rnn(line_tensor[i], hidden)    return output# Go through a bunch of examples and record which are correctly guessedfor i in range(n_confusion):    category, line, category_tensor, line_tensor = randomTrainingExample()    output = evaluate(line_tensor)    guess, guess_i = categoryFromOutput(output)    category_i = all_categories.index(category)    confusion[category_i][guess_i] += 1# Normalize by dividing every row by its sumfor i in range(n_categories):    confusion[i] = confusion[i] / confusion[i].sum()# Set up plotfig = plt.figure()ax = fig.add_subplot(111)cax = ax.matshow(confusion.numpy())fig.colorbar(cax)# Set up axesax.set_xticklabels([''] + all_categories, rotation=90)ax.set_yticklabels([''] + all_categories)# Force label at every tickax.xaxis.set_major_locator(ticker.MultipleLocator(1))ax.yaxis.set_major_locator(ticker.MultipleLocator(1))# sphinx_gallery_thumbnail_number = 2plt.show()        For deploying the model to handle the user inputs, we do the following:    def predict(input_line, n_predictions=3):    print('\\n&gt; %s' % input_line)    with torch.no_grad():        output = evaluate(lineToTensor(input_line))        # Get top N categories        topv, topi = output.topk(n_predictions, 1, True)        predictions = []        for i in range(n_predictions):            value = topv[0][i].item()            category_index = topi[0][i].item()            print('(%.2f) %s' % (value, all_categories[category_index]))            predictions.append([value, all_categories[category_index]])predict('Dovesky')  Now we proceed the second task: generating names using RNN. The results should look like:$ python sample.py Russian RUSRovakovUantovShavakovThe data preprocessing is pretty much alike to the previous task, now we take a look at the model architecture of this task.\"Network Architecture\" by Sean Robertson      Construct the model as following. The input of the model is a triplet [category, input_at_t, hidden_state], the output would then serve as the input for the next time step output_at_t=input_at_t+1.    import torchimport torch.nn as nnclass RNN(nn.Module):    def __init__(self, input_size, hidden_size, output_size):        super(RNN, self).__init__()        self.hidden_size = hidden_size        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)        self.o2o = nn.Linear(hidden_size + output_size, output_size)        self.dropout = nn.Dropout(0.1)        self.softmax = nn.LogSoftmax(dim=1)    def forward(self, category, input, hidden):        input_combined = torch.cat((category, input, hidden), 1)        hidden = self.i2h(input_combined)        output = self.i2o(input_combined)        output_combined = torch.cat((hidden, output), 1)        output = self.o2o(output_combined)        output = self.dropout(output)        output = self.softmax(output)        return output, hidden    def initHidden(self):        return torch.zeros(1, self.hidden_size)        We need to first get the random pairs of (category, name). Then construct the input item as (category, cur_letter, hidden), the network predict the next letter at every time step. In practice, we create the pairs ('a', 'b'), ('b', 'c'), ('c', '&lt;EOS&gt;') for the word 'abc'.    import random# Random item from a listdef randomChoice(l):    return l[random.randint(0, len(l) - 1)]# Get a random category and random line from that categorydef randomTrainingPair():    category = randomChoice(all_categories)    line = randomChoice(category_lines[category])    return category, line# One-hot vector for categorydef categoryTensor(category):    li = all_categories.index(category)    tensor = torch.zeros(1, n_categories)    tensor[0][li] = 1    return tensor# One-hot matrix of first to last letters (not including EOS) for inputdef inputTensor(line):    tensor = torch.zeros(len(line), 1, n_letters)    for li in range(len(line)):        letter = line[li]        tensor[li][0][all_letters.find(letter)] = 1    return tensor# ``LongTensor`` of second letter to end (EOS) for targetdef targetTensor(line):    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]    letter_indexes.append(n_letters - 1) # EOS    return torch.LongTensor(letter_indexes)# Make category, input, and target tensors from a random category, line pairdef randomTrainingExample():    category, line = randomTrainingPair()    category_tensor = categoryTensor(category)    input_line_tensor = inputTensor(line)    target_line_tensor = targetTensor(line)    return category_tensor, input_line_tensor, target_line_tensor        Now we train the network via:    criterion = nn.NLLLoss()learning_rate = 0.0005def train(category_tensor, input_line_tensor, target_line_tensor):    target_line_tensor.unsqueeze_(-1)    hidden = rnn.initHidden()    rnn.zero_grad()    loss = torch.Tensor([0]) # you can also just simply use ``loss = 0``    for i in range(input_line_tensor.size(0)):        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)        l = criterion(output, target_line_tensor[i])        loss += l    loss.backward()    for p in rnn.parameters():        p.data.add_(p.grad.data, alpha=-learning_rate)    return output, loss.item() / input_line_tensor.size(0)        Now to generate a name given a category, we sample the network via:    max_length = 20# Sample from a category and starting letterdef sample(category, start_letter='A'):    with torch.no_grad():  # no need to track history in sampling        category_tensor = categoryTensor(category)        input = inputTensor(start_letter)        hidden = rnn.initHidden()        output_name = start_letter        for i in range(max_length):            output, hidden = rnn(category_tensor, input[0], hidden)            topv, topi = output.topk(1)            topi = topi[0][0]            if topi == n_letters - 1:                break            else:                letter = all_letters[topi]                output_name += letter            input = inputTensor(letter)        return output_name# Get multiple samples from one category and multiple starting lettersdef samples(category, start_letters='ABC'):    for start_letter in start_letters:        print(sample(category, start_letter))samples('Russian', 'RUS')  Encoder-Decoder ArchitectureRecall that our goal is to model the sequence prediction problem, “Sequence to Sequence Learningwith Neural Networks” proposes the sequence-to-sequence learning with LSTM network. The very fundamental idea of solving this problem is the encoder-decoder architecture. Recall that the RNN would produce the hidden state at every time step, when a sentence is feed into RNN, every word in the sentence represents the input at every time step. We recognize the final hidden state as the context vector, being an abstract representation of the input sentence. Now you can imagine how another RNN take the context vector as part of the input to decode the information from this representation. The diagram illustration is:\"sequence to sequence RNNs\" by pytorch-seq2seq tutorialThe above example shows the process of machine traslation. In the decoder, we reuse the decoded output at $t-1$ as the input at $t$. This is called autoregressive model, where the overall goal is to predict the next element in the sequence with all the previous predictions. Formally, the autoregressive model is a probabilistic model $p(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(t)})$ such that:\\[p\\left(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(t)}\\right)=\\prod_{t=1}^T p\\left(\\mathbf{x}^{(t)} \\mid \\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(t-1)}\\right)\\]The network usually uses the softmax activation for predicting the distribution (assuming the discrete distribution). In MT task, the decoder would output the translated sequence one word by one word, the previous predicted words would be feed into the decoder as part of the input.We now walk through the steps in typical sequence to sequence modeling process. The following contents are mainly from this tutorial, and it’s written much better than my post, please refer the origin post if anything is unclear.      Datasets are organized to be pairs of English version and German version of one sentence.    train_data, valid_data, test_data = (    dataset[\"train\"],    dataset[\"validation\"],    dataset[\"test\"],)train_data[0]&gt; {'en': 'Two young, White males are outside near many bushes.','de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}        A tokenizer is used to turn a string into a list of tokens that make up the string. A token is a generalized word in that case, the tokens may be words, punctuation, numbers and any special symbols.    &gt; python -m spacy download en_core_web_sm&gt; python -m spacy download de_core_news_smen_nlp = spacy.load(\"en_core_web_sm\")de_nlp = spacy.load(\"de_core_news_sm\")string = \"What a lovely day it is today!\"[token.text for token in en_nlp.tokenizer(string)]&gt; ['What', 'a', 'lovely', 'day', 'it', 'is', 'today', '!']  # apply tokenizer to all the datasetsdef tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]    if lower:        en_tokens = [token.lower() for token in en_tokens]        de_tokens = [token.lower() for token in de_tokens]    en_tokens = [sos_token] + en_tokens + [eos_token]    de_tokens = [sos_token] + de_tokens + [eos_token]    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}  max_length = 1_000lower = Truesos_token = \"&lt;sos&gt;\"eos_token = \"&lt;eos&gt;\"fn_kwargs = {    \"en_nlp\": en_nlp,    \"de_nlp\": de_nlp,    \"max_length\": max_length,    \"lower\": lower,    \"sos_token\": sos_token,    \"eos_token\": eos_token,}train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)        Next we need to build a vocabulary for the source and target languages. The vocabulary transform every token to a index. For the tokens appear in the validation and test sets but not in the training set, we use an “unknown token” (&lt;UNK&gt;) to denote. To make the model aware of &lt;UNK&gt; during training, we replace the tokens which appear less than min_freq with &lt;UNK&gt;. Another special token is &lt;PAD&gt;, in cases that we wish to pass a batch of sentences with different length to the model, we use &lt;PAD&gt; to pad the shorter sentences.    min_freq = 2unk_token = \"&lt;unk&gt;\"pad_token = \"&lt;pad&gt;\"special_tokens = [    unk_token,    pad_token,    sos_token,    eos_token,]en_vocab = torchtext.vocab.build_vocab_from_iterator(    train_data[\"en_tokens\"],    min_freq=min_freq,    specials=special_tokens,)de_vocab = torchtext.vocab.build_vocab_from_iterator(    train_data[\"de_tokens\"],    min_freq=min_freq,    specials=special_tokens,)unk_index = en_vocab[unk_token]pad_index = en_vocab[pad_token]en_vocab.set_default_index(unk_index)de_vocab.set_default_index(unk_index)def numericalize_example(example, en_vocab, de_vocab):    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])    return {\"en_ids\": en_ids, \"de_ids\": de_ids}fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)data_type = \"torch\"format_columns = [\"en_ids\", \"de_ids\"]train_data = train_data.with_format(    type=data_type, columns=format_columns, output_all_columns=True)valid_data = valid_data.with_format(    type=data_type,    columns=format_columns,    output_all_columns=True,)test_data = test_data.with_format(    type=data_type,    columns=format_columns,    output_all_columns=True,)        Now we create the dataloader for feeding the data into model.    # padding the batch with pad_index (the index of &lt;PAD&gt;)def get_collate_fn(pad_index):    def collate_fn(batch):        batch_en_ids = [example[\"en_ids\"] for example in batch]        batch_de_ids = [example[\"de_ids\"] for example in batch]        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)        batch = {            \"en_ids\": batch_en_ids,            \"de_ids\": batch_de_ids,        }        return batch    return collate_fndef get_data_loader(dataset, batch_size, pad_index, shuffle=False):    collate_fn = get_collate_fn(pad_index)    data_loader = torch.utils.data.DataLoader(        dataset=dataset,        batch_size=batch_size,        collate_fn=collate_fn,        shuffle=shuffle,    )    return data_loaderbatch_size = 128train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)test_data_loader = get_data_loader(test_data, batch_size, pad_index)        Build our model in three parts: The encode, the decoder, connecting the encoder and the decoder as the seq2seq model.    class Encoder(nn.Module):    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):        super().__init__()        self.hidden_dim = hidden_dim        self.n_layers = n_layers        self.embedding = nn.Embedding(input_dim, embedding_dim)        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)        self.dropout = nn.Dropout(dropout)    def forward(self, src):        embedded = self.dropout(self.embedding(src))        outputs, (hidden, cell) = self.rnn(embedded)        return hidden, cellclass Decoder(nn.Module):    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):        super().__init__()        self.output_dim = output_dim        self.hidden_dim = hidden_dim        self.n_layers = n_layers        self.embedding = nn.Embedding(output_dim, embedding_dim)        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)        self.fc_out = nn.Linear(hidden_dim, output_dim)        self.dropout = nn.Dropout(dropout)    def forward(self, input, hidden, cell):        input = input.unsqueeze(0)        embedded = self.dropout(self.embedding(input))        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))        prediction = self.fc_out(output.squeeze(0))        return prediction, hidden, cellclass Seq2Seq(nn.Module):    def __init__(self, encoder, decoder, device):        super().__init__()        self.encoder = encoder        self.decoder = decoder        self.device = device        assert (            encoder.hidden_dim == decoder.hidden_dim        ),        assert (            encoder.n_layers == decoder.n_layers        ),    def forward(self, src, trg, teacher_forcing_ratio):        batch_size = trg.shape[1]        trg_length = trg.shape[0]        trg_vocab_size = self.decoder.output_dim        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)        hidden, cell = self.encoder(src)        input = trg[0, :]        for t in range(1, trg_length):            output, hidden, cell = self.decoder(input, hidden, cell)            outputs[t] = output            teacher_force = random.random() &lt; teacher_forcing_ratio            top1 = output.argmax(1)            input = trg[t] if teacher_force else top1        return outputs        Train, evaluate, and serve the model via:    input_dim = len(de_vocab)output_dim = len(en_vocab)encoder_embedding_dim = 256decoder_embedding_dim = 256hidden_dim = 512n_layers = 2encoder_dropout = 0.5decoder_dropout = 0.5device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")encoder = Encoder(    input_dim,    encoder_embedding_dim,    hidden_dim,    n_layers,    encoder_dropout,)decoder = Decoder(    output_dim,    decoder_embedding_dim,    hidden_dim,    n_layers,    decoder_dropout,)model = Seq2Seq(encoder, decoder, device).to(device)def init_weights(m):    for name, param in m.named_parameters():        nn.init.uniform_(param.data, -0.08, 0.08)model.apply(init_weights)def count_parameters(model):    return sum(p.numel() for p in model.parameters() if p.requires_grad)print(f\"The model has {count_parameters(model):,} trainable parameters\")# &gt; The model has 13,898,501 trainable parametersoptimizer = optim.Adam(model.parameters())criterion = nn.CrossEntropyLoss(ignore_index=pad_index)def train_fn(    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device):    model.train()    epoch_loss = 0    for i, batch in enumerate(data_loader):        src = batch[\"de_ids\"].to(device)        trg = batch[\"en_ids\"].to(device)        optimizer.zero_grad()        output = model(src, trg, teacher_forcing_ratio)        output_dim = output.shape[-1]        output = output[1:].view(-1, output_dim)        trg = trg[1:].view(-1)        # trg = [(trg length - 1) * batch size]        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(data_loader)def evaluate_fn(model, data_loader, criterion, device):    model.eval()    epoch_loss = 0    with torch.no_grad():        for i, batch in enumerate(data_loader):            src = batch[\"de_ids\"].to(device)            trg = batch[\"en_ids\"].to(device)            output = model(src, trg, 0)            output_dim = output.shape[-1]            output = output[1:].view(-1, output_dim)            trg = trg[1:].view(-1)            loss = criterion(output, trg)            epoch_loss += loss.item()    return epoch_loss / len(data_loader)# Trainingn_epochs = 10clip = 1.0teacher_forcing_ratio = 0.5best_valid_loss = float(\"inf\")for epoch in tqdm.tqdm(range(n_epochs)):    train_loss = train_fn(        model,        train_data_loader,        optimizer,        criterion,        clip,        teacher_forcing_ratio,        device,    )    valid_loss = evaluate_fn(        model,        valid_data_loader,        criterion,        device,    )    if valid_loss &lt; best_valid_loss:        best_valid_loss = valid_loss        torch.save(model.state_dict(), \"tut1-model.pt\")    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")# Evaluatingmodel.load_state_dict(torch.load(\"tut1-model.pt\"))test_loss = evaluate_fn(model, test_data_loader, criterion, device)print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")# Deployingdef translate_sentence(    sentence,    model,    en_nlp,    de_nlp,    en_vocab,    de_vocab,    lower,    sos_token,    eos_token,    device,    max_output_length=25,):    model.eval()    with torch.no_grad():        if isinstance(sentence, str):            tokens = [token.text for token in de_nlp.tokenizer(sentence)]        else:            tokens = [token for token in sentence]        if lower:            tokens = [token.lower() for token in tokens]        tokens = [sos_token] + tokens + [eos_token]        ids = de_vocab.lookup_indices(tokens)        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)        hidden, cell = model.encoder(tensor)        inputs = en_vocab.lookup_indices([sos_token])        for _ in range(max_output_length):            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)            predicted_token = output.argmax(-1).item()            inputs.append(predicted_token)            if predicted_token == en_vocab[eos_token]:                break        tokens = en_vocab.lookup_tokens(inputs)    return tokens    Self-Attention and Transformer  You may noticed that the context vector serves as the information bottleneck (IB) of the encoder-decoder architecture. The IB would compress and preserve the useful representation of the origin input, this aim is, however, not assured to be accomplished. Since the context vector is merely the final hidden state of the RNN encoder, it is natural to think that the context vector would somehow lose information from the faraway input tokens. Now if we stack all the hidden states into one big tensor, apparently there would be redundance of information. We need a selection of the hidden states to preserve the important information, with respect to what? The simple solution is to let the decoder choose. We now introduce the attention mechanism. This mechanism still starts from a simple idea: we cannot take all the hidden states as the context vector, we can sum over them to create one context vector. The summation is weighted by the decoder. So for each decoder step, we regenerate the context vector, to focus on the information that is relavant to the current prediction. Still, a simple idea is to compute a relavance score between the last hidden state of the current decoder input and each encoder hidden state output $\\operatorname{score}(\\mathbf{h}{i-1}^d, \\mathbf{h}_j^e)=\\mathbf{h}{i-1}^d\\cdot\\mathbf{h}_j^e$. This similarity is then used to compute weights:\\[\\alpha_{i j} =\\operatorname{softmax}\\left(\\operatorname{score}\\left(\\mathbf{h}_{i-1}^d, \\mathbf{h}_j^e\\right)\\right) =\\frac{\\exp \\left(\\operatorname{score}\\left(\\mathbf{h}_{i-1}^d, \\mathbf{h}_j^e\\right)\\right.}{\\sum_k \\exp \\left(\\operatorname{score}\\left(\\mathbf{h}_{i-1}^d, \\mathbf{h}_k^e\\right)\\right)}\\]For the discussion of transformers in this post, we mainly focus on building the connection between this architecture and the NLP problems. Several facts need to be clarified:      We discuss the transformers in solving the problem of causal language modeling, which means to predict the next word in the sentence in an autoregressive fashion.        Transformers are not built with recurrent blocks, so it’s different from the way we build RNN (LSTM) blocks. Instead, transformers use multiple multi-head self-attention blocks to project the origin input to a series of hidden states of the same length as the input, and finally obtain a highly abstracted representation.        Transformer’s key novelty is the self-attention mechanism, which allows the transformer to build layer-by-layer information aggregation.  Self-attention appears in two ways: causal (backward looking) self-attention and the bidirectional self-attention. The former assumes the causal structure in the sequence where only the historical information is accessable, while the latter could access all the information in the sequence. In this post, we mainly focus on the causal setting, since it fits in the intuition of natural language directly. As we could conclude from the origin attention mechanism, the only aim is to computing a relavance score between the current hidden state and the historical hidden states, the dot product similarity may be too naive. The self-attention introduced a role-based encoding scheme:  When the embedding is in the role of the current focus, we refer to it as a query.  When the embedding is in the role of a preceding input, we refer to it as a key.  When it serves as a key, we also compute a value to serve as the embedding to be weighed. So the final aggregation is calculated based on value instead of the embedding x itself.The self-attention is calculated as:\\[\\begin{aligned}\\mathbf{q}_i=\\mathbf{x}_i \\mathbf{W}^{\\mathbf{Q}} ; \\mathbf{k}_i &amp; =\\mathbf{x}_i \\mathbf{W}^{\\mathbf{k}} ; \\mathbf{v}_i=\\mathbf{x}_i \\mathbf{W}^{\\mathbf{v}} \\\\\\operatorname{score}\\left(\\mathbf{x}_i, \\mathbf{x}_j\\right) &amp; =\\frac{\\mathbf{q}_i \\cdot \\mathbf{k}_j}{\\sqrt{d_k}} \\\\\\alpha_{i j} &amp; =\\operatorname{softmax}\\left(\\operatorname{score}\\left(\\mathbf{x}_i, \\mathbf{x}_j\\right)\\right) \\forall j \\leq i \\\\\\mathbf{a}_i &amp; =\\sum_{j \\leq i} \\alpha_{i j} \\mathbf{v}_j\\end{aligned}\\]It’s clear now, we transform the origin embedding into three different vector spaces, and produce the final output to the next layer in the vector space of value. Here $d_k$ is the dimension of the vector space spanned by keys, the division by $\\sqrt{d_k}$ is to maintain the numerical stability of the dot product. Another advantage is that for a sequence constitutes of multiple tokens, the self-attention could be effectively calculated by stacking the embeddings of every token:\\[\\begin{aligned}\\mathbf{Q}=\\mathbf{X} \\mathbf{W}^{\\mathbf{Q}} ; \\mathbf{K} &amp; =\\mathbf{X} \\mathbf{W}^{\\mathbf{K}} ; \\mathbf{V}=\\mathbf{X} \\mathbf{W}^{\\mathbf{V}} \\\\\\mathbf{A} &amp; =\\operatorname{softmax}(\\frac{\\mathbf{Q} \\cdot \\mathbf{K}^T}{\\sqrt{d_k}})\\mathbf{V}\\end{aligned}\\]For the causal self-attention, we need to mask the upper triangular of the matrix $\\mathbf{Q} \\cdot \\mathbf{K}^T$, corresponding to the future keys. This matrix also reveals the quadratic relationship between the computation scale and the input sequence length.The above description is referred to as the single head self-attention, the building block of the multi-head self-attention, which is what transformer used in practice. The multi-head self-attention adopts a group of the single-head versions, which means that we calculate $h$ different self-attention results with $\\mathbf{W}_i^{\\mathbf{Q}}$, $\\mathbf{W}_i^{\\mathbf{k}}$, and $\\mathbf{W}_i^{\\mathbf{V}}$, for $i\\in[h]$. We aggregate the results with a linear projection $\\mathbf{W}^\\mathbf{O}$ to produce the output:\\[\\begin{aligned}\\mathbf{Q}_i=\\mathbf{X} \\mathbf{W}_i^{\\mathbf{Q}} ; \\mathbf{K}_i &amp; =\\mathbf{X} \\mathbf{W}_i^{\\mathbf{K}} ; \\mathbf{V}_i=\\mathbf{X} \\mathbf{W}_i^{\\mathbf{V}} \\\\head_i &amp; =\\operatorname{softmax}(\\frac{\\mathbf{Q}_i \\cdot \\mathbf{K}_i^T}{\\sqrt{d_k}})\\mathbf{V}_i \\\\\\mathbf{A}=\\text { MultiHeadAttention }(\\mathbf{X})&amp;=\\left(\\text { head }_1 \\oplus \\text { head }_2 \\ldots \\oplus \\text { head }_h\\right) \\mathbf{W}^O\\end{aligned}\\]where the $\\oplus$ denotes the concatenation.",
            "content_html": "<p>This post is the note for reviewing the development of the Large Language Models (LLMs), for more detailed and rigorous text, please refer to the monographs.</p><p><strong>Warning:</strong> This post should not serve as an introduction text to whom may not be familiar with the deep learning methodologies, since it contains intensive personal opinions, not assured to be correct.</p><h1 id=\"table-of-contents\">Table of Contents</h1><ul>  <li><a href=\"#table-of-contents\">Table of Contents</a>    <ul>      <li><a href=\"#sequence-to-sequence-learning-via-rnn\">Sequence-to-sequence Learning via RNN</a>        <ul>          <li><a href=\"#recurrent-neural-network-and-long-short-term-memory-network\">Recurrent Neural Network and Long Short Term Memory Network</a></li>          <li><a href=\"#encoder-decoder-architecture\">Encoder-Decoder Architecture</a></li>          <li><a href=\"#self-attention-and-transformer\">Self-Attention and Transformer</a></li>        </ul>      </li>    </ul>  </li></ul><h2 id=\"sequence-to-sequence-learning-via-rnn\">Sequence-to-sequence Learning via RNN</h2><p>Despite the purpose of giving an introduction of neural language processing, we will start from reviewing  the machine translation (MT) task, for the simplicity  of the ideas – transfrom one sequence of words to another sequence of words (in some other language).  Note that in the original <a href=\"https://arxiv.org/pdf/1409.3215.pdf\">paper</a> written by Ilya et al., the sequence to sequence learning could be a quite generalizable concept, below are several examples of sequences:</p><p><img style=\"display: block;\" class=\"img-fluid\" src=\"https://i.imgur.com/ZVfrUta.png\" alt=\"seq2seq.\" /></p><p class=\"small\">\"Examples of sequences\" by seq2seq ICML 17' tutorial</p><p>In my opinion, the motivation for developing the modern sequence to sequence learning methods is driven by the following two problems:</p><ul>  <li>    <p>The traditional ML methods rely on the consistent input dimensions. That is to say, the model only deals with the inputs that share a constant shape.</p>  </li>  <li>    <p>Given the training pairs (each pair contains two sequences usually sampled from two domains), the task is essentially to match the distribution of the former to the latter. How do we model, evaluate or optimize the process?</p>  </li></ul><p>The first problem drives the development of recurrent neural network (RNN), while the second problem drives the architecture design of the moderm seq2seq learning models. We now briefly introduce the three important models to support the further discussion, they are RNN, LSTM, and self-attention.</p><h3 id=\"recurrent-neural-network-and-long-short-term-memory-network\">Recurrent Neural Network and Long Short Term Memory Network</h3><p>We would give a short introduction of the model structures and optimization techniques, the main focus of this section remains to be the discussion in the context of sequence-to-sequence learning.</p><p>In short, Recurrent Neural Network (RNN) is a kind of NN architectures in which the model computation graph is directed cyclic graph, while the Long Short Term Memory (LSTM) Network is a more complex version of the core network of RNN. Though sound intuitive, RNNs are proven to be powerful in many ways, e.g., <a href=\"https://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf\">RNNs are Turing Complete</a>, <a href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\">RNNs are nearly intelligence-equivalent by approaching the asymptotic limit in text compression</a>. To make the following introduction non-trivial, we consider the predicting-next-character task, where the model is provided a sequence of characters and is required to predict the next character in the end of the sequence. (Note: The discussion here are mainly derived from the Ph.D <a href=\"http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf\">thesis</a> of Ilya Sutskever).</p><p><strong>RNN Formulation</strong>:</p><p>There are two perspectives on RNN, view it as a single network whose computation graph is cyclic, or view it as a super deep neural network with shared weights among the blocks (unrolled view).</p><p><img style=\"display: block;\" class=\"img-fluid\" src=\"https://i.imgur.com/ELw9Iu9.png\" alt=\"RNN.\" /></p><p class=\"small\">\"An unrolled RNN\" by colah's blog</p><p>My apology for the inconsistency, but we are going to use a slightly different notation from the figure. Let $i_t$ be the input at t-th moment, and $o_t$ be the output at t-th moment. Aparently RNN utilizes a hidden representation of the data, denoted by $h_t$. Now we define the input-output map as:</p>\\[o_t = g(h^o_t) \\\\h^o_t = W_{oh}h_t + b_o \\\\h_t = f(h^i_t) \\\\h^i_t = W_{hi}i_t + W_{hh}h_{t-1} + b_h\\]<p>If you are familiar with the feed forward network, these are just two dense layers connected by the corresponding activation functions. The only difference is that the input layer takes the last moment’s state as part of the input, i.e., a combination of the input and the hidden state.</p><p>Before we discuss the LSTM network, we take a glance at the ackpropagation through time algorithm (BPTT). We first define the training loss function of RNN as the cumulative loss over time:</p>\\[\\mathcal{L}(o, y) = \\sum_{t=1}^T \\mathcal{l}_t(o_t, y_t)\\]<p>Let’s calculate the partial derivative of the loss over $W_{hi}$ and $W_{hh}$, since the calculation w.r.t $W_{oh}$ is straight forward. We have the following:</p>\\[\\frac{\\partial \\mathcal{L}}{\\partial W_{hh}} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{l}_t}{\\partial W_{hh}} =  \\sum_{t=1}^T \\frac{\\partial \\mathcal{l}_t}{\\partial o_t}\\frac{\\partial o_t}{\\partial h^o_t}W_{oh}\\frac{\\partial h_t}{\\partial h^i_t}\\frac{\\partial h^i_t}{\\partial W_{hh}} \\\\\\frac{\\partial \\mathcal{L}}{\\partial W_{hi}} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{l}_t}{\\partial W_{hi}} =  \\sum_{t=1}^T \\frac{\\partial \\mathcal{l}_t}{\\partial o_t}\\frac{\\partial o_t}{\\partial h^o_t}W_{oh}\\frac{\\partial h_t}{\\partial h^i_t}\\frac{\\partial h^i_t}{\\partial W_{hi}}\\]<p>Note here you cannot directly calculate that $\\frac{\\partial h^i_t}{\\partial W_{hh}}=h_{t-1}$ since $h_{t-1} = h_{t-1}(\\cdot, W_{hh})$, that’s because $W_{hh}$ is shared across the whole sequence. Instead, we apply the chain rule on the partial derivative of the multiplication $W_{hh}h^{t-1}$, we have:</p><p>\\(\\frac{\\partial h_t}{\\partial W_{hh}}=\\frac{\\partial h_t}{\\partial h^i_t}(h_{t-1}+W_{hh}\\frac{\\partial h_{t-1}}{\\partial W_{hh}})=\\frac{\\partial h_t}{\\partial h^i_t}(h_{t-1}+W_{hh}(\\frac{\\partial h_{t-1}}{\\partial h^i_{t-1}}(h_{t-2}+W_{hh}\\frac{\\partial h_{t-2}}{\\partial W_{hh}}))))= ...\\)Following the chain rule, we obtain the following:\\(\\frac{\\partial h_t}{\\partial W_{hh}} = \\sum_{j=0}^{t-1} \\left(\\prod_{k=0}^{j}\\frac{\\partial h_{t-k}}{\\partial h^i_{t-k}}W_{hh}\\right)h_{t-j-1}\\)The above computation intuitively not stable, since the high order matrix multiplication is introduced. The resulted training process would be in face of potential gradient explosion and gradient vanishing. A straight forward solution is to truncate the historical computation with a relatively short window, and $j$ in the formula would start from $t-w$, where $w$ is the window size. Apparently we lose the long-term learning ability immediately, and that’s where the LSTM network helps. You may have noticed that one of the most apparent problems of vanila RNN is that it encodes all the historical (and any potential future) information into one matrix, which may align with the intuition of our observations on the humans, yet failed to be effectively trained with gradient descent.</p><blockquote>  <p>Note: You can try to enhance RNN to mitigate these problems, sometimes it’s just a problem of imagination.</p></blockquote><p>LSTM network tries to solve this problem with an architecture that splits the knowledge representation into two parts: The hidden state $h$ and the memory $C$. The good thing about $C$ is $\\frac{\\partial C_t}{\\partial C_{t-1}}=1$. How would this help? Let’s look at RNN and BPTT in an abstract view. Say, we regard the recurrent network as extracting a knowledge representation $W$ from data. So what we need to do is to define an interface of the knowledge representation $h$, a data to interface transformation $i$, and a interface to data transformation $o$. So the learning process could be defined as:\\(\\min \\mathbb{E}_{x,y\\sim P(x,y)}[\\mathcal{L}(o(x), y)]\\\\o(x)=o(h(i(x),W))\\)This looks like any ML objectives, for the data structure of sequences, we model the data as $x=[x_1, x_2, …x_T]$, then for RNN, we could process the data segment by segment. Now $i(x)$ is just any neural layer operation $\\phi_i (W_ix+b_i)$. The point is that we model $h$ as $h = [h_1, h_2, … h_t]$, while $h_t=W_{hh}h_{t-1}+W_{ih}x_t+b_h=W[h_{t-1}, x_t, 1]$, and $o=\\phi_o(h)$, then we get the formulation of RNN. The gradient calculation follows:\\(\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial \\mathcal{L_t}}{\\partial W} = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial \\mathcal{L_t}}{\\partial o_t}\\frac{\\partial o_t}{\\partial h_t}\\sum_{k=1}^t \\prod_{j=k+1}^t\\left(\\frac{\\partial h_j}{\\partial h_{j-1}}\\right)\\frac{\\partial h_k}{\\partial W}\\)<a href=\"https://proceedings.mlr.press/v28/pascanu13.html\">“On the difficulty of training recurrent neural networks”</a> pointed out that, for some constant $\\gamma$:\\(\\prod_{j=k+1}^t\\frac{\\partial h_j}{\\partial h_{j-1}} = \\prod_{j=k+1}^t diag(\\frac{\\partial h_j}{\\partial h^i_{j-1}})\\frac{\\partial h^i_{j-1}}{\\partial h_{j-1}}\\approx \\left\\{ \\begin{array}{rcl}&amp; 0\\quad \\text{for }max(eigen(diag))&lt;\\frac{1}{\\gamma} \\\\&amp; \\infty\\quad \\text{for }max(eigen(diag))&gt;\\frac{1}{\\gamma}\\end{array}\\right.\\)</p><p>The interesting part is that now we can see that the forward calculation already involved with the $W_{hh}^t$. Of course if we model $i(x)$ as a direct map from $x$ to $h$, the problem is solved, but imagin how big the space of $x$ is, for example $x$ represents an arbitrary sentence written in English. So for now we still model $x$ as a sequence decomposition $[x_1, x_2, …x_T]$, and see if there are better modelling for $h$. Now let’s take a look at LSTM network.</p><p><img style=\"display: block;\" class=\"img-fluid\" src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" alt=\"LSTM.\" /></p><p class=\"small\">\"LSTM network\" by colah's blog</p><p>This needs a few explanations, so the upper horizontal line in the figure denotes the memory $C$ and the lower horizontal line denotes the hidden state $h$. The calculation follows (from the left vertical line to the right):</p>\\[A_t = \\sigma(W_A[h_{t-1},x_t]+b_A)\\\\B_t = \\sigma(W_B[h_{t-1},x_t]+b_B)\\\\\\hat{C}_t=tanh(W_C[h_{t-1},x_t]+b_C)\\\\C_t = A_t\\odot C_{t-1}+B_t\\odot \\hat{C}_t\\\\D_t = \\sigma(W_D[h_{t-1},x_t]+b_D)\\\\h_t = D_t\\odot tanh(C_t)\\]<p>where $\\odot$ denotes the hadamard product $A\\odot B_{ij}=A_{ij}B_{ij}$. The terminology “gate” in most of the introduction of LSTM denotes the sigmoid activation, since the resulted vector would serve as a mask with values near 0 or near 1. In the origin form of LSTM network, $A_t$ is set to be $A_{i}=1, \\forall i$, thus we have $\\frac{\\partial C_t}{\\partial C_{t-1}}=I$. Here the variant uses a learnt gate (called “forget gate”) to control the memory, the gate would clear some of the values in the memory. So we can initialize $b_A$ as near 1 and then the network is trained to construct forget gate. Now the architecture seems a little bit complicate, we can use the gated recurrent unit (GRU) to simplify the process (practically, GRU saves memory usage):</p>\\[A_t = \\sigma(W_A[h_{t-1},x_t]+b_A)\\\\B_t = \\sigma(W_B[h_{t-1},x_t]+b_B)\\\\C_t=tanh(W_C[A_t\\odot h_{t-1},x_t]+b_C)\\\\h_t = (\\mathbf{1}-B_t)\\odot h_{t-1} + B_t\\odot C_t\\]<p>Now we’ve come to a critical point, before we go any further, think for a minute: how can we improve those models in every possible ways?</p><p>The following are two examples in <a href=\"https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\">pytorch turorial: Classifying names via RNN</a> and <a href=\"https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\">pytorch tutorial: Generating names via RNN</a>.</p><p>In the first tutorial, refering to the origin post:</p><p><em>“We will be building and training a basic character-level Recurrent Neural Network (RNN) to classify words. … A character-level RNN reads words as a series of characters - outputting a prediction and “hidden state” at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to. Specifically, we’ll train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling.”</em></p><p>Now if we take a look at the goal:</p><pre><code class=\"language-shell\">$ python predict.py Hinton(-0.47) Scottish(-1.52) English(-3.57) Irish$ python predict.py Schmidhuber(-0.19) German(-2.48) Czech(-2.68) Dutch</code></pre><p>We will jump the data preprocessing part in this post, and assume the input data are organized as <code>{language: [name1, name2, ...]}</code>, indicating the languages and the corresponding names.</p><p>Now we will walk through the (mostly standard) procedures:</p><ul>  <li>    <p>The input letter is converted into tensors using one-hot encoding: <code>a/b/c...=[0,...,0,1,0,...,0]</code>. The word is represented by a tensor stacking the letters: <code>word.size=torch.Size([line_length, 1, n_letters_alphabet])</code>.</p>    <pre><code class=\"language-python\">import torch# Find letter index from all_letters, e.g. \"a\" = 0def letterToIndex(letter):    return all_letters.find(letter)  # Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensordef letterToTensor(letter):    tensor = torch.zeros(1, n_letters)    tensor[0][letterToIndex(letter)] = 1    return tensor  # Turn a line into a &lt;line_length x 1 x n_letters&gt;, or an array of one-hot letter vectorsdef lineToTensor(line):    tensor = torch.zeros(len(line), 1, n_letters)    for li, letter in enumerate(line):        tensor[li][0][letterToIndex(letter)] = 1    return tensor</code></pre>  </li>  <li>    <p>Create an RNN with only forward operations, the pytorch computation graph would handle the recurrent information transmission:</p>    <pre><code class=\"language-python\">import torch.nn as nnimport torch.nn.functional as Fclass RNN(nn.Module):    def __init__(self, input_size, hidden_size, output_size):        super(RNN, self).__init__()        self.hidden_size = hidden_size        self.i2h = nn.Linear(input_size, hidden_size)        self.h2h = nn.Linear(hidden_size, hidden_size)        self.h2o = nn.Linear(hidden_size, output_size)        self.softmax = nn.LogSoftmax(dim=1)    def forward(self, input, hidden):        hidden = F.tanh(self.i2h(input) + self.h2h(hidden))        output = self.h2o(hidden)        output = self.softmax(output)        return output, hidden    def initHidden(self):        return torch.zeros(1, self.hidden_size)n_hidden = 128rnn = RNN(n_letters, n_hidden, n_categories)</code></pre>  </li>  <li>    <p>We can train the RNN by a training function like:</p>    <pre><code class=\"language-python\">learning_rate = 0.005def train(category_tensor, line_tensor):    hidden = rnn.initHidden()    rnn.zero_grad()    for i in range(line_tensor.size()[0]):        output, hidden = rnn(line_tensor[i], hidden)    loss = criterion(output, category_tensor)    loss.backward()    # Add parameters' gradients to their values, multiplied by learning rate    for p in rnn.parameters():        p.data.add_(p.grad.data, alpha=-learning_rate)    return output, loss.item()</code></pre>  </li>  <li>    <p>We can run the training process for the whole corpus:</p>    <pre><code class=\"language-python\">import timeimport mathn_iters, print_every, plot_every = 100000, 5000, 1000# Keep track of losses for plottingcurrent_loss = 0all_losses = []def randomChoice(l):    return l[random.randint(0, len(l) - 1)]def randomTrainingExample():    category = randomChoice(all_categories)    line = randomChoice(category_lines[category])    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)    line_tensor = lineToTensor(line)    return category, line, category_tensor, line_tensordef categoryFromOutput(output):    top_n, top_i = output.topk(1)    category_i = top_i[0].item()    return all_categories[category_i], category_idef timeSince(since):    now = time.time()    s = now - since    m = math.floor(s / 60)    s -= m * 60    return '%dm %ds' % (m, s)start = time.time()for iter in range(1, n_iters + 1):    category, line, category_tensor, line_tensor = randomTrainingExample()    output, loss = train(category_tensor, line_tensor)    current_loss += loss    # Print ``iter`` number, loss, name and guess    if iter % print_every == 0:        guess, guess_i = categoryFromOutput(output)        correct = '✓' if guess == category else '✗ (%s)' % category        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))    # Add current loss avg to list of losses    if iter % plot_every == 0:        all_losses.append(current_loss / plot_every)        current_loss = 0</code></pre>  </li>  <li>    <p>Finally, we can evaluate our model via:</p>    <pre><code class=\"language-python\"># Keep track of correct guesses in a confusion matrixconfusion = torch.zeros(n_categories, n_categories)n_confusion = 10000# Just return an output given a linedef evaluate(line_tensor):    hidden = rnn.initHidden()    for i in range(line_tensor.size()[0]):        output, hidden = rnn(line_tensor[i], hidden)    return output# Go through a bunch of examples and record which are correctly guessedfor i in range(n_confusion):    category, line, category_tensor, line_tensor = randomTrainingExample()    output = evaluate(line_tensor)    guess, guess_i = categoryFromOutput(output)    category_i = all_categories.index(category)    confusion[category_i][guess_i] += 1# Normalize by dividing every row by its sumfor i in range(n_categories):    confusion[i] = confusion[i] / confusion[i].sum()# Set up plotfig = plt.figure()ax = fig.add_subplot(111)cax = ax.matshow(confusion.numpy())fig.colorbar(cax)# Set up axesax.set_xticklabels([''] + all_categories, rotation=90)ax.set_yticklabels([''] + all_categories)# Force label at every tickax.xaxis.set_major_locator(ticker.MultipleLocator(1))ax.yaxis.set_major_locator(ticker.MultipleLocator(1))# sphinx_gallery_thumbnail_number = 2plt.show()</code></pre>  </li>  <li>    <p>For deploying the model to handle the user inputs, we do the following:</p>    <pre><code class=\"language-python\">def predict(input_line, n_predictions=3):    print('\\n&gt; %s' % input_line)    with torch.no_grad():        output = evaluate(lineToTensor(input_line))        # Get top N categories        topv, topi = output.topk(n_predictions, 1, True)        predictions = []        for i in range(n_predictions):            value = topv[0][i].item()            category_index = topi[0][i].item()            print('(%.2f) %s' % (value, all_categories[category_index]))            predictions.append([value, all_categories[category_index]])predict('Dovesky')</code></pre>  </li></ul><p>Now we proceed the second task: generating names using RNN. The results should look like:</p><pre><code class=\"language-shell\">$ python sample.py Russian RUSRovakovUantovShavakov</code></pre><p>The data preprocessing is pretty much alike to the previous task, now we take a look at the model architecture of this task.</p><p><img style=\"display: block;\" class=\"img-fluid\" src=\"https://i.imgur.com/jzVrf7f.png\" alt=\"RNN generator.\" /></p><p class=\"small\">\"Network Architecture\" by Sean Robertson</p><ul>  <li>    <p>Construct the model as following. The input of the model is a triplet <code>[category, input_at_t, hidden_state]</code>, the output would then serve as the input for the next time step <code>output_at_t=input_at_t+1</code>.</p>    <pre><code class=\"language-python\">import torchimport torch.nn as nnclass RNN(nn.Module):    def __init__(self, input_size, hidden_size, output_size):        super(RNN, self).__init__()        self.hidden_size = hidden_size        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)        self.o2o = nn.Linear(hidden_size + output_size, output_size)        self.dropout = nn.Dropout(0.1)        self.softmax = nn.LogSoftmax(dim=1)    def forward(self, category, input, hidden):        input_combined = torch.cat((category, input, hidden), 1)        hidden = self.i2h(input_combined)        output = self.i2o(input_combined)        output_combined = torch.cat((hidden, output), 1)        output = self.o2o(output_combined)        output = self.dropout(output)        output = self.softmax(output)        return output, hidden    def initHidden(self):        return torch.zeros(1, self.hidden_size)</code></pre>  </li>  <li>    <p>We need to first get the random pairs of <code>(category, name)</code>. Then construct the input item as <code>(category, cur_letter, hidden)</code>, the network predict the next letter at every time step. In practice, we create the pairs <code>('a', 'b')</code>, <code>('b', 'c')</code>, <code>('c', '&lt;EOS&gt;')</code> for the word <code>'abc'</code>.</p>    <pre><code class=\"language-python\">import random# Random item from a listdef randomChoice(l):    return l[random.randint(0, len(l) - 1)]# Get a random category and random line from that categorydef randomTrainingPair():    category = randomChoice(all_categories)    line = randomChoice(category_lines[category])    return category, line# One-hot vector for categorydef categoryTensor(category):    li = all_categories.index(category)    tensor = torch.zeros(1, n_categories)    tensor[0][li] = 1    return tensor# One-hot matrix of first to last letters (not including EOS) for inputdef inputTensor(line):    tensor = torch.zeros(len(line), 1, n_letters)    for li in range(len(line)):        letter = line[li]        tensor[li][0][all_letters.find(letter)] = 1    return tensor# ``LongTensor`` of second letter to end (EOS) for targetdef targetTensor(line):    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]    letter_indexes.append(n_letters - 1) # EOS    return torch.LongTensor(letter_indexes)# Make category, input, and target tensors from a random category, line pairdef randomTrainingExample():    category, line = randomTrainingPair()    category_tensor = categoryTensor(category)    input_line_tensor = inputTensor(line)    target_line_tensor = targetTensor(line)    return category_tensor, input_line_tensor, target_line_tensor</code></pre>  </li>  <li>    <p>Now we train the network via:</p>    <pre><code class=\"language-python\">criterion = nn.NLLLoss()learning_rate = 0.0005def train(category_tensor, input_line_tensor, target_line_tensor):    target_line_tensor.unsqueeze_(-1)    hidden = rnn.initHidden()    rnn.zero_grad()    loss = torch.Tensor([0]) # you can also just simply use ``loss = 0``    for i in range(input_line_tensor.size(0)):        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)        l = criterion(output, target_line_tensor[i])        loss += l    loss.backward()    for p in rnn.parameters():        p.data.add_(p.grad.data, alpha=-learning_rate)    return output, loss.item() / input_line_tensor.size(0)</code></pre>  </li>  <li>    <p>Now to generate a name given a category, we sample the network via:</p>    <pre><code class=\"language-python\">max_length = 20# Sample from a category and starting letterdef sample(category, start_letter='A'):    with torch.no_grad():  # no need to track history in sampling        category_tensor = categoryTensor(category)        input = inputTensor(start_letter)        hidden = rnn.initHidden()        output_name = start_letter        for i in range(max_length):            output, hidden = rnn(category_tensor, input[0], hidden)            topv, topi = output.topk(1)            topi = topi[0][0]            if topi == n_letters - 1:                break            else:                letter = all_letters[topi]                output_name += letter            input = inputTensor(letter)        return output_name# Get multiple samples from one category and multiple starting lettersdef samples(category, start_letters='ABC'):    for start_letter in start_letters:        print(sample(category, start_letter))samples('Russian', 'RUS')</code></pre>  </li></ul><h3 id=\"encoder-decoder-architecture\">Encoder-Decoder Architecture</h3><p>Recall that our goal is to model the sequence prediction problem, <a href=\"https://arxiv.org/pdf/1409.3215.pdf\">“Sequence to Sequence Learningwith Neural Networks”</a> proposes the sequence-to-sequence learning with LSTM network. The very fundamental idea of solving this problem is the encoder-decoder architecture. Recall that the RNN would produce the hidden state at every time step, when a sentence is feed into RNN, every word in the sentence represents the input at every time step. We recognize the final hidden state as the context vector, being an abstract representation of the input sentence. Now you can imagine how another RNN take the context vector as part of the input to decode the information from this representation. The diagram illustration is:</p><p><img style=\"display: block;\" class=\"img-fluid\" src=\"https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/b3cd54c72cd6e4e63f672d334c795b4fe744ef92//assets/seq2seq1.png\" alt=\"Seq-to-seq architecture\" /></p><p class=\"small\">\"sequence to sequence RNNs\" by pytorch-seq2seq tutorial</p><p>The above example shows the process of machine traslation. In the decoder, we reuse the decoded output at $t-1$ as the input at $t$. This is called autoregressive model, where the overall goal is to predict the next element in the sequence with all the previous predictions. Formally, the autoregressive model is a probabilistic model $p(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(t)})$ such that:</p>\\[p\\left(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(t)}\\right)=\\prod_{t=1}^T p\\left(\\mathbf{x}^{(t)} \\mid \\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(t-1)}\\right)\\]<p>The network usually uses the softmax activation for predicting the distribution (assuming the discrete distribution). In MT task, the decoder would output the translated sequence one word by one word, the previous predicted words would be feed into the decoder as part of the input.</p><p>We now walk through the steps in typical sequence to sequence modeling process. The following contents are mainly from this <a href=\"https://github.com/bentrevett/pytorch-seq2seq\">tutorial</a>, and it’s written much better than my post, please refer the origin post if anything is unclear.</p><ul>  <li>    <p>Datasets are organized to be pairs of English version and German version of one sentence.</p>    <pre><code class=\"language-python\">train_data, valid_data, test_data = (    dataset[\"train\"],    dataset[\"validation\"],    dataset[\"test\"],)train_data[0]&gt; {'en': 'Two young, White males are outside near many bushes.','de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}</code></pre>  </li>  <li>    <p>A tokenizer is used to turn a string into a list of tokens that make up the string. A token is a generalized word in that case, the tokens may be words, punctuation, numbers and any special symbols.</p>    <pre><code class=\"language-python\">&gt; python -m spacy download en_core_web_sm&gt; python -m spacy download de_core_news_smen_nlp = spacy.load(\"en_core_web_sm\")de_nlp = spacy.load(\"de_core_news_sm\")string = \"What a lovely day it is today!\"[token.text for token in en_nlp.tokenizer(string)]&gt; ['What', 'a', 'lovely', 'day', 'it', 'is', 'today', '!']  # apply tokenizer to all the datasetsdef tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]    if lower:        en_tokens = [token.lower() for token in en_tokens]        de_tokens = [token.lower() for token in de_tokens]    en_tokens = [sos_token] + en_tokens + [eos_token]    de_tokens = [sos_token] + de_tokens + [eos_token]    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}  max_length = 1_000lower = Truesos_token = \"&lt;sos&gt;\"eos_token = \"&lt;eos&gt;\"fn_kwargs = {    \"en_nlp\": en_nlp,    \"de_nlp\": de_nlp,    \"max_length\": max_length,    \"lower\": lower,    \"sos_token\": sos_token,    \"eos_token\": eos_token,}train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)</code></pre>  </li>  <li>    <p>Next we need to build a vocabulary for the source and target languages. The vocabulary transform every token to a index. For the tokens appear in the validation and test sets but not in the training set, we use an “unknown token” (<code>&lt;UNK&gt;</code>) to denote. To make the model aware of <code>&lt;UNK&gt;</code> during training, we replace the tokens which appear less than <code>min_freq</code> with <code>&lt;UNK&gt;</code>. Another special token is <code>&lt;PAD&gt;</code>, in cases that we wish to pass a batch of sentences with different length to the model, we use <code>&lt;PAD&gt;</code> to pad the shorter sentences.</p>    <pre><code class=\"language-python\">min_freq = 2unk_token = \"&lt;unk&gt;\"pad_token = \"&lt;pad&gt;\"special_tokens = [    unk_token,    pad_token,    sos_token,    eos_token,]en_vocab = torchtext.vocab.build_vocab_from_iterator(    train_data[\"en_tokens\"],    min_freq=min_freq,    specials=special_tokens,)de_vocab = torchtext.vocab.build_vocab_from_iterator(    train_data[\"de_tokens\"],    min_freq=min_freq,    specials=special_tokens,)unk_index = en_vocab[unk_token]pad_index = en_vocab[pad_token]en_vocab.set_default_index(unk_index)de_vocab.set_default_index(unk_index)def numericalize_example(example, en_vocab, de_vocab):    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])    return {\"en_ids\": en_ids, \"de_ids\": de_ids}fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)data_type = \"torch\"format_columns = [\"en_ids\", \"de_ids\"]train_data = train_data.with_format(    type=data_type, columns=format_columns, output_all_columns=True)valid_data = valid_data.with_format(    type=data_type,    columns=format_columns,    output_all_columns=True,)test_data = test_data.with_format(    type=data_type,    columns=format_columns,    output_all_columns=True,)</code></pre>  </li>  <li>    <p>Now we create the dataloader for feeding the data into model.</p>    <pre><code class=\"language-python\"># padding the batch with pad_index (the index of &lt;PAD&gt;)def get_collate_fn(pad_index):    def collate_fn(batch):        batch_en_ids = [example[\"en_ids\"] for example in batch]        batch_de_ids = [example[\"de_ids\"] for example in batch]        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)        batch = {            \"en_ids\": batch_en_ids,            \"de_ids\": batch_de_ids,        }        return batch    return collate_fndef get_data_loader(dataset, batch_size, pad_index, shuffle=False):    collate_fn = get_collate_fn(pad_index)    data_loader = torch.utils.data.DataLoader(        dataset=dataset,        batch_size=batch_size,        collate_fn=collate_fn,        shuffle=shuffle,    )    return data_loaderbatch_size = 128train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)test_data_loader = get_data_loader(test_data, batch_size, pad_index)</code></pre>  </li>  <li>    <p>Build our model in three parts: The encode, the decoder, connecting the encoder and the decoder as the seq2seq model.</p>    <pre><code class=\"language-python\">class Encoder(nn.Module):    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):        super().__init__()        self.hidden_dim = hidden_dim        self.n_layers = n_layers        self.embedding = nn.Embedding(input_dim, embedding_dim)        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)        self.dropout = nn.Dropout(dropout)    def forward(self, src):        embedded = self.dropout(self.embedding(src))        outputs, (hidden, cell) = self.rnn(embedded)        return hidden, cellclass Decoder(nn.Module):    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):        super().__init__()        self.output_dim = output_dim        self.hidden_dim = hidden_dim        self.n_layers = n_layers        self.embedding = nn.Embedding(output_dim, embedding_dim)        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)        self.fc_out = nn.Linear(hidden_dim, output_dim)        self.dropout = nn.Dropout(dropout)    def forward(self, input, hidden, cell):        input = input.unsqueeze(0)        embedded = self.dropout(self.embedding(input))        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))        prediction = self.fc_out(output.squeeze(0))        return prediction, hidden, cellclass Seq2Seq(nn.Module):    def __init__(self, encoder, decoder, device):        super().__init__()        self.encoder = encoder        self.decoder = decoder        self.device = device        assert (            encoder.hidden_dim == decoder.hidden_dim        ),        assert (            encoder.n_layers == decoder.n_layers        ),    def forward(self, src, trg, teacher_forcing_ratio):        batch_size = trg.shape[1]        trg_length = trg.shape[0]        trg_vocab_size = self.decoder.output_dim        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)        hidden, cell = self.encoder(src)        input = trg[0, :]        for t in range(1, trg_length):            output, hidden, cell = self.decoder(input, hidden, cell)            outputs[t] = output            teacher_force = random.random() &lt; teacher_forcing_ratio            top1 = output.argmax(1)            input = trg[t] if teacher_force else top1        return outputs</code></pre>  </li>  <li>    <p>Train, evaluate, and serve the model via:</p>    <pre><code class=\"language-python\">input_dim = len(de_vocab)output_dim = len(en_vocab)encoder_embedding_dim = 256decoder_embedding_dim = 256hidden_dim = 512n_layers = 2encoder_dropout = 0.5decoder_dropout = 0.5device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")encoder = Encoder(    input_dim,    encoder_embedding_dim,    hidden_dim,    n_layers,    encoder_dropout,)decoder = Decoder(    output_dim,    decoder_embedding_dim,    hidden_dim,    n_layers,    decoder_dropout,)model = Seq2Seq(encoder, decoder, device).to(device)def init_weights(m):    for name, param in m.named_parameters():        nn.init.uniform_(param.data, -0.08, 0.08)model.apply(init_weights)def count_parameters(model):    return sum(p.numel() for p in model.parameters() if p.requires_grad)print(f\"The model has {count_parameters(model):,} trainable parameters\")# &gt; The model has 13,898,501 trainable parametersoptimizer = optim.Adam(model.parameters())criterion = nn.CrossEntropyLoss(ignore_index=pad_index)def train_fn(    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device):    model.train()    epoch_loss = 0    for i, batch in enumerate(data_loader):        src = batch[\"de_ids\"].to(device)        trg = batch[\"en_ids\"].to(device)        optimizer.zero_grad()        output = model(src, trg, teacher_forcing_ratio)        output_dim = output.shape[-1]        output = output[1:].view(-1, output_dim)        trg = trg[1:].view(-1)        # trg = [(trg length - 1) * batch size]        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()    return epoch_loss / len(data_loader)def evaluate_fn(model, data_loader, criterion, device):    model.eval()    epoch_loss = 0    with torch.no_grad():        for i, batch in enumerate(data_loader):            src = batch[\"de_ids\"].to(device)            trg = batch[\"en_ids\"].to(device)            output = model(src, trg, 0)            output_dim = output.shape[-1]            output = output[1:].view(-1, output_dim)            trg = trg[1:].view(-1)            loss = criterion(output, trg)            epoch_loss += loss.item()    return epoch_loss / len(data_loader)# Trainingn_epochs = 10clip = 1.0teacher_forcing_ratio = 0.5best_valid_loss = float(\"inf\")for epoch in tqdm.tqdm(range(n_epochs)):    train_loss = train_fn(        model,        train_data_loader,        optimizer,        criterion,        clip,        teacher_forcing_ratio,        device,    )    valid_loss = evaluate_fn(        model,        valid_data_loader,        criterion,        device,    )    if valid_loss &lt; best_valid_loss:        best_valid_loss = valid_loss        torch.save(model.state_dict(), \"tut1-model.pt\")    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")# Evaluatingmodel.load_state_dict(torch.load(\"tut1-model.pt\"))test_loss = evaluate_fn(model, test_data_loader, criterion, device)print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")# Deployingdef translate_sentence(    sentence,    model,    en_nlp,    de_nlp,    en_vocab,    de_vocab,    lower,    sos_token,    eos_token,    device,    max_output_length=25,):    model.eval()    with torch.no_grad():        if isinstance(sentence, str):            tokens = [token.text for token in de_nlp.tokenizer(sentence)]        else:            tokens = [token for token in sentence]        if lower:            tokens = [token.lower() for token in tokens]        tokens = [sos_token] + tokens + [eos_token]        ids = de_vocab.lookup_indices(tokens)        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)        hidden, cell = model.encoder(tensor)        inputs = en_vocab.lookup_indices([sos_token])        for _ in range(max_output_length):            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)            predicted_token = output.argmax(-1).item()            inputs.append(predicted_token)            if predicted_token == en_vocab[eos_token]:                break        tokens = en_vocab.lookup_tokens(inputs)    return tokens</code></pre>    <h3 id=\"self-attention-and-transformer\">Self-Attention and Transformer</h3>  </li></ul><p>You may noticed that the context vector serves as the information bottleneck (IB) of the encoder-decoder architecture. The IB would compress and preserve the useful representation of the origin input, this aim is, however, not assured to be accomplished. Since the context vector is merely the final hidden state of the RNN encoder, it is natural to think that the context vector would somehow lose information from the faraway input tokens. Now if we stack all the hidden states into one big tensor, apparently there would be redundance of information. We need a selection of the hidden states to preserve the important information, with respect to what? The simple solution is to let the decoder choose. We now introduce the attention mechanism. This mechanism still starts from a simple idea: we cannot take all the hidden states as the context vector, we can sum over them to create one context vector. The summation is weighted by the decoder. So for each decoder step, we regenerate the context vector, to focus on the information that is relavant to the current prediction. Still, a simple idea is to compute a relavance score between the last hidden state of the current decoder input and each encoder hidden state output $\\operatorname{score}(\\mathbf{h}<em>{i-1}^d, \\mathbf{h}_j^e)=\\mathbf{h}</em>{i-1}^d\\cdot\\mathbf{h}_j^e$. This similarity is then used to compute weights:</p>\\[\\alpha_{i j} =\\operatorname{softmax}\\left(\\operatorname{score}\\left(\\mathbf{h}_{i-1}^d, \\mathbf{h}_j^e\\right)\\right) =\\frac{\\exp \\left(\\operatorname{score}\\left(\\mathbf{h}_{i-1}^d, \\mathbf{h}_j^e\\right)\\right.}{\\sum_k \\exp \\left(\\operatorname{score}\\left(\\mathbf{h}_{i-1}^d, \\mathbf{h}_k^e\\right)\\right)}\\]<p>For the discussion of transformers in this post, we mainly focus on building the connection between this architecture and the NLP problems. Several facts need to be clarified:</p><ul>  <li>    <p>We discuss the transformers in solving the problem of causal language modeling, which means to predict the next word in the sentence in an autoregressive fashion.</p>  </li>  <li>    <p>Transformers are not built with recurrent blocks, so it’s different from the way we build RNN (LSTM) blocks. Instead, transformers use multiple multi-head self-attention blocks to project the origin input to a series of hidden states of the same length as the input, and finally obtain a highly abstracted representation.</p>  </li>  <li>    <p>Transformer’s key novelty is the self-attention mechanism, which allows the transformer to build layer-by-layer information aggregation.</p>  </li></ul><p>Self-attention appears in two ways: causal (backward looking) self-attention and the bidirectional self-attention. The former assumes the causal structure in the sequence where only the historical information is accessable, while the latter could access all the information in the sequence. In this post, we mainly focus on the causal setting, since it fits in the intuition of natural language directly. As we could conclude from the origin attention mechanism, the only aim is to computing a relavance score between the current hidden state and the historical hidden states, the dot product similarity may be too naive. The self-attention introduced a role-based encoding scheme:</p><ul>  <li>When the embedding is in the role of the current focus, we refer to it as a <code>query</code>.</li>  <li>When the embedding is in the role of a preceding input, we refer to it as a <code>key</code>.</li>  <li>When it serves as a <code>key</code>, we also compute a <code>value</code> to serve as the embedding to be weighed. So the final aggregation is calculated based on <code>value</code> instead of the embedding <code>x</code> itself.</li></ul><p>The self-attention is calculated as:</p>\\[\\begin{aligned}\\mathbf{q}_i=\\mathbf{x}_i \\mathbf{W}^{\\mathbf{Q}} ; \\mathbf{k}_i &amp; =\\mathbf{x}_i \\mathbf{W}^{\\mathbf{k}} ; \\mathbf{v}_i=\\mathbf{x}_i \\mathbf{W}^{\\mathbf{v}} \\\\\\operatorname{score}\\left(\\mathbf{x}_i, \\mathbf{x}_j\\right) &amp; =\\frac{\\mathbf{q}_i \\cdot \\mathbf{k}_j}{\\sqrt{d_k}} \\\\\\alpha_{i j} &amp; =\\operatorname{softmax}\\left(\\operatorname{score}\\left(\\mathbf{x}_i, \\mathbf{x}_j\\right)\\right) \\forall j \\leq i \\\\\\mathbf{a}_i &amp; =\\sum_{j \\leq i} \\alpha_{i j} \\mathbf{v}_j\\end{aligned}\\]<p>It’s clear now, we transform the origin embedding into three different vector spaces, and produce the final output to the next layer in the vector space of <code>value</code>. Here $d_k$ is the dimension of the vector space spanned by <code>key</code>s, the division by $\\sqrt{d_k}$ is to maintain the numerical stability of the dot product. Another advantage is that for a sequence constitutes of multiple tokens, the self-attention could be effectively calculated by stacking the embeddings of every token:</p>\\[\\begin{aligned}\\mathbf{Q}=\\mathbf{X} \\mathbf{W}^{\\mathbf{Q}} ; \\mathbf{K} &amp; =\\mathbf{X} \\mathbf{W}^{\\mathbf{K}} ; \\mathbf{V}=\\mathbf{X} \\mathbf{W}^{\\mathbf{V}} \\\\\\mathbf{A} &amp; =\\operatorname{softmax}(\\frac{\\mathbf{Q} \\cdot \\mathbf{K}^T}{\\sqrt{d_k}})\\mathbf{V}\\end{aligned}\\]<p>For the causal self-attention, we need to mask the upper triangular of the matrix $\\mathbf{Q} \\cdot \\mathbf{K}^T$, corresponding to the future <code>key</code>s. This matrix also reveals the quadratic relationship between the computation scale and the input sequence length.</p><p>The above description is referred to as the single head self-attention, the building block of the multi-head self-attention, which is what transformer used in practice. The multi-head self-attention adopts a group of the single-head versions, which means that we calculate $h$ different self-attention results with $\\mathbf{W}_i^{\\mathbf{Q}}$, $\\mathbf{W}_i^{\\mathbf{k}}$, and $\\mathbf{W}_i^{\\mathbf{V}}$, for $i\\in[h]$. We aggregate the results with a linear projection $\\mathbf{W}^\\mathbf{O}$ to produce the output:</p>\\[\\begin{aligned}\\mathbf{Q}_i=\\mathbf{X} \\mathbf{W}_i^{\\mathbf{Q}} ; \\mathbf{K}_i &amp; =\\mathbf{X} \\mathbf{W}_i^{\\mathbf{K}} ; \\mathbf{V}_i=\\mathbf{X} \\mathbf{W}_i^{\\mathbf{V}} \\\\head_i &amp; =\\operatorname{softmax}(\\frac{\\mathbf{Q}_i \\cdot \\mathbf{K}_i^T}{\\sqrt{d_k}})\\mathbf{V}_i \\\\\\mathbf{A}=\\text { MultiHeadAttention }(\\mathbf{X})&amp;=\\left(\\text { head }_1 \\oplus \\text { head }_2 \\ldots \\oplus \\text { head }_h\\right) \\mathbf{W}^O\\end{aligned}\\]<p>where the $\\oplus$ denotes the concatenation.</p>",
            "url": "http://localhost:4000/2024/04/13/introllm",
            
            
            
            "tags": ["LLM","introduction","NLP"],
            
            "date_published": "2024-04-13T00:00:00+08:00",
            "date_modified": "2024-04-13T00:00:00+08:00",
            
                "author":  {
                "name": "Qihang Wang",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/04/10/fundmentalmath",
            "title": "Revisiting Foundamental Mathematics in Machine Learning",
            "summary": null,
            "content_text": "This post is the note for reviewing the fundamental mathematics in machine learning, especially, deep learning area. We will briefly revisit the basic concepts and useful tools in the analysis of ML algorithms.Here are some excellent materials you may find helpful:  Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning  The matrix cookbook  Matrix theory- Basic Results and TechniquesIn fact, most of the contents in this post derive from [1].Table of Contents  Table of Contents          Preliminaries                  Matrices and Linear Algebra          Curves, Scalar Fields, and Gradients                    PreliminariesNote: This post assumes you have the preliminary knowledge of simple calculus in univariate case and linear algebra. Here provides some concepts which help us see the things in an abstract way (meaning that we then know where can or cannot we generalize our analysis to).Matrices and Linear AlgebraBefore we proceed, the matrices are useful tools in every aspect in practice. Intuitively, matrices denote the linear mappings between two vector spaces (with appropriate basis defined). You would definitely use the matrix notations when dealing with the problems involve with transformations between vector space, and the perspective of vector space would definitely help when memorizing the concepts.A set $V$ is called a vector space over a field $\\mathbb{F}$ if the following hold:  $u+v$, $u,v\\in V$ and $cv$, $c\\in\\mathbb{F}$, $v\\in V$ are defined.  $u+v \\in V$ for all $u, v \\in V$.  $c v \\in V$ for all $c \\in \\mathbb{F}$ and $v \\in V$.  $u+v=v+u$ for all $u, v \\in V$.  $(u+v)+w=u+(v+w)$ for all $u, v, w \\in V$.  There is an element $0 \\in V$ such that $v+0=v$ for all $v \\in V$.  For each $v \\in V$ there is an element $-v \\in V$ so that $v+(-v)=0$.  $c(u+v)=c u+c v$ for all $c \\in \\mathbb{F}$ and $u, v \\in V$.  $(a+b) v=a v+b v$ for all $a, b \\in \\mathbb{F}$ and $v \\in V$.  $(a b) v=a(b v)$ for all $a, b \\in \\mathbb{F}$ and $v \\in V$.  $1 v=v$ for all $v \\in V$.Some examples of vector space are:  $\\mathbb{F}_n(x)$, the collections of polynomials over a field $\\mathbb{F}$ with degrees at most $n$, w.r.t the ordinary addition and scalar multiplication.  $\\mathbb{F}^k_n$, the set of all polynomials of degree at most $n$ in $k$ variables.  $C[a,b]$, the set of all (real-valued) continuous functions on $[a,b]$.  $C’(x)$, the set of all functions of continuous derivatives on $\\mathbb{R}$.  The set of all even functions.  The set of all odd functions.  The set of set of all functions $f$ such that $f(0)=0$.Let $S$ be a nonempty subset of a vector space $V$ over a field $\\mathbb{F}$. Denote by Span$S$ the collection of all finite linear combinations of the vectors in $S$; that is, Span$S$ consists of all vectors of the form\\(c_1 v_1+c_2 v_2+\\cdots+c_t v_t, \\quad t=1,2, \\ldots, c_i \\in \\mathbb{F}, v_i \\in S,\\)Note that:  Span $S$ is also a vector space.  $S$ spans the vector space $V$ if Span$S=V$.A set $S=\\lbrace v_1, v_2, \\ldots, v_k\\rbrace$ is said to be linearly independent if\\(c_1 v_1+c_2 v_2+\\cdots+c_k v_k=0\\)holds only when $c_1=c_2=\\cdots=c_k=0$. If there are also nontrivial solutions, i.e., not all $c$ are zero, then $S$ is linearly dependent.A basis of a vector space $V$ is a linearly independent set that spans $V$. The number of elements in a basis of $V$ is called the dimension of $V$, denoted as dim$V$. We write dim$V=0$ if $V=\\lbrace 0\\rbrace$. We write dim$V=\\infty$ if no finite set could span $V$.If $\\lbrace u_1, u_2, \\ldots, u_n\\rbrace$ is a basis for a vector space $V$, then every $x$ in $V$ can be uniquely expressed as a linear combination of the basis vectors:\\(x=x_1 u_1+x_2 u_2+\\cdots+x_n u_n,\\)where the $x_i\\in\\mathbb{F}$. The $n$-tuple $\\left(x_1, x_2, \\ldots, x_n\\right)$ is called the coordinate of vector $x$ w.r.t the basis.Let $W$ be a subset of a vector space $V$. If $W$ is also a vector space under the addition and scalar multiplication for $V$, then $W$ is called a subspace of $V$. We have $W$ is a subspace iff $W$ is closed under the addition and scalar multiplication.The sum of subspaces $V_1$, $V_2$ is defined as $V_1+V_2=\\lbrace v_1+v_2\\mid v_1\\in V_1, v_2\\in V_2\\rbrace$. This sum is also a subspace, and also the intersection of $V_1$ and $V_2$ is a subspace. We have $V_1\\bigcap V_2\\subseteq V_i\\subseteq V_1+V_2$. The sum $V_1+V_2$ is called a direct sum, symbolized by $V_1 \\oplus V_2$, if $v_1+v_2=0, v_1 \\in V_1, v_2 \\in V_2 \\Rightarrow v_1=v_2=0$. $V_1+V_2$ is a direct sum iff $\\dim(V_1+V_2)=\\dim V_1 +\\dim V_2$. If $\\lbrace u_1, \\ldots, u_s\\rbrace$ is a basis for $V_1$ and $\\lbrace v_1, \\ldots, v_t\\rbrace$ is a basis for $V_2$, then $\\lbrace u_1, \\ldots, u_s, v_1, \\ldots, v_t\\rbrace$ is a basis for $V_1 \\oplus V_2$.To study the properties of the vector spaces, we have the following:  Let $\\lbrace v_1, v_2, \\ldots, v_k\\rbrace$ be a linearly independent subset of $V$, if $\\dim V=n$, then $k\\leq n$, and if $k&lt;n$, then there exists a vector $v_{k+1}\\in V$ such that $\\lbrace v_1, v_2, \\ldots, v_{k+1}\\rbrace$ is linearly independent.  Let $V_1$ and $V_2$ be subspaces of $V$. Then $\\dim V_1 + \\dim V_2 = \\dim (V_1+V_2)+\\dim (V_1\\bigcap V_2)$.A matrix $A$ with $m$ rows and $n$ columns is an $m\\times n$ rectangular array $(a_{ij})$ whose entries are in some field $\\mathbb{F}$. All the $m\\times n$ matrices over field $\\mathbb{F}$ constitute a vector space $\\mathbb{M}_{m\\times n}(\\mathbb{F})$, whose basis are the matrices with only one entry’s value equals to 1 and 0 for all the other entries. Some basic matrix operations are:Addition: $A+B=(a_{ij}+b_{ij})$Multiplication: $AB=(a_{i1}b_{1j}+a_{i2}b_{2j}+…+a_{in}b_{nj})$ or in column vectors form: $AB=(Ab_1, Ab_2, …,Ab_n)$Transpose: $A^T=(a_{ji})$Conjugate: $\\bar{A}=(\\bar{a}_{ij})$Conjugate Transpose: $A^*=(\\bar{a}_{ji})$Adjoint: $\\text{adj}A=((-1)^{j+i}\\det A(j\\mid i))$A square complex matrix $A=\\left(a_{i j}\\right)$ is said to be  diagonal if $a_{i j}=0, i \\neq j$  upper-triangular if $a_{i j}=0, i&gt;j$  symmetric if $A^T=A$  orthogonal if $A^T A=A A^T=I$  Hermitian if $A^*=A$  normal if $A^* A=A A^*$  unitary if $A^* A=A A^*=I$Partition: \\(A=\\left(\\begin{array}{ll}B &amp; C \\\\D &amp; E\\end{array}\\right)\\)and if the shapes match, we have: \\(\\left(\\begin{array}{cc}A &amp; B \\\\0 &amp; C\\end{array}\\right)\\left(\\begin{array}{cc}X &amp; Y \\\\U &amp; V\\end{array}\\right)=\\left(\\begin{array}{cc}A X+B U &amp; A Y+B V \\\\C U &amp; C V\\end{array}\\right)\\)Elementary row(column) operations are:  Interchange two rows(columns).  Multiply a row(column) by a nonzero constant.  Add a multiple of a row(column) to another row(column).The elementary matrices are those can be obtained from $I_n$ by one elementary row (column) operation. The elementary operations on the other matrix could be represented by multiplying the elementary matrices. Let $A$ be an $m\\times n$ matrix, let $E$ be $m\\times m$ elementary matrix and $F$ be $n\\times n$ matrix, then $EAF$ denotes the transformation of both row operation of $E$ and the column operation of $F$ on $A$. Then we have the following conclusions about the rank decomposition.  Let $A$ be an $m \\times n$ matrix over a field $\\mathbb{F}$. Then there exist an $m \\times m$ matrix $P$ and an $n \\times n$ matrix $Q$, both of which are products of elementary matrices with entries from $\\mathbb{F}$, such that \\(P A Q=\\left(\\begin{array}{cc}I_r &amp; 0 \\\\0 &amp; 0\\end{array}\\right)\\)we have $r=\\text{rank}(A)$.  Let $A$ be an $m \\times n$ (real or complex) matrix of rank $r$. Let $\\operatorname{Ker} A$ be the null space of $A$, i.e., $\\operatorname{Ker} A={x: A x=0}$. Then $\\operatorname{dim} \\operatorname{Ker} A=n-r$.The determinant of matrix $A$, $\\det A$ or $\\mid A\\mid$, is a number associated with $A$. Here list two ways of defining the determinant:  Let $p$ be a permutation on ${1,2,…,n}$, we say $p$ is even if ${1,2,…,n}$ could be restored from $p$ in even number of interchanges, odd otherwise. Define $\\text{sign}(p)=1$ if $p$ is even and $\\text{sign}(p)=-1$ if $p$ is odd. Then we have $\\operatorname{det} A=\\sum_{p \\in S_n} \\operatorname{sign}(p) \\prod_{t=1}^n a_{t p(t)}$.  We can also define the determinant by the Laplace formula as $\\operatorname{det} A=\\sum_{j=1}^n(-1)^{1+j} a_{1 j} \\operatorname{det} A(1 \\mid j)$, where $A(1 \\mid j)$ is a submatrix of $A$ deleting the $1$-st row and the $j$-th column.The determinant has the following properties:  The determinant changes sign if two rows(columns) are interchanged.  The determinant is unchanged if a constant multiple of one row(column) is added to another row(column).  The determinant is a linear function of any row(column) when all the other rows(columns) are held fixed.  For $A,B\\in\\mathbb{M}_n$, we have $\\det AB=\\det A\\det B$.  For $A\\in\\mathbb{M}_n$ and $C\\in\\mathbb{M}_m$, we have \\(\\left|\\begin{array}{ll}A &amp; B \\\\0 &amp; C\\end{array}\\right|=\\operatorname{det} A \\operatorname{det} C\\)A squared matrix is said to be invertible, or nonsingular if there exists a matrix $B$ such that $AB=BA=I$, then $B$ is called the inverse of $A$, $B=A^{-1}$. Define the adjoint of $A$ as $\\text{adj}A=((-1)^{j+i}\\det A(j\\mid i))$, we could also have $A^{-1}=\\frac{1}{\\operatorname{det} A} \\operatorname{adj}(A)$. We could conclude that if $E_1E_2…E_k A=I$, then $A^{-1}=E_1E_2…E_k$, which means we can apply row operations on $(A,I)$, then we obtain $A^{-1}$ from $(I, A^{-1})$.Two square matrices $A$ and $B$ are said to be similar if for some invertible matrix $P$, we have $P^{-1}AP=B$.The following statements are equal:  $A$ is invertible, i.e., $A B=B A=I$ for some $B \\in \\mathbb{M}_n$.  $A B=I$ (or $B A=I$ ) for some $B \\in \\mathbb{M}_n$.  $A$ is of rank $n$.  A is a product of elementary matrices.  $A x=0$ has only the trivial solution $x=0$.$A x=b$ has a unique solution for each $b \\in \\mathbb{C}^n$.  $\\operatorname{det} A \\neq 0$.  The column vectors of $A$ are linearly independent.  The row vectors of $A$ are linearly independent.Let $V$ and $W$ be vector spaces over a field $\\mathbb{F}$. A map $\\mathcal{A}: V \\mapsto W$ is called a linear transformation from $V$ to $W$ if for all $u, v \\in V, c \\in \\mathbb{F}$, $\\mathcal{A}(u+v)=\\mathcal{A}(u)+\\mathcal{A}(v)$, and $\\mathcal{A}(c v)=c \\mathcal{A}(v)$.Let $\\mathcal{A}$ be a linear transformation from $V$ to $W$. The subset in $W$, $\\operatorname{Im}(\\mathcal{A})=\\lbrace\\mathcal{A}(v): v \\in V\\rbrace$ is a subspace of $W$, called the image of $\\mathcal{A}$, and the subset in $V$. $\\operatorname{Ker}(\\mathcal{A})={v \\in V: \\mathcal{A}(v)=0 \\in W}$ is a subspace of $V$, called the kernel or null space of $\\mathcal{A}$. If $V$ is of dimension $n$, then $\\operatorname{dim} \\operatorname{Im}(\\mathcal{A})+\\operatorname{dim} \\operatorname{Ker}(\\mathcal{A})=n$.A matrix of shape $m\\times n$ is always a linear transformation from $\\mathbb{F}^n$ to $\\mathbb{F}^m$. A linear transformation can also be implemented by a matrix: $\\mathcal{A}(x)=Ax$. Under different bases the matrix $A$ may have different forms, but they are similar.“Given a linear transformation on a vector space, it is a central theme of linear algebra to find a basis of the vector space so that the matrix of a linear transformation is as simple as possible, in the sense that the matrix contains more zeros or has a particular structure. In the words of matrices, the given matrix is reduced to a canonical form via similarity”– Matrix theory- Basic Results and TechniquesLet $\\mathcal{A}$ be a linear transformation on a vector space $V$ over $\\mathbb{C}$. A nonzero vector $v \\in V$ is called an eigenvector of $\\mathcal{A}$ belonging to an eigenvalue $\\lambda \\in \\mathbb{C}$ if $\\mathcal{A}(v)=\\lambda v, \\quad v \\neq 0$. The eigen vectors belonging to different eigen values are linearly independent.If $A$ has $n$ linearly independent eigenvectors belonging to eigen values $\\lambda_1, \\lambda_2,…\\lambda_n$, then $A$, under the basis formed by the corresponding eigenvectors, has a diagonal matrix representation:\\(\\left(\\begin{array}{cccc}\\lambda_1 &amp; &amp; &amp; 0 \\\\&amp; \\lambda_2 &amp; &amp; \\\\&amp; &amp; \\ddots &amp; \\\\0 &amp; &amp; &amp; \\lambda_n\\end{array}\\right)\\)In other words, the eigenvalues of $A$ are those $\\lambda\\in \\mathbb{F}$ such that $\\det (\\lambda I-A)=0$. The eigenvectors then are the vectors who serve as the bases of converting $\\mathcal{A}$ to $A$. Furthermore, suppose $A$ is $n\\times n$ complex matrix, the polynomial in $\\lambda$: $p_A(\\lambda)=\\det(\\lambda I_n-A)$ is called the characteristic polynomial of $A$, the zeros of the polynomial are the eigenvalues of $A$. It follows that every $n$-square matrix has $n$ (possibly repeatedly) eigenvalues.The trace of an $n$-square matrix $A$ is defined as $\\text{tr}(A)=\\sum_{i=1}^{n}\\lambda_i$.It follows that:  $\\text{tr}A=\\sum_{i=1}^n a_{ii}$  $\\det A = \\prod_{i=1}^n \\lambda_i$Curves, Scalar Fields, and GradientsThe overall goal is to generalize the analysis on the function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ to $f:\\mathbb{R}^m\\rightarrow\\mathbb{R}^n$. Some useful concepts are:(Derivative of the univariate scalar-valued function) A function is called differentiable at a point $a$ of its domain, if its domain contains an open interval containing $a$, and the limit:\\(\\lim_{h\\rightarrow 0}\\frac{f(a+h)-f(a)}{h}\\)exists. The $\\epsilon-\\delta$ definition of the derivative of $f(x)$ w.r.t its input $x$ states, $\\forall \\epsilon\\in\\mathbb{R}^+$,  there exists $\\delta\\in\\mathbb{R}^+$, such that, $\\forall |h|&lt;\\delta$ and $f(a+h)$ is defined, and\\(|L-\\frac{f(a+h)-f(a)}{h}|&lt;\\epsilon\\)(Curve) The curves take the form $f:\\mathbb{R}\\rightarrow\\mathbb{R}^n$. We could use $\\mathbf{x}(t)$ to denote the curve with $\\mathbf{x}\\in \\mathbf{R}^n$ and $t\\in\\mathbb{R}$. The curve is said to be differentiable if as $\\Delta t\\rightarrow 0$, we have $\\mathbf{x}(t+\\Delta t)-\\mathbf{x}(t)=\\mathbf{x}’(t)\\Delta t+\\mathcal{O}(\\Delta t^2)$. This serves as the definition of the derivative $\\mathbf{x}’(t)$:\\[\\frac{d\\mathbf{x}}{dt}=\\mathbf{x}'(t)=\\lim_{\\Delta t\\rightarrow 0}\\frac{\\Delta\\mathbf{x}}{\\Delta t}\\]it also follows that $\\frac{d}{dt}(\\mathbf{g}\\mathbf{h})=\\frac{d\\mathbf{g}}{dt}\\mathbf{h}+\\frac{d\\mathbf{h}}{dt}\\mathbf{g}$. The derivative is also called tangent vector sometimes. Note that the tangent vector depends on the selection of parameter $t$, which prevents us to obtain an invariant measure of the curve. We could instead study the arc length:\\[s=\\int_{t_0}^t dt'|x'(t')|\\]moreover, we have $\\Delta s=|\\Delta x|+\\mathcal{O}(|\\Delta x|^2)$ (as an aside, $|\\cdot|$ here means the length or norm of a vector), we then have $\\frac{ds}{dt}=sign(|x’|)$, where $sign(x)$ means that we need to set $x$ to be positive or negative according to whether the direction increase $t$ or not. With $s$ as our invariant parameterisation, we are able to define the induced tangent vector as $\\frac{dx}{ds}$, easy to verify that the norm of this tangent vector is always 1. Similarly, the ‘invariance’ induced by the arc-length could be applied into the curvature of the curve (the second order derivative) as $\\kappa(x)=\\frac{d^2 x}{ds^2}$. Now curves map a scalar parameter to a vector value $x\\in\\mathbb{R}^n$, the scalar field does the contrary: $\\phi:\\mathbb{R}^n\\rightarrow \\mathbb{R}$. Generally we can define the vector field as $\\phi:\\mathbb{R}^n\\rightarrow \\mathbb{R}^n$. Consider the scalar field $\\phi$, we could access the derivatives by its gradient: \\(\\nabla \\phi=\\frac{\\partial\\phi}{\\partial x^i}\\mathbf{e}^i\\)Basicly we just take derivative of $\\phi$ w.r.t every coordinate, and constitute a new vector.",
            "content_html": "<p>This post is the note for reviewing the fundamental mathematics in machine learning, especially, deep learning area. We will briefly revisit the basic concepts and useful tools in the analysis of ML algorithms.</p><p>Here are some excellent materials you may find helpful:</p><ol>  <li><a href=\"https://www.cis.upenn.edu/~jean/math-deep.pdf\">Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning</a></li>  <li><a href=\"https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\">The matrix cookbook</a></li>  <li><a href=\"https://link.springer.com/book/10.1007/978-1-4614-1099-7\">Matrix theory- Basic Results and Techniques</a></li></ol><p>In fact, most of the contents in this post derive from [1].</p><h1 id=\"table-of-contents\">Table of Contents</h1><ul>  <li><a href=\"#table-of-contents\">Table of Contents</a>    <ul>      <li><a href=\"#preliminaries\">Preliminaries</a>        <ul>          <li><a href=\"#matrices-and-linear-algebra\">Matrices and Linear Algebra</a></li>          <li><a href=\"#curves-scalar-fields-and-gradients\">Curves, Scalar Fields, and Gradients</a></li>        </ul>      </li>    </ul>  </li></ul><h2 id=\"preliminaries\">Preliminaries</h2><p><strong>Note</strong>: This post assumes you have the preliminary knowledge of simple calculus in univariate case and linear algebra. Here provides some concepts which help us see the things in an abstract way (meaning that we then know where can or cannot we generalize our analysis to).</p><h3 id=\"matrices-and-linear-algebra\">Matrices and Linear Algebra</h3><p>Before we proceed, the matrices are useful tools in every aspect in practice. Intuitively, matrices denote the linear mappings between two vector spaces (with appropriate basis defined). You would definitely use the matrix notations when dealing with the problems involve with transformations between vector space, and the perspective of vector space would definitely help when memorizing the concepts.</p><p>A set $V$ is called a <strong>vector space</strong> over a field $\\mathbb{F}$ if the following hold:</p><ul>  <li>$u+v$, $u,v\\in V$ and $cv$, $c\\in\\mathbb{F}$, $v\\in V$ are defined.</li>  <li>$u+v \\in V$ for all $u, v \\in V$.</li>  <li>$c v \\in V$ for all $c \\in \\mathbb{F}$ and $v \\in V$.</li>  <li>$u+v=v+u$ for all $u, v \\in V$.</li>  <li>$(u+v)+w=u+(v+w)$ for all $u, v, w \\in V$.</li>  <li>There is an element $0 \\in V$ such that $v+0=v$ for all $v \\in V$.</li>  <li>For each $v \\in V$ there is an element $-v \\in V$ so that $v+(-v)=0$.</li>  <li>$c(u+v)=c u+c v$ for all $c \\in \\mathbb{F}$ and $u, v \\in V$.</li>  <li>$(a+b) v=a v+b v$ for all $a, b \\in \\mathbb{F}$ and $v \\in V$.</li>  <li>$(a b) v=a(b v)$ for all $a, b \\in \\mathbb{F}$ and $v \\in V$.</li>  <li>$1 v=v$ for all $v \\in V$.</li></ul><p>Some examples of vector space are:</p><ul>  <li>$\\mathbb{F}_n(x)$, the collections of polynomials over a field $\\mathbb{F}$ with degrees at most $n$, w.r.t the ordinary addition and scalar multiplication.</li>  <li>$\\mathbb{F}^k_n$, the set of all polynomials of degree at most $n$ in $k$ variables.</li>  <li>$C[a,b]$, the set of all (real-valued) continuous functions on $[a,b]$.</li>  <li>$C’(x)$, the set of all functions of continuous derivatives on $\\mathbb{R}$.</li>  <li>The set of all even functions.</li>  <li>The set of all odd functions.</li>  <li>The set of set of all functions $f$ such that $f(0)=0$.</li></ul><p>Let $S$ be a nonempty subset of a vector space $V$ over a field $\\mathbb{F}$. Denote by <strong>Span</strong>$S$ the collection of all finite linear combinations of the vectors in $S$; that is, Span$S$ consists of all vectors of the form\\(c_1 v_1+c_2 v_2+\\cdots+c_t v_t, \\quad t=1,2, \\ldots, c_i \\in \\mathbb{F}, v_i \\in S,\\)</p><p>Note that:</p><ul>  <li>Span $S$ is also a vector space.</li>  <li>$S$ spans the vector space $V$ if Span$S=V$.</li></ul><p>A set $S=\\lbrace v_1, v_2, \\ldots, v_k\\rbrace$ is said to be <strong>linearly independent</strong> if\\(c_1 v_1+c_2 v_2+\\cdots+c_k v_k=0\\)holds only when $c_1=c_2=\\cdots=c_k=0$. If there are also nontrivial solutions, i.e., not all $c$ are zero, then $S$ is <strong>linearly dependent</strong>.</p><p>A basis of a vector space $V$ is a linearly independent set that spans $V$. The number of elements in a basis of $V$ is called the <strong>dimension</strong> of $V$, denoted as dim$V$. We write dim$V=0$ if $V=\\lbrace 0\\rbrace$. We write dim$V=\\infty$ if no finite set could span $V$.</p><p>If $\\lbrace u_1, u_2, \\ldots, u_n\\rbrace$ is a basis for a vector space $V$, then every $x$ in $V$ can be uniquely expressed as a linear combination of the basis vectors:\\(x=x_1 u_1+x_2 u_2+\\cdots+x_n u_n,\\)where the $x_i\\in\\mathbb{F}$. The $n$-tuple $\\left(x_1, x_2, \\ldots, x_n\\right)$ is called the <strong>coordinate</strong> of vector $x$ w.r.t the basis.</p><p>Let $W$ be a subset of a vector space $V$. If $W$ is also a vector space under the addition and scalar multiplication for $V$, then $W$ is called a <strong>subspace</strong> of $V$. We have $W$ is a subspace iff $W$ is closed under the addition and scalar multiplication.</p><p>The <strong>sum of subspaces</strong> $V_1$, $V_2$ is defined as $V_1+V_2=\\lbrace v_1+v_2\\mid v_1\\in V_1, v_2\\in V_2\\rbrace$. This sum is also a subspace, and also the intersection of $V_1$ and $V_2$ is a subspace. We have $V_1\\bigcap V_2\\subseteq V_i\\subseteq V_1+V_2$. The sum $V_1+V_2$ is called a <strong>direct sum</strong>, symbolized by $V_1 \\oplus V_2$, if $v_1+v_2=0, v_1 \\in V_1, v_2 \\in V_2 \\Rightarrow v_1=v_2=0$. $V_1+V_2$ is a direct sum iff $\\dim(V_1+V_2)=\\dim V_1 +\\dim V_2$. If $\\lbrace u_1, \\ldots, u_s\\rbrace$ is a basis for $V_1$ and $\\lbrace v_1, \\ldots, v_t\\rbrace$ is a basis for $V_2$, then $\\lbrace u_1, \\ldots, u_s, v_1, \\ldots, v_t\\rbrace$ is a basis for $V_1 \\oplus V_2$.</p><p>To study the properties of the vector spaces, we have the following:</p><ul>  <li>Let $\\lbrace v_1, v_2, \\ldots, v_k\\rbrace$ be a linearly independent subset of $V$, if $\\dim V=n$, then $k\\leq n$, and if $k&lt;n$, then there exists a vector $v_{k+1}\\in V$ such that $\\lbrace v_1, v_2, \\ldots, v_{k+1}\\rbrace$ is linearly independent.</li>  <li>Let $V_1$ and $V_2$ be subspaces of $V$. Then $\\dim V_1 + \\dim V_2 = \\dim (V_1+V_2)+\\dim (V_1\\bigcap V_2)$.</li></ul><p>A <strong>matrix</strong> $A$ with $m$ rows and $n$ columns is an $m\\times n$ rectangular array $(a_{ij})$ whose entries are in some field $\\mathbb{F}$. All the $m\\times n$ matrices over field $\\mathbb{F}$ constitute a vector space $\\mathbb{M}_{m\\times n}(\\mathbb{F})$, whose basis are the matrices with only one entry’s value equals to 1 and 0 for all the other entries. Some basic matrix operations are:</p><p><strong>Addition:</strong> $A+B=(a_{ij}+b_{ij})$</p><p><strong>Multiplication:</strong> $AB=(a_{i1}b_{1j}+a_{i2}b_{2j}+…+a_{in}b_{nj})$ or in column vectors form: $AB=(Ab_1, Ab_2, …,Ab_n)$</p><p><strong>Transpose:</strong> $A^T=(a_{ji})$</p><p><strong>Conjugate:</strong> $\\bar{A}=(\\bar{a}_{ij})$</p><p><strong>Conjugate Transpose:</strong> $A^*=(\\bar{a}_{ji})$</p><p><strong>Adjoint:</strong> $\\text{adj}A=((-1)^{j+i}\\det A(j\\mid i))$</p><p>A square complex matrix $A=\\left(a_{i j}\\right)$ is said to be</p><ul>  <li>diagonal if $a_{i j}=0, i \\neq j$</li>  <li>upper-triangular if $a_{i j}=0, i&gt;j$</li>  <li>symmetric if $A^T=A$</li>  <li>orthogonal if $A^T A=A A^T=I$</li>  <li>Hermitian if $A^*=A$</li>  <li>normal if $A^* A=A A^*$</li>  <li>unitary if $A^* A=A A^*=I$</li></ul><p><strong>Partition:</strong> \\(A=\\left(\\begin{array}{ll}B &amp; C \\\\D &amp; E\\end{array}\\right)\\)and if the shapes match, we have: \\(\\left(\\begin{array}{cc}A &amp; B \\\\0 &amp; C\\end{array}\\right)\\left(\\begin{array}{cc}X &amp; Y \\\\U &amp; V\\end{array}\\right)=\\left(\\begin{array}{cc}A X+B U &amp; A Y+B V \\\\C U &amp; C V\\end{array}\\right)\\)</p><p><strong>Elementary row(column) operations</strong> are:</p><ul>  <li>Interchange two rows(columns).</li>  <li>Multiply a row(column) by a nonzero constant.</li>  <li>Add a multiple of a row(column) to another row(column).</li></ul><p>The elementary matrices are those can be obtained from $I_n$ by one elementary row (column) operation. The elementary operations on the other matrix could be represented by multiplying the elementary matrices. Let $A$ be an $m\\times n$ matrix, let $E$ be $m\\times m$ elementary matrix and $F$ be $n\\times n$ matrix, then $EAF$ denotes the transformation of both row operation of $E$ and the column operation of $F$ on $A$. Then we have the following conclusions about the rank decomposition.</p><ul>  <li>Let $A$ be an $m \\times n$ matrix over a field $\\mathbb{F}$. Then there exist an $m \\times m$ matrix $P$ and an $n \\times n$ matrix $Q$, both of which are products of elementary matrices with entries from $\\mathbb{F}$, such that \\(P A Q=\\left(\\begin{array}{cc}I_r &amp; 0 \\\\0 &amp; 0\\end{array}\\right)\\)we have $r=\\text{rank}(A)$.</li>  <li>Let $A$ be an $m \\times n$ (real or complex) matrix of rank $r$. Let $\\operatorname{Ker} A$ be the <strong>null space</strong> of $A$, i.e., $\\operatorname{Ker} A={x: A x=0}$. Then $\\operatorname{dim} \\operatorname{Ker} A=n-r$.</li></ul><p>The <strong>determinant</strong> of matrix $A$, $\\det A$ or $\\mid A\\mid$, is a number associated with $A$. Here list two ways of defining the determinant:</p><ul>  <li>Let $p$ be a permutation on ${1,2,…,n}$, we say $p$ is even if ${1,2,…,n}$ could be restored from $p$ in even number of interchanges, odd otherwise. Define $\\text{sign}(p)=1$ if $p$ is even and $\\text{sign}(p)=-1$ if $p$ is odd. Then we have $\\operatorname{det} A=\\sum_{p \\in S_n} \\operatorname{sign}(p) \\prod_{t=1}^n a_{t p(t)}$.</li>  <li>We can also define the determinant by the Laplace formula as $\\operatorname{det} A=\\sum_{j=1}^n(-1)^{1+j} a_{1 j} \\operatorname{det} A(1 \\mid j)$, where $A(1 \\mid j)$ is a submatrix of $A$ deleting the $1$-st row and the $j$-th column.</li></ul><p>The determinant has the following properties:</p><ul>  <li>The determinant changes sign if two rows(columns) are interchanged.</li>  <li>The determinant is unchanged if a constant multiple of one row(column) is added to another row(column).</li>  <li>The determinant is a linear function of any row(column) when all the other rows(columns) are held fixed.</li>  <li>For $A,B\\in\\mathbb{M}_n$, we have $\\det AB=\\det A\\det B$.</li>  <li>For $A\\in\\mathbb{M}_n$ and $C\\in\\mathbb{M}_m$, we have \\(\\left|\\begin{array}{ll}A &amp; B \\\\0 &amp; C\\end{array}\\right|=\\operatorname{det} A \\operatorname{det} C\\)</li></ul><p>A squared matrix is said to be <strong>invertible</strong>, or <strong>nonsingular</strong> if there exists a matrix $B$ such that $AB=BA=I$, then $B$ is called the inverse of $A$, $B=A^{-1}$. Define the <strong>adjoint</strong> of $A$ as $\\text{adj}A=((-1)^{j+i}\\det A(j\\mid i))$, we could also have $A^{-1}=\\frac{1}{\\operatorname{det} A} \\operatorname{adj}(A)$. We could conclude that if $E_1E_2…E_k A=I$, then $A^{-1}=E_1E_2…E_k$, which means we can apply row operations on $(A,I)$, then we obtain $A^{-1}$ from $(I, A^{-1})$.</p><p>Two square matrices $A$ and $B$ are said to be <strong>similar</strong> if for some invertible matrix $P$, we have $P^{-1}AP=B$.</p><p>The following statements are equal:</p><ul>  <li>$A$ is invertible, i.e., $A B=B A=I$ for some $B \\in \\mathbb{M}_n$.</li>  <li>$A B=I$ (or $B A=I$ ) for some $B \\in \\mathbb{M}_n$.</li>  <li>$A$ is of rank $n$.</li>  <li>A is a product of elementary matrices.</li>  <li>$A x=0$ has only the trivial solution $x=0$.$A x=b$ has a unique solution for each $b \\in \\mathbb{C}^n$.</li>  <li>$\\operatorname{det} A \\neq 0$.</li>  <li>The column vectors of $A$ are linearly independent.</li>  <li>The row vectors of $A$ are linearly independent.</li></ul><p>Let $V$ and $W$ be vector spaces over a field $\\mathbb{F}$. A map $\\mathcal{A}: V \\mapsto W$ is called a <strong>linear transformation</strong> from $V$ to $W$ if for all $u, v \\in V, c \\in \\mathbb{F}$, $\\mathcal{A}(u+v)=\\mathcal{A}(u)+\\mathcal{A}(v)$, and $\\mathcal{A}(c v)=c \\mathcal{A}(v)$.</p><p>Let $\\mathcal{A}$ be a linear transformation from $V$ to $W$. The subset in $W$, $\\operatorname{Im}(\\mathcal{A})=\\lbrace\\mathcal{A}(v): v \\in V\\rbrace$ is a subspace of $W$, called the image of $\\mathcal{A}$, and the subset in $V$. $\\operatorname{Ker}(\\mathcal{A})={v \\in V: \\mathcal{A}(v)=0 \\in W}$ is a subspace of $V$, called the kernel or null space of $\\mathcal{A}$. If $V$ is of dimension $n$, then $\\operatorname{dim} \\operatorname{Im}(\\mathcal{A})+\\operatorname{dim} \\operatorname{Ker}(\\mathcal{A})=n$.</p><p>A matrix of shape $m\\times n$ is always a linear transformation from $\\mathbb{F}^n$ to $\\mathbb{F}^m$. A linear transformation can also be implemented by a matrix: $\\mathcal{A}(x)=Ax$. Under different bases the matrix $A$ may have different forms, but they are similar.</p><p><strong>“Given a linear transformation on a vector space, it is a central theme of linear algebra to find a basis of the vector space so that the matrix of a linear transformation is as simple as possible, in the sense that the matrix contains more zeros or has a particular structure. In the words of matrices, the given matrix is reduced to a canonical form via similarity”</strong></p><p>– Matrix theory- Basic Results and Techniques</p><p>Let $\\mathcal{A}$ be a linear transformation on a vector space $V$ over $\\mathbb{C}$. A nonzero vector $v \\in V$ is called an <strong>eigenvector</strong> of $\\mathcal{A}$ belonging to an <strong>eigenvalue</strong> $\\lambda \\in \\mathbb{C}$ if $\\mathcal{A}(v)=\\lambda v, \\quad v \\neq 0$. The eigen vectors belonging to different eigen values are linearly independent.</p><p>If $A$ has $n$ linearly independent eigenvectors belonging to eigen values $\\lambda_1, \\lambda_2,…\\lambda_n$, then $A$, under the basis formed by the corresponding eigenvectors, has a diagonal matrix representation:\\(\\left(\\begin{array}{cccc}\\lambda_1 &amp; &amp; &amp; 0 \\\\&amp; \\lambda_2 &amp; &amp; \\\\&amp; &amp; \\ddots &amp; \\\\0 &amp; &amp; &amp; \\lambda_n\\end{array}\\right)\\)In other words, the eigenvalues of $A$ are those $\\lambda\\in \\mathbb{F}$ such that $\\det (\\lambda I-A)=0$. The eigenvectors then are the vectors who serve as the bases of converting $\\mathcal{A}$ to $A$. Furthermore, suppose $A$ is $n\\times n$ complex matrix, the polynomial in $\\lambda$: $p_A(\\lambda)=\\det(\\lambda I_n-A)$ is called the characteristic polynomial of $A$, the zeros of the polynomial are the eigenvalues of $A$. It follows that every $n$-square matrix has $n$ (possibly repeatedly) eigenvalues.</p><p>The <strong>trace</strong> of an $n$-square matrix $A$ is defined as $\\text{tr}(A)=\\sum_{i=1}^{n}\\lambda_i$.</p><p>It follows that:</p><ul>  <li>$\\text{tr}A=\\sum_{i=1}^n a_{ii}$</li>  <li>$\\det A = \\prod_{i=1}^n \\lambda_i$</li></ul><h3 id=\"curves-scalar-fields-and-gradients\">Curves, Scalar Fields, and Gradients</h3><p>The overall goal is to generalize the analysis on the function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ to $f:\\mathbb{R}^m\\rightarrow\\mathbb{R}^n$. Some useful concepts are:</p><p><strong>(Derivative of the univariate scalar-valued function)</strong> A function is called <em>differentiable</em> at a point $a$ of its domain, if its domain contains an open interval containing $a$, and the limit:\\(\\lim_{h\\rightarrow 0}\\frac{f(a+h)-f(a)}{h}\\)exists. The $\\epsilon-\\delta$ definition of the derivative of $f(x)$ w.r.t its input $x$ states, $\\forall \\epsilon\\in\\mathbb{R}^+$,  there exists $\\delta\\in\\mathbb{R}^+$, such that, $\\forall |h|&lt;\\delta$ and $f(a+h)$ is defined, and\\(|L-\\frac{f(a+h)-f(a)}{h}|&lt;\\epsilon\\)</p><p><strong>(Curve)</strong> The curves take the form $f:\\mathbb{R}\\rightarrow\\mathbb{R}^n$. We could use $\\mathbf{x}(t)$ to denote the curve with $\\mathbf{x}\\in \\mathbf{R}^n$ and $t\\in\\mathbb{R}$. The curve is said to be <em>differentiable</em> if as $\\Delta t\\rightarrow 0$, we have $\\mathbf{x}(t+\\Delta t)-\\mathbf{x}(t)=\\mathbf{x}’(t)\\Delta t+\\mathcal{O}(\\Delta t^2)$. This serves as the definition of the derivative $\\mathbf{x}’(t)$:</p>\\[\\frac{d\\mathbf{x}}{dt}=\\mathbf{x}'(t)=\\lim_{\\Delta t\\rightarrow 0}\\frac{\\Delta\\mathbf{x}}{\\Delta t}\\]<p>it also follows that $\\frac{d}{dt}(\\mathbf{g}\\mathbf{h})=\\frac{d\\mathbf{g}}{dt}\\mathbf{h}+\\frac{d\\mathbf{h}}{dt}\\mathbf{g}$. The derivative is also called <em>tangent vector</em> sometimes. Note that the tangent vector depends on the selection of parameter $t$, which prevents us to obtain an invariant measure of the curve. We could instead study the arc length:</p>\\[s=\\int_{t_0}^t dt'|x'(t')|\\]<p>moreover, we have $\\Delta s=|\\Delta x|+\\mathcal{O}(|\\Delta x|^2)$ (as an aside, $|\\cdot|$ here means the length or norm of a vector), we then have $\\frac{ds}{dt}=sign(|x’|)$, where $sign(x)$ means that we need to set $x$ to be positive or negative according to whether the direction increase $t$ or not. With $s$ as our invariant parameterisation, we are able to define the induced tangent vector as $\\frac{dx}{ds}$, easy to verify that the norm of this tangent vector is always 1. Similarly, the ‘invariance’ induced by the arc-length could be applied into the curvature of the curve (the second order derivative) as $\\kappa(x)=\\frac{d^2 x}{ds^2}$. Now curves map a scalar parameter to a vector value $x\\in\\mathbb{R}^n$, the scalar field does the contrary: $\\phi:\\mathbb{R}^n\\rightarrow \\mathbb{R}$. Generally we can define the vector field as $\\phi:\\mathbb{R}^n\\rightarrow \\mathbb{R}^n$. Consider the scalar field $\\phi$, we could access the derivatives by its gradient: \\(\\nabla \\phi=\\frac{\\partial\\phi}{\\partial x^i}\\mathbf{e}^i\\)Basicly we just take derivative of $\\phi$ w.r.t every coordinate, and constitute a new vector.</p>",
            "url": "http://localhost:4000/2024/04/10/fundmentalmath",
            
            
            
            "tags": ["math","deep learning","algebra"],
            
            "date_published": "2024-04-10T00:00:00+08:00",
            "date_modified": "2024-04-10T00:00:00+08:00",
            
                "author":  {
                "name": "Qihang Wang",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "http://localhost:4000/2021/04/30/test-post-1",
            "title": "Building my blog",
            "summary": null,
            "content_text": "Despite there are a lot of ways to build a blog site, I utilized the jeckll academic theme and github pages to reduce the efforts. Find the commands and installation guides below:Installation$ git clone https://github.com/yak-fumblepack/jekyll-academic.git$ bundle installBuilding$ bundle exec jekyll buildUse --verbose if you would like a detailed log.Serving$ bundle exec jekyll serve --watch --livereload --incremental",
            "content_html": "<p>Despite there are a lot of ways to build a blog site, I utilized the jeckll academic theme and github pages to reduce the efforts. Find the commands and installation guides below:</p><h2 id=\"installation\">Installation</h2><pre><code class=\"language-shell\">$ git clone https://github.com/yak-fumblepack/jekyll-academic.git</code></pre><pre><code class=\"language-shell\">$ bundle install</code></pre><h2 id=\"building\">Building</h2><pre><code class=\"language-shell\">$ bundle exec jekyll build</code></pre><p>Use <code>--verbose</code> if you would like a detailed log.</p><h2 id=\"serving\">Serving</h2><pre><code class=\"language-shell\">$ bundle exec jekyll serve --watch --livereload --incremental</code></pre>",
            "url": "http://localhost:4000/2021/04/30/test-post-1",
            
            
            
            "tags": ["First post","Test"],
            
            "date_published": "2021-04-30T00:00:00+08:00",
            "date_modified": "2021-04-30T00:00:00+08:00",
            
                "author":  {
                "name": "Qihang Wang",
                "url": null,
                "avatar": null
                }
                
            
        }
    
    ]
}